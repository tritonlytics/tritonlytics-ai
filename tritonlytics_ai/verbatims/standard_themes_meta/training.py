# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02e_verbatims-standard-themes-meta-training.ipynb (unless otherwise specified).

__all__ = ['sentiment_mse', 'is_example_acc', 'get_metrics', 'get_loss_func', 'Meta_MM_HF_BaseModelCallback', 'get_cbs',
           'Meta_MM', 'get_learner', 'train', 'get_standard_theme_preds']

# Cell
import os, datetime
import sklearn.metrics as skm

from fastai import metrics as fa_metrics
from fastai.text.all import *
from transformers import *
from blurr.utils import *
from blurr.data.core import *
from blurr.modeling.all import MultiTargetLoss
from blurr.modeling.core import *

from ...utils import *
from ..core import *

# Cell
# define metrics
def sentiment_mse(preds, *targs):
    return fa_metrics.mse(preds[0], targs[0])

def is_example_acc(preds, *targs):
    return fa_metrics.accuracy(preds[1], targs[1])

# Cell
def get_metrics(train_config={}):
    config = {**meta_standard_themes_train_config, **train_config}
    return [ sentiment_mse, is_example_acc ]

# Cell
def get_loss_func(dls, train_df=None, use_weighted=False):
    loss_classes_kwargs = [{}, {}]

    if (use_weighted and train_df is not None):
        is_example_weights = list(np.max(train_df.is_example.value_counts()) /train_df.is_example.value_counts())
        loss_classes_kwargs[1] = {'weight': FloatTensor(is_example_weights).to(dls.device)}

    loss_func = MultiTargetLoss(loss_classes=[MSELossFlat, CrossEntropyLossFlat],
                                loss_classes_kwargs=loss_classes_kwargs,
                                weights=[1, 0.1],
                                reduction='mean')

    return loss_func

# Cell
class Meta_MM_HF_BaseModelCallback(HF_BaseModelCallback):
    def __init__(self, cls_idx=0):
        super().__init__()
        self.cls_idx = cls_idx

    def after_pred(self):
        super().after_pred()
        if (self.learn.pred[0].dim() == 3):
            self.learn.pred = (self.learn.pred[0][:,self.cls_idx,:], self.learn.pred[1][:,self.cls_idx,:])

# Cell
def get_cbs(train_config={}):
    config = {**meta_standard_themes_train_config, **train_config}

    best_model_cb = SaveModelCallback(monitor=config['save_model_monitor'],
                                      comp=config['save_model_comp'],
                                      fname=config['save_model_filename'],
                                      reset_on_fit=False)

    return [Meta_MM_HF_BaseModelCallback], [best_model_cb] # (learn_cbs, fit_cbs)

# Cell
class Meta_MM(Module):
    def __init__(self, in_features=50):
        super().__init__()
        self.pred_is_example = nn.Linear(in_features, 2, bias=False)
        self.pred_avg_sentiment = nn.Linear(in_features, 1, bias=False)
        self.pred_avg_sent_range = SigmoidRange(1., 5.1)

    def forward(self, x):
        is_example = self.pred_is_example(x)
        avg_sentiment = self.pred_avg_sent_range(self.pred_avg_sentiment(x))

        return avg_sentiment, is_example

# Cell
def get_learner(hf_model, dls, train_df=None, use_weighted_loss=False, use_fp16=True,
                opt_func=partial(Adam, mom=0.9, sqr_mom=0.98, eps=1e-6, weight_decay=0.1),
                train_config={}):
    config = {**meta_standard_themes_train_config, **train_config}

    # swap out classifier for our Meta_MM module
    last_layer = list(hf_model.named_children())[-1]
    last_layer_name = last_layer[0]
    in_features = hf_model._modules[last_layer_name].dense.in_features
    hf_model._modules[last_layer_name] = Meta_MM(in_features=in_features)

    # build learner
    model = HF_BaseModelWrapper(hf_model)
    loss_func = get_loss_func(dls, train_df, use_weighted_loss)
    learn_cbs, fit_cbs = get_cbs(config)
    learn_metrics = get_metrics(config)

    learn = Learner(dls, model, loss_func=loss_func, opt_func=opt_func,
                    metrics=learn_metrics, cbs=learn_cbs, splitter=hf_splitter, path=config['learner_path'])

    if (use_fp16): learn = learn.to_fp16()
    learn.create_opt() # -> will create your layer groups based on your "splitter" function
    learn.freeze()

    return learn, fit_cbs

# Cell
def train(hf_arch, hf_config, hf_tokenizer, hf_model, train_config={}):

    config = {**meta_standard_themes_train_config, **train_config}
    m_pre, m_suf = config['m_pre'], config['m_suf']

    df = get_meta_standard_theme_train_data(train_config=config)
    train_df, valid_df = df[df.is_valid == False], df[df.is_valid == True]

    dls = get_meta_standard_theme_train_dls(df, hf_arch, hf_tokenizer, train_config=config)

    learn, fit_cbs = get_learner(hf_model, dls, train_df=None, use_weighted_loss=False, train_config=config)

    with learn.no_logging():
        lr_min, lr_steep = learn.lr_find()
        learn.fit_one_cycle(1, lr_max=lr_min, cbs=fit_cbs)

        learn.unfreeze()
        lr_min, lr_steep = learn.lr_find()
        learn.fit_one_cycle(10, lr_max=slice(lr_min/10, lr_min), cbs=fit_cbs)

        # export model for inference (SavedModelCallback already saves the best model)
        learn.export(fname=config['export_filename'])

    # ===== fetch scores, probs, targs, losses, and optional f-scores/thresholds ====
    learn = learn.load(config['save_model_filename'])
    scores = dict(zip(learn.recorder.metric_names[2:], learn.validate()))
    probs, targs, losses = learn.get_preds(dl=dls.valid, with_loss=True)

    # determine optimal threshold based on desired f-score
    average, sample_weight = config['opt_beta_average'], config['opt_beta_sample_weight']

    f05 = OptimalMultiThresholdMetrics(beta=0.5, start=0.05, end=.5, sigmoid=False,
                                       average=average, sample_weight=sample_weight)
    f1 = OptimalMultiThresholdMetrics(beta=1, start=0.05, end=.5, sigmoid=False,
                                       average=average, sample_weight=sample_weight)
    f2 = OptimalMultiThresholdMetrics(beta=2, start=0.05, end=.5, sigmoid=False,
                                       average=average, sample_weight=sample_weight)

    is_example_prob_true = torch.softmax(probs[1], dim=-1)[:,1]
    scores['is_example_f05'], scores['is_example_f1'], scores['is_example_f2'] = {}, {}, {}

    scores['is_example_f05']['threshold'] = f05.opt_th(is_example_prob_true, targs[1])
    scores['is_example_f1']['threshold'] = f1.opt_th(is_example_prob_true, targs[1])
    scores['is_example_f2']['threshold'] = f2.opt_th(is_example_prob_true, targs[1])

    scores['is_example_f05']['score'] = f05.opt_fscore(is_example_prob_true, targs[1])
    scores['is_example_f1']['score'] = f1.opt_fscore(is_example_prob_true, targs[1])
    scores['is_example_f2']['score'] = f2.opt_fscore(is_example_prob_true, targs[1])

    scores['sentiment'] = {
        'mae': skm.mean_absolute_error(targs[0], probs[0]),
        'mse': skm.mean_squared_error(targs[0], probs[0]),
        'rmse': math.sqrt(skm.mean_squared_error(targs[0], probs[0]))
    }

    # save scores from validation set
    yyyymmdd = datetime.today().strftime("%Y%m%d")

    with open(f"{config['learner_path']}/{yyyymmdd}_model_scores{m_suf}.pkl", 'wb') as f:
        pickle.dump(scores, f)

    # save train/validation probs, targs, losses for review
    test_dl = dls.test_dl(df, with_labels=True)
    probs, targs, losses = learn.get_preds(dl=test_dl, with_loss=True)
    is_example_prob_true = torch.softmax(probs[1], dim=-1)[:,1]

    probs_df = pd.DataFrame(np.concatenate((probs[0].numpy(), is_example_prob_true[:,None]), axis=-1),
                            columns=['pred_sentiment', 'prob_is_example'])
    targs_df = pd.DataFrame(np.concatenate((targs[0].numpy()[:,None], targs[1].numpy()[:,None]), axis=-1),
                            columns= ['targ_sentiment', 'targ_is_example'])
    losses_df = pd.DataFrame(losses.numpy(), columns=['loss'])
    final_df = pd.concat([df.reset_index(), probs_df, targs_df, losses_df], axis=1)

    final_df.to_csv(f"{config['learner_path']}/{yyyymmdd}_model_preds{m_suf}.csv", index=False)

    return scores, final_df

# Cell
def get_standard_theme_preds(inf_df, learner_export_path=None, device=torch.device('cpu'), train_config={}):
    config = {**meta_standard_themes_train_config, **train_config}
    m_pre, m_suf = config['m_pre'], config['m_suf']

    # 1. grab learner, procs, and data
    cpu = device.type == 'cpu'
    if (learner_export_path is None): learner_export_path = f"{config['learner_path']}/{config['export_filename']}"

    inf_learn = load_learner(fname=learner_export_path, cpu=cpu)
    inf_learn.model = inf_learn.model.to(device)
    inf_learn.model = inf_learn.model.eval()

    # 2. define a suitable dataloader
    inf_dl = inf_learn.dls.test_dl(inf_df, rm_type_tfms=None, bs=16)

    # 3. get probs and document vectors
    test_probs_sent, test_probs_is_example = [], []
    with torch.no_grad():
        for index, b in enumerate(inf_dl):
            if index % 1000 == 0:  print(index)

            # note: even though there is no targets, each batch is a tuple!
            probs = inf_learn.model(b[0])

            # why "detach"? the computation of gradients wrt the weights of netG can be fully
            # avoided in the backward pass if the graph is detached where it is.
            test_probs_sent.append(to_detach(probs[0][0]))
            test_probs_is_example.append(to_detach(torch.softmax(probs[0][1], dim=-1)))

    all_probs_sent = L(torch.cat(test_probs_sent))
    all_probs_is_example = L(torch.cat(test_probs_is_example))

    # 4. ensure results are returned in order
    # test_dl.get_idxs() => unsorted/original order items
    all_probs_sent = all_probs_sent[0][np.argsort(inf_dl.get_idxs())]
    all_probs_is_example = all_probs_is_example[0][np.argsort(inf_dl.get_idxs())]

    # 5. return ordered results
    inf_learn, inf_dl = None, None; gc.collect()

    return all_probs_sent, all_probs_is_example