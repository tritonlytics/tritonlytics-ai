# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02c_verbatims-standard-themes-meta.ipynb (unless otherwise specified).

__all__ = ['train_config', 'get_train_data', 'get_train_x', 'get_train_dls', 'sentiment_mse', 'is_example_acc',
           'get_metrics', 'get_loss_func', 'Meta_MM_HF_BaseModelCallback', 'get_cbs', 'Meta_MM', 'get_learner', 'train',
           'after_trial_cleanup', 'objective', 'get_preds', 'build_meta_inf_df']

# Cell
import os, datetime, gc
import sklearn.metrics as skm
from fastai.text.all import *
from fastai import metrics as fa_metrics
from transformers import *

import optuna

from blurr.utils import *
from blurr.data.core import *
from blurr.modeling.core import *
from blurr.modeling.all import MultiTargetLoss

from ..utils import *

from transformers import logging as hf_logging
hf_logging.set_verbosity_error()

# Cell
train_config = {
    'm_pre': '',
    'm_suf': '_multilabel_hf',
    'base_model_name': 'verbatim_standard_theme_meta',

    'orig_txt_cols': ['theme', 'answer_text'],
    'txt_cols': ['_theme_', '_text_'],
    'max_seq_length': 256,
    'batch_size': 8,
    'corpus_suf': '_multitask',
    'train_data': STANDARD_THEME_META_PATH/'train.csv',
    'valid_data': STANDARD_THEME_META_PATH/'test.csv',

    'opt_beta': 0.5,
    'opt_beta_average': 'binary',
    'opt_beta_sample_weight': None,
    'opt_start': 0.08,
    'opt_end': 0.7,

    'save_model_monitor': 'valid_loss',
    'save_model_comp': np.less,
    'learner_path': STANDARD_THEME_META_PATH
}

train_config.update({
    'cache_data_path': STANDARD_THEME_META_PATH/f"data_{train_config['base_model_name']}.pkl",
    'save_model_filename': f"{train_config['m_pre']}{train_config['base_model_name']}{train_config['m_suf']}_bestmodel",
    'export_filename': f"{train_config['m_pre']}{train_config['base_model_name']}{train_config['m_suf']}_export.pkl",
})

# Cell
def get_train_data(train_config_updates={}):
    config = {**train_config, **train_config_updates}

    train_df = pd.read_csv(config['train_data'])
    train_df.dropna(subset=config['orig_txt_cols'], inplace=True)
    train_df.reset_index(drop=True, inplace=True)
    train_df['is_valid'] = False
    train_df[train_config['txt_cols']] = train_df[train_config['orig_txt_cols']]

    if ('valid_data' in config and config['valid_data'] is not None):
        valid_df = pd.read_csv(config['valid_data'])
        valid_df.dropna(subset=config['orig_txt_cols'], inplace=True)
        valid_df.reset_index(drop=True, inplace=True)
        valid_df['is_valid'] = True
        valid_df[train_config['txt_cols']] = valid_df[train_config['orig_txt_cols']]

        return pd.concat([train_df, valid_df])

    return train_df

# Cell
def get_train_x(inp, txt_cols): return 'theme: ' + ' comment: '.join(inp[txt_cols].values)

def get_train_dls(df, hf_arch, hf_config, hf_tokenizer, hf_model,
                  train_config_updates={}, use_cache=False):

    config = {**train_config, **train_config_updates}
    cache_path = config['cache_data_path'] if ('cache_data_path' in config) else None

    if (use_cache and cache_path is not None):
        if (os.path.isfile(cache_path)):
            dls = torch.load(cache_path)
            dls.bs = config['batch_size']
            return dls

    blocks = (
        HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=config['max_seq_length']),
        RegressionBlock(),
        CategoryBlock()
    )

    dblock = DataBlock(blocks=blocks,
                   get_x=partial(get_train_x, txt_cols=config['txt_cols']),
                   get_y=[ColReader('avg_sentiment'), ColReader('is_example')],
                   splitter=ColSplitter(col='is_valid'),
                   n_inp=1)

    set_seed(TL_RAND_SEED)
    dls = dblock.dataloaders(df, bs=config['batch_size'], num_workers=0)
    if (cache_path is not None): torch.save(dls, config['cache_data_path'])

    return dls

# Cell
# define metrics
def sentiment_mse(preds, *targs):
    return fa_metrics.mse(preds[0], targs[0])

def is_example_acc(preds, *targs):
    return fa_metrics.accuracy(preds[1], targs[1])

# Cell
def get_metrics(train_config_updates={}):
    config = {**train_config, **train_config_updates}
    return [ sentiment_mse, is_example_acc ]

# Cell
def get_loss_func(dls, train_df=None, use_weighted=False):
    loss_classes_kwargs = [{}, {}]

    if (use_weighted and train_df is not None):
        is_example_weights = list(np.max(train_df.is_example.value_counts()) /train_df.is_example.value_counts())
        loss_classes_kwargs[1] = {'weight': FloatTensor(is_example_weights).to(dls.device)}

    loss_func = MultiTargetLoss(loss_classes=[MSELossFlat, CrossEntropyLossFlat],
                                loss_classes_kwargs=loss_classes_kwargs,
                                weights=[1, 0.1],
                                reduction='mean')

    return loss_func

# Cell
class Meta_MM_HF_BaseModelCallback(HF_BaseModelCallback):
    def __init__(self, cls_idx=0):
        super().__init__()
        self.cls_idx = cls_idx

    def after_pred(self):
        super().after_pred()
        if (self.learn.pred[0].dim() == 3):
            self.learn.pred = (self.learn.pred[0][:,self.cls_idx,:], self.learn.pred[1][:,self.cls_idx,:])

# Cell
def get_cbs(train_config_updates={}, add_save_model_cb=True):
    config = {**train_config, **train_config_updates}
    fit_cbs = []

    best_model_cb = SaveModelCallback(monitor=config['save_model_monitor'],
                                      comp=config['save_model_comp'],
                                      fname=config['save_model_filename'],
                                      reset_on_fit=False)

    if (add_save_model_cb): fit_cbs.append(best_model_cb)

    return [Meta_MM_HF_BaseModelCallback], fit_cbs # (learn_cbs, fit_cbs)

# Cell
class Meta_MM(Module):
    def __init__(self, in_features=50):
        super().__init__()
        self.pred_is_example = nn.Linear(in_features, 2, bias=False)
        self.pred_avg_sentiment = nn.Linear(in_features, 1, bias=False)
        self.pred_avg_sent_range = SigmoidRange(1., 5.1)

    def forward(self, x):
        is_example = self.pred_is_example(x)
        avg_sentiment = self.pred_avg_sent_range(self.pred_avg_sentiment(x))

        return avg_sentiment, is_example

# Cell
def get_learner(hf_model, dls, train_df=None, use_weighted_loss=False, use_fp16=True,
                opt_func=partial(Adam, mom=0.9, sqr_mom=0.98, eps=1e-6, wd=0.1),
                add_save_model_cb=True, train_config_updates={}):

    config = {**train_config, **train_config_updates}

    # swap out classifier for our Meta_MM module
    last_layer = list(hf_model.named_children())[-1]
    last_layer_name = last_layer[0]
    in_features = hf_model._modules[last_layer_name].dense.in_features
    hf_model._modules[last_layer_name] = Meta_MM(in_features=in_features)

    # build learner
    model = HF_BaseModelWrapper(hf_model)
    loss_func = get_loss_func(dls, train_df, use_weighted_loss)
    learn_cbs, fit_cbs = get_cbs(config, add_save_model_cb=add_save_model_cb)
    learn_metrics = get_metrics(config)

    set_seed(TL_RAND_SEED)
    learn = Learner(dls, model, loss_func=loss_func, opt_func=opt_func,
                    metrics=learn_metrics, cbs=learn_cbs, splitter=hf_splitter, path=config['learner_path'])

    if (use_fp16): learn = learn.to_fp16()
    learn.create_opt() # -> will create your layer groups based on your "splitter" function
    learn.freeze()

    return learn, fit_cbs

# Cell
def train(params, trial=None, yyyymmdd = datetime.today().strftime("%Y%m%d"), train_config_updates={}):

    config = {**train_config, **train_config_updates}
    m_pre, m_suf, base_model_name = config['m_pre'], config['m_suf'], config['base_model_name']
    full_model_name = f'{m_pre}{base_model_name}{m_suf}'

    # 1. grab our huggingface objects
    task = HF_TASKS_AUTO.SequenceClassification
    hf_config = AutoConfig.from_pretrained(params["pretrained_model_name"])

    if (f'{params["pretrained_model_name"]}_config_overrides' in params):
        hf_config.update(params[f'{params["pretrained_model_name"]}_config_overrides'])
    else:
        config_overrides = { k:v for k,v in params.items() if (k in hf_config.to_dict()) }
        hf_config.update(config_overrides)

    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(params["pretrained_model_name"],
                                                                                   task=task,
                                                                                   config=hf_config)

    # 2. build our dls and learner
    df = get_train_data(train_config_updates=config)
    train_df, valid_df = df[df.is_valid == False], df[df.is_valid == True]

    set_seed(TL_RAND_SEED)
    dls = get_train_dls(df, hf_arch, hf_config, hf_tokenizer, hf_model, train_config_updates=config, use_cache=False)

    set_seed(TL_RAND_SEED)
    learn, fit_cbs = get_learner(hf_model,
                                 dls,
                                 train_df=None,
                                 use_weighted_loss=params["use_weighted_loss"],
                                 use_fp16=params["use_fp16"],
                                 add_save_model_cb=params['save_model'],
                                 train_config_updates=config)

    if (trial is not None): learn.add_cb(FastAIPruningCallbackv2(trial=trial, monitor=params['optimize_for']))

    # 3. train
    with learn.no_logging():
        set_seed(TL_RAND_SEED)
        learn.fit_one_cycle(params["n_frozen_epochs"], lr_max=params["frozen_lr"], cbs=fit_cbs)

        learn.unfreeze()
        set_seed(TL_RAND_SEED)
        learn.fit_one_cycle(params["n_unfrozen_epochs"],
                            lr_max=slice(params["unfrozen_lr_min"], params["unfrozen_lr_max"]),
                            cbs=fit_cbs)

        # export model for inference (SavedModelCallback already saves the best model if save_mode=True)
        if (trial is None): learn.export(fname=f"{yyyymmdd}_{config['export_filename']}")

    # 4. evaluate
    scores = dict(zip(learn.recorder.metric_names[2:], learn.validate()))

    try:
        if (trial is not None): return scores[params['optimize_for']]

        probs, targs, losses = learn.get_preds(dl=dls.valid, with_loss=True)

        # determine optimal threshold based on desired f-score
        average, sample_weight = config['opt_beta_average'], config['opt_beta_sample_weight']

        f05 = OptimalMultiThresholdMetrics(beta=0.5, start=0.05, end=.5, sigmoid=False,
                                           average=average, sample_weight=sample_weight)
        f1 = OptimalMultiThresholdMetrics(beta=1, start=0.05, end=.5, sigmoid=False,
                                           average=average, sample_weight=sample_weight)
        f2 = OptimalMultiThresholdMetrics(beta=2, start=0.05, end=.5, sigmoid=False,
                                           average=average, sample_weight=sample_weight)

        is_example_prob_true = torch.softmax(probs[1], dim=-1)[:,1]
        scores['is_example_f05'], scores['is_example_f1'], scores['is_example_f2'] = {}, {}, {}

        scores['is_example_f05']['threshold'] = f05.opt_th(is_example_prob_true, targs[1])
        scores['is_example_f1']['threshold'] = f1.opt_th(is_example_prob_true, targs[1])
        scores['is_example_f2']['threshold'] = f2.opt_th(is_example_prob_true, targs[1])

        scores['is_example_f05']['score'] = f05.opt_fscore(is_example_prob_true, targs[1])
        scores['is_example_f1']['score'] = f1.opt_fscore(is_example_prob_true, targs[1])
        scores['is_example_f2']['score'] = f2.opt_fscore(is_example_prob_true, targs[1])

        scores['sentiment'] = {
            'mae': skm.mean_absolute_error(targs[0], probs[0]).item(),
            'mse': skm.mean_squared_error(targs[0], probs[0]).item(),
            'rmse': math.sqrt(skm.mean_squared_error(targs[0], probs[0]).item())
        }

        # save scores from validation set if mode == training
        with open(f"{config['learner_path']}/{yyyymmdd}_{full_model_name}_train_scores.json", 'w') as f:
            json.dump(scores, f, indent=4)

        # save train/validation probs, targs, losses for review
        test_dl = dls.test_dl(df, with_labels=True)
        probs, targs, losses = learn.get_preds(dl=test_dl, with_loss=True)
        is_example_prob_true = torch.softmax(probs[1], dim=-1)[:,1]

        probs_df = pd.DataFrame(np.concatenate((probs[0].numpy(), is_example_prob_true[:,None]), axis=-1),
                                columns=['pred_sentiment', 'prob_is_example'])
        targs_df = pd.DataFrame(np.concatenate((targs[0].numpy()[:,None], targs[1].numpy()[:,None]), axis=-1),
                                columns= ['targ_sentiment', 'targ_is_example'])
        losses_df = pd.DataFrame(losses.numpy(), columns=['loss'])

        final_df = pd.concat([df.reset_index(), probs_df, targs_df, losses_df], axis=1)

        final_df.to_csv(f"{config['learner_path']}/{yyyymmdd}_{full_model_name}_train_results.csv", index=False)
        return scores, final_df

    finally:
        # cleanup
        del learn; del dls
        del hf_arch; del hf_config; del hf_tokenizer; del hf_model

# Cell
def after_trial_cleanup(study, trial):
    gc.collect()
    torch.cuda.empty_cache()

# Cell
def objective(trial, yyyymmdd = datetime.today().strftime("%Y%m%d"), train_config_updates={}):
    opt_params = {
        'pretrained_model_name': trial.suggest_categorical("pretrained_model_name", ["facebook/bart-base"]),

        'save_model': trial.suggest_categorical("save_model", [True, False]),
        'use_weighted_loss': trial.suggest_categorical("use_weighted_loss", [True, False]),
        'use_fp16': trial.suggest_categorical("use_fp16", [True]),
        'n_frozen_epochs': trial.suggest_int("n_frozen_epochs", 1, 3),
        'n_unfrozen_epochs': trial.suggest_int("n_unfrozen_epochs", 0, 10),
        'frozen_lr': trial.suggest_loguniform("frozen_lr", 1e-4, 1e-3),
        'unfrozen_lr_max': trial.suggest_loguniform("unfrozen_lr_max", 1e-7, 1e-6),
        'unfrozen_lr_min': trial.suggest_loguniform("unfrozen_lr_min", 1e-9, 1e-7),
        'optimize_for': 'valid_loss',

        'facebook/bart-base_config_overrides': {
            'activation_dropout': trial.suggest_discrete_uniform('activation_dropout', 0.0, 0.3, 0.05),
            'attention_dropout': trial.suggest_discrete_uniform('attention_dropout', 0.0, 0.3, 0.05),
            'classif_dropout': trial.suggest_discrete_uniform('classif_dropout', 0.0, 0.3, 0.05),
            'dropout': trial.suggest_discrete_uniform('dropout', 0.0, 0.3, 0.05)
        },
        'roberta-base_config_overrides': {
            'attention_probs_dropout_prob': trial.suggest_discrete_uniform('attention_probs_dropout_prob', 0.0, 0.3, 0.05),
            'hidden_dropout_prob': trial.suggest_discrete_uniform('hidden_dropout_prob', 0.0, 0.3, 0.05)
        }
    }

    score = train(opt_params, trial=trial, yyyymmdd=yyyymmdd, train_config_updates=train_config_updates)
    return score

# Cell
def get_preds(inf_df, yyyymmdd=None, learner_export_path=None, train_scores_path=None,
              device=torch.device('cpu'), train_config_updates={}):

    config = {**train_config, **train_config_updates}
    m_pre, m_suf, base_model_name = config['m_pre'], config['m_suf'], config['base_model_name']
    full_model_name = f'{m_pre}{base_model_name}{m_suf}'

    # 1. grab learner, procs, and data
    cpu = device.type == 'cpu'

    if (yyyymmdd is None and learner_export_path is None):
        export_dir = Path(config['learner_path'])
        learner_export_path = sorted(export_dir.glob(f"[0-9]*_{config['export_filename']}"), reverse=True)[0]
        train_scores_path = export_dir/f"{learner_export_path.stem.split('_')[0]}_{full_model_name}_train_scores.json"

    if (learner_export_path is None):
        learner_export_path = f"{config['learner_path']}/{yyyymmdd}_{config['export_filename']}"

    if (train_scores_path is None):
        train_scores_path = f"{config['learner_path']}/{yyyymmdd}_{full_model_name}_train_scores.json"

    with open(train_scores_path) as f: training_results = json.load(f)

    inf_learn = load_learner(fname=learner_export_path, cpu=cpu)
    inf_learn.model = inf_learn.model.to(device)
    inf_learn.model = inf_learn.model.eval()

    # 2. define a suitable dataloader
    inf_df = inf_df.copy()
    inf_df[config['txt_cols']] = inf_df[config['orig_txt_cols']]
    inf_df.dropna(subset=config['txt_cols'], inplace=True)
    inf_df.reset_index(drop=True, inplace=True)
    inf_dl = inf_learn.dls.test_dl(inf_df, rm_type_tfms=None, bs=16)

    # 3. get probs and document vectors
    test_probs_sent, test_probs_is_example = [], []
    with torch.no_grad():
        for index, b in enumerate(inf_dl):
            if index % 1000 == 0:  print(index)

            # note: even though there is no targets, each batch is a tuple!
            probs = inf_learn.model(b[0])[0]

            # why "detach"? the computation of gradients wrt the weights of netG can be fully
            # avoided in the backward pass if the graph is detached where it is.
            test_probs_sent.append(to_detach(probs[0]))
            test_probs_is_example.append(to_detach(torch.softmax(probs[1], dim=-1)))

    all_probs_sent = L(torch.cat(test_probs_sent))
    all_probs_is_example = L(torch.cat(test_probs_is_example))

    # 4. ensure results are returned in order
    # test_dl.get_idxs() => unsorted/original order items
    all_probs_sent = all_probs_sent[0][np.argsort(inf_dl.get_idxs())]
    all_probs_is_example = all_probs_is_example[0][np.argsort(inf_dl.get_idxs())]

    # 5. return results with scores in a df, probs, and labels
    combined_probs = np.concatenate((all_probs_sent.numpy(), all_probs_is_example.numpy()[:,1][:,None]), axis=1)
    prob_labels = ['prob_' + lbl for lbl in STANDARD_THEME_META_LABELS]
    probs_df = pd.DataFrame(combined_probs, columns=prob_labels)

    for lbl in STANDARD_THEME_META_LABELS[1:]:
        probs_df[f'pred_{lbl}'] = (probs_df[f'prob_{lbl}'] > training_results['is_example_f05']['threshold']).astype(np.int64)

    final_df = pd.concat([inf_df, probs_df], axis=1)
    final_df.drop(columns=config['txt_cols'], inplace=True)

#     final_df['valid_loss'] = training_results['valid_loss']
#     final_df['sentiment_mse'] = training_results['sentiment']['mse']
#     final_df['sentiment_mae'] = training_results['sentiment']['mae']
#     final_df['sentiment_rmse'] = training_results['sentiment']['rmse']
#     final_df['is_example_f05_threshold'] = training_results['is_example_f05']['threshold']
#     final_df['is_example_f05_score'] = training_results['is_example_f05']['score']
#     final_df['is_example_f1_threshold'] = training_results['is_example_f1']['threshold']
#     final_df['is_example_f1_score'] = training_results['is_example_f1']['score']
#     final_df['is_example_f2_threshold'] = training_results['is_example_f2']['threshold']
#     final_df['is_example_f2_score'] = training_results['is_example_f2']['score']

    # cleanup
    try: del inf_learn; del inf_dl
    except: pass
    finally: gc.collect(); torch.cuda.empty_cache()

    return final_df, Path(learner_export_path).stem, training_results, STANDARD_THEME_META_LABELS

# Cell
def build_meta_inf_df(themes_df, theme_prob_threshold= 0.5, fixed_cols=None):
    inf_df = themes_df.copy()

    if (fixed_cols is None):
        fixed_cols = [ col for col in inf_df.columns if not col.startswith('pred_') and not col.startswith('prob_') ]

    prob_theme_cols = filter_col = [col for col in inf_df if col.startswith('prob_')]

    inf_df = inf_df.melt(id_vars=fixed_cols, value_vars=prob_theme_cols, var_name='theme', value_name='theme_prob')
    inf_df = inf_df.loc[inf_df.theme_prob >= theme_prob_threshold]

    inf_df['url_friendly_theme'] = inf_df.theme.apply(
        lambda s: re.sub("(.*?)_([a-zA-Z])","\g<1> \g<2>",s).replace('prob', '').strip().title().replace(' ',''))

    inf_df['theme'] = inf_df.url_friendly_theme.apply(lambda s: re.sub("([a-z])([A-Z])","\g<1> \g<2>",s))

    inf_df.reset_index(inplace=True)
    return inf_df