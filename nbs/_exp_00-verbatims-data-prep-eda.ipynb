{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tritonlytics Verbatims Data Prep & EDA\n",
    "\n",
    "Prepare the verbatims data for models and perform exploratory data analysis (EDA).  This notebook will generate the necessary files for both our Language Model and Classification work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html, pdb, requests\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tritonlytics_ai.utils import *\n",
    "from sklearn import model_selection\n",
    "from fastai import __version__ as fa2_version\n",
    "from fastai.text.all import *\n",
    "\n",
    "# from fastai import *        \n",
    "# from fastai.text import *  \n",
    "\n",
    "# from tritonlytics import Metrics as metrics_util, DataGeneration as dg_util, PandasUtil as pd_util\n",
    "# from tritonlytics.evaluation import *\n",
    "# from tritonlytics.callbacks import RocAucEvaluation\n",
    "\n",
    "# import dill as pickle\n",
    "\n",
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_es = spacy.load('es_core_news_sm')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# pandas and plotting config\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai version: 2.0.12\n"
     ]
    }
   ],
   "source": [
    "print(f'fastai version: {fa2_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name(torch.cuda.current_device())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download  latest training data\n",
    "\n",
    "Note: LM training is too big so it is uploaded as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbatims_clean_filename = 'verbatims-clean.csv'\n",
    "verbatims_raw_filename = 'verbatims-raw.csv'\n",
    "\n",
    "model_types = [\n",
    "    'verbatim-classification-sentiment',\n",
    "    'verbatim-classification-metadata-standardthemes',\n",
    "    'verbatim-classification-css-themes',\n",
    "    'verbatim-classification-saw-themes',\n",
    "    'verbatim-summarization-adhoc-themes'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_type in model_types:\n",
    "#     response = requests.get(f'https://tritonlytics-admin/api/ml/models/{model_type}/training-data')\n",
    "#     data = response.json()\n",
    "    \n",
    "#     pd.DataFrame(data).to_csv(RAW_DATA_PATH/f'{model_type}-raw.csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chunksize = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm_cols = list(TASK_LM_DTYPES.keys())\n",
    "                   \n",
    "df = pd.read_csv(RAW_DATA_PATH/verbatims_raw_filename, dtype=TASK_LM_DTYPES, parse_dates=[])\n",
    "df = df[lm_cols]\n",
    "\n",
    "display(len(df))\n",
    "display(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd_advanced_describe(df, include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['Language'] == 'English', lm_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**NOTES** \n",
    "- *(wtg3 2/22/2019): Removing this for now as it hasn't proved useful*\n",
    "\n",
    "Add clean (stop word removed), lemmatized, and clean_lemmatized columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spacy_models = defaultdict(lambda: None, { 'English': spacy_en, 'Spanish': spacy_es })\n",
    "lang_cols = defaultdict(lambda: 'AnswerText_NonEnglish', { 'English': 'AnswerText'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_cols(df, id_cols=['id'], lang_col='lang'):\n",
    "    rows = []\n",
    "    \n",
    "    pbar = progress_bar(df.iterrows(), total=len(df))\n",
    "    for index, row in pbar:\n",
    "        # get the correct spacy model for the language\n",
    "        spacy_fn = spacy_models[row[lang_col]]\n",
    "        if (spacy_fn == None): continue;\n",
    "            \n",
    "        # only process the text field if something is there\n",
    "        txt_col = lang_cols[row[lang_col]]\n",
    "        txt = str(row[txt_col])\n",
    "        if (txt == None): continue\n",
    "        \n",
    "        # grab tokens, entities, and word tokens\n",
    "        tokens = spacy_fn(txt)\n",
    "        ents = tokens.ents\n",
    "        words = [ token for token in tokens if (not token.is_punct) ]\n",
    "        \n",
    "        # will prepend dictionary of ids to both cols and ent_cols dicts\n",
    "        cols = OrderedDict({ el:row[el] for el in id_cols })\n",
    "        \n",
    "        # add different versions of text\n",
    "        cols[f'{txt_col}_Cleaned'] = ' '.join([ t.text for t in tokens if (not t.is_stop) ])\n",
    "        cols[f'{txt_col}_Lemmatized'] = ' '.join([ t.lemma_ for t in tokens ])\n",
    "        cols[f'{txt_col}_Cleaned_Lemmatized'] = ' '.join([ t.lemma_ for t in tokens if (not t.is_stop) ]) \n",
    "        \n",
    "        rows.append(cols)\n",
    "    \n",
    "    # overwrite any existing columns with new values\n",
    "    df = pd.merge(df, pd.DataFrame(rows, columns=cols.keys()), on=id_cols, suffixes=('_x', ''))\n",
    "    return df.loc[:, ~df.columns.str.endswith('_x')]\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df = add_cols(df, ['id'], 'language')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Create train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idxs = np.random.permutation(len(df))\n",
    "df = df.iloc[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_docs, val_docs = model_selection.train_test_split(df, test_size=0.1, random_state=42)\n",
    "len(trn_docs), len(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.concat([trn_docs, val_docs]).to_csv(LM_PATH/'all.csv', index=False)\n",
    "trn_docs[lm_cols].to_csv(LM_PATH/'train.csv', index=False)\n",
    "val_docs[lm_cols].to_csv(LM_PATH/'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sentiment classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chunksize = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_cols = list(TASK_LM_DTYPES_SC.keys()) + list(TASK_SENTIMENT_DTYPES.keys())\n",
    "               \n",
    "df = pd.read_json(RAW_DATA_PATH/'verbatim-classification-sentiment-raw.json', \n",
    "                  dtype={**TASK_LM_DTYPES, **TASK_SENTIMENT_DTYPES}, convert_dates=date_cols)\n",
    "df = df[sent_cols]\n",
    "\n",
    "display(len(df))\n",
    "display(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd_advanced_describe(df, include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ensure that all expected binary labels are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ensure binary labels are between 0 and 1\n",
    "df[SENT_LABELS[1:]] = df[SENT_LABELS[1:]].clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If a verbatim is tagged as very negative then it should also be considered negative (same with very positive), so ensure that is the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if IsVeryNegatve=1 then set IsNegative=1, same with IsVeryPositive and IsPositive\n",
    "df.loc[df.is_very_negative == 1, 'is_negative'] = 1\n",
    "df.loc[df.is_very_positive == 1, 'is_positive'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are going to programmatically determine Overall Sentiment (1-5) based on binary labels because reviewers weren't labeling this field to start with.  We'll want to revisit this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.overall_sentiment = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if OverallSentiment is null, use labels to populate\n",
    "df.loc[(pd.isna(df.overall_sentiment)) & \n",
    "       (df.is_very_positive == 1) & (df.is_very_negative == 0) & (df.is_negative == 0),\n",
    "        'overall_sentiment'] = 5\n",
    "        \n",
    "df.loc[(pd.isna(df.overall_sentiment)) & \n",
    "       (df.is_very_negative == 1) & (df.is_very_positive == 0) & (df.is_positive == 0),\n",
    "        'overall_sentiment'] = 1\n",
    "        \n",
    "df.loc[(pd.isna(df.overall_sentiment)) & \n",
    "        (df.is_very_positive == 0) & (df.is_positive == 1) & (df.is_very_negative == 0) & (df.is_negative == 0),\n",
    "        'overall_sentiment'] = 4\n",
    "        \n",
    "df.loc[(pd.isna(df.overall_sentiment)) & \n",
    "        (df.is_very_positive == 1) & (df.is_very_negative == 0) & (df.is_negative == 1),\n",
    "        'overall_sentiment'] = 4\n",
    "        \n",
    "df.loc[(pd.isna(df.overall_sentiment)) & \n",
    "        (df.is_very_positive == 0) & (df.is_positive == 0) & (df.is_very_negative == 0) & (df.is_negative == 1),\n",
    "        'overall_sentiment'] = 2\n",
    "        \n",
    "df.loc[(pd.isna(df.overall_sentiment)) & \n",
    "        (df.is_very_negative == 1) & (df.is_very_positive == 0) & (df.is_positive == 1),\n",
    "        'overall_sentiment'] = 2\n",
    "        \n",
    "# default to 3-neutral\n",
    "df.loc[(pd.isna(df.overall_sentiment)), 'overall_sentiment'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# display(df[df.overall_sentiment == 2.0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Create train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idxs = np.random.permutation(len(df))\n",
    "df = df.iloc[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_docs, val_docs = model_selection.train_test_split(df, test_size=0.1, random_state=42)\n",
    "len(trn_docs), len(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.concat([trn_docs, val_docs]).to_csv(SENTIMENT_CLS_PATH/'all.csv', index=False)\n",
    "trn_docs[sent_cols].to_csv(SENTIMENT_CLS_PATH/'train.csv', index=False)\n",
    "val_docs[sent_cols].to_csv(SENTIMENT_CLS_PATH/'test.csv', index=False)\n",
    "\n",
    "(SENTIMENT_CLS_PATH/'labels_sent.txt').open('w').writelines(f'{c}\\n' for c in SENT_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Standard Theme classification model - CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chunksize = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_cols = list(TASK_LM_DTYPES_SC.keys()) + list(TASK_STANDARD_THEME_CSS_DTYPES.keys())\n",
    "               \n",
    "df = pd.read_json(RAW_DATA_PATH/'verbatim-classification-css-themes-raw.json', \n",
    "                  dtype={**TASK_LM_DTYPES, **TASK_STANDARD_THEME_CSS_DTYPES}, convert_dates=date_cols)\n",
    "df = df[sent_cols]\n",
    "\n",
    "display(len(df))\n",
    "display(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd_advanced_describe(df, include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ensure that all expected binary labels are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ensure binary labels are between 0 and 1\n",
    "df[STANDARD_THEME_CSS_LABELS] = df[STANDARD_THEME_CSS_LABELS].clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Create train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idxs = np.random.permutation(len(df))\n",
    "df = df.iloc[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_docs, val_docs = model_selection.train_test_split(df, test_size=0.1, random_state=42)\n",
    "len(trn_docs), len(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.concat([trn_docs, val_docs]).to_csv(STANDARD_THEME_CSS_PATH/'all.csv', index=False)\n",
    "trn_docs[sent_cols].to_csv(STANDARD_THEME_CSS_PATH/'train.csv', index=False)\n",
    "val_docs[sent_cols].to_csv(STANDARD_THEME_CSS_PATH/'test.csv', index=False)\n",
    "\n",
    "( STANDARD_THEME_CSS_PATH/'labels_.txt').open('w').writelines(f'{c}\\n' for c in STANDARD_THEME_CSS_LABELS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Theme classification model - S@W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_cols = list(TASK_LM_DTYPES_SC.keys()) + list(TASK_STANDARD_THEME_SAW_DTYPES.keys())\n",
    "               \n",
    "df = pd.read_json(RAW_DATA_PATH/'verbatim-classification-saw-themes-raw.json', \n",
    "                  dtype={**TASK_LM_DTYPES, **TASK_STANDARD_THEME_SAW_DTYPES}, convert_dates=date_cols)\n",
    "df = df[sent_cols]\n",
    "\n",
    "display(len(df))\n",
    "display(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_advanced_describe(df, include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that all expected binary labels are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure binary labels are between 0 and 1\n",
    "df[STANDARD_THEME_SAW_LABELS] = df[STANDARD_THEME_SAW_LABELS].clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idxs = np.random.permutation(len(df))\n",
    "df = df.iloc[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_docs, val_docs = model_selection.train_test_split(df, test_size=0.1, random_state=42)\n",
    "len(trn_docs), len(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([trn_docs, val_docs]).to_csv(STANDARD_THEME_SAW_PATH/'all.csv', index=False)\n",
    "trn_docs[sent_cols].to_csv(STANDARD_THEME_SAW_PATH/'train.csv', index=False)\n",
    "val_docs[sent_cols].to_csv(STANDARD_THEME_SAW_PATH/'test.csv', index=False)\n",
    "\n",
    "( STANDARD_THEME_SAW_PATH/'labels.txt').open('w').writelines(f'{c}\\n' for c in STANDARD_THEME_SAW_LABELS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Theme classification model - Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_cols = list(TASK_LM_DTYPES_SC.keys()) + list(TASK_STANDARD_THEME_META_DTYPES.keys())\n",
    "               \n",
    "df = pd.read_json(RAW_DATA_PATH/'verbatim-classification-metadata-standardthemes-raw.json', \n",
    "                  dtype={**TASK_LM_DTYPES, **TASK_STANDARD_THEME_META_DTYPES}, convert_dates=date_cols)\n",
    "df = df[sent_cols]\n",
    "\n",
    "display(len(df))\n",
    "display(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_advanced_describe(df, include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that all expected binary labels are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure binary labels are between 0 and 1\n",
    "df.is_example = df.is_example.clip(0, 1)\n",
    "\n",
    "# default avg sentiment to 3 if null\n",
    "df.loc[(pd.isna(df.avg_sentiment)), 'avg_sentiment'] = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idxs = np.random.permutation(len(df))\n",
    "df = df.iloc[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_docs, val_docs = model_selection.train_test_split(df, test_size=0.1, random_state=42)\n",
    "len(trn_docs), len(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([trn_docs, val_docs]).to_csv(STANDARD_THEME_META_PATH/'all.csv', index=False)\n",
    "trn_docs[sent_cols].to_csv(STANDARD_THEME_META_PATH/'train.csv', index=False)\n",
    "val_docs[sent_cols].to_csv(STANDARD_THEME_META_PATH/'test.csv', index=False)\n",
    "\n",
    "( STANDARD_THEME_META_PATH/'labels.txt').open('w').writelines(f'{c}\\n' for c in STANDARD_THEME_META_LABELS )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(LM_PATH/'all.csv', dtype=TASK_LM_DTYPES)\n",
    "\n",
    "print(f'# Examples: {len(df)}')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df = pd.read_csv(SENTIMENT_CLS_PATH/'all.csv', dtype={**TASK_LM_DTYPES_SC, **TASK_SENTIMENT_DTYPES})\n",
    "trn_cls_df = pd.read_csv(SENTIMENT_CLS_PATH/'train.csv', dtype={**TASK_LM_DTYPES, **TASK_SENTIMENT_DTYPES})\n",
    "val_cls_df = pd.read_csv(SENTIMENT_CLS_PATH/'test.csv', dtype={**TASK_LM_DTYPES, **TASK_SENTIMENT_DTYPES})\n",
    "\n",
    "cls_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_sz, n_test_sz = len(trn_cls_df), len(val_cls_df)\n",
    "n_total = n_train_sz + n_test_sz\n",
    "\n",
    "print('\\tTrain\\tTest')\n",
    "print(f'Size:\\t{n_train_sz} | {n_test_sz}')\n",
    "print(f'%:\\t{round(n_train_sz / n_total, 2)}   | {round(n_test_sz / n_total, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the **class distribution**\n",
    "\n",
    "Define the labels we want to predict including a \"None\" label for those comments with 0's for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = list(TASK_SENTIMENT_DTYPES.keys())[1:]\n",
    "cls_df['None'] = 1 - cls_df[label_cols].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts_df = cls_df[label_cols + ['None']].apply(pd.Series.value_counts)\n",
    "display(value_counts_df)\n",
    "\n",
    "n_clean = value_counts_df.loc[1, \"None\"]\n",
    "n_labeled = value_counts_df.loc[0, \"None\"]\n",
    "p_labeled = round(n_labeled / len(cls_df), 2) * 100\n",
    "labeled_to_clean_ratio = round(n_labeled / n_clean, 2)\n",
    "\n",
    "print(f'Clean comments: {n_clean}')\n",
    "print(f'Labeled comments: {n_labeled}({p_labeled}%)')\n",
    "print(f'Ratio of labeled comments: {labeled_to_clean_ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = value_counts_df.iloc[1,:]\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title('# Occurences per label')\n",
    "plt.ylabel('# Occurences')\n",
    "plt.xlabel('Labels')\n",
    "\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "\n",
    "for rect, lbl in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, lbl, ha='center', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see to what degree **comments are tagged with multiple labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_labels_s = cls_df[label_cols].sum(axis=1).value_counts().sort_index()\n",
    "# display(mult_labels_s)\n",
    "\n",
    "x = mult_labels_s\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title('# Labels per comment')\n",
    "plt.ylabel('# of comments')\n",
    "plt.xlabel('# of labels')\n",
    "\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "\n",
    "for rect, lbl in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, lbl, ha='center', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how these **label correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = cls_df[label_cols].corr()\n",
    "# display(corr_df)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_df, xticklabels=corr_df.columns.values, yticklabels=corr_df.columns.values, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at **examples of each label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl in label_cols:\n",
    "    ex = cls_df[cls_df[lbl] == 1].answer_text.iloc[:2]\n",
    "    print(f'{lbl.upper()}:\\n{ex}\\n-----------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at **WordClouds for each label to identify the most frequent words for each label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl in label_cols:\n",
    "    df = cls_df[cls_df[lbl] == 1]\n",
    "    txt = df.answer_text.values\n",
    "    wc = WordCloud(background_color='black', max_words=4000, stopwords=spacy.lang.en.STOP_WORDS)\n",
    "    \n",
    "    # see: https://stackoverflow.com/a/10880820/54818 on why ' '.join(txt) doesn't work\n",
    "    wc.generate(\",\".join(map(str, txt)))\n",
    "    \n",
    "    plt.figure(figsize=(20,14))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Words in {lbl.upper()} comments', fontsize=20)\n",
    "    plt.imshow(wc.recolor(colormap='viridis', random_state=42), alpha=0.98)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
