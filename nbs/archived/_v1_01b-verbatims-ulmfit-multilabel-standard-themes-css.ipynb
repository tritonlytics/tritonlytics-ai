{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tritonlytics Multilabel Classification - Standard CSS Themes\n",
    "\n",
    "Experiments related to building a LM and multilabel classification model for survey comments captured in the Tritonlytics survey delivery system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *   # Quick accesss to NLP functionality\n",
    "from fastai.callbacks import *\n",
    "\n",
    "import pdb\n",
    "from tritonlytics import Metrics as metrics_util, DataGeneration as dg_util, PandasUtil as pd_util\n",
    "from tritonlytics.evaluation import *\n",
    "from tritonlytics.callbacks import RocAucEvaluation\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_es = spacy.load('es')\n",
    "\n",
    "# pandas and plotting config\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'fastai version: {__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convert_to_snakecase(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower().replace('__', '_') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view : apostrophe lookup dict\n",
    "appos_regex_repl = {\n",
    "    r\"\\baren't\\b\" : \"are not\",\n",
    "    r\"\\bcan't\\b\" : \"cannot\",\n",
    "    r\"\\bcouldn't\\b\" : \"could not\",\n",
    "    r\"\\bdidn't\\b\" : \"did not\",\n",
    "    r\"\\bdoesn't\\b\" : \"does not\",\n",
    "    r\"\\bdon't\\b\" : \"do not\",\n",
    "    r\"\\bhadn't\\b\" : \"had not\",\n",
    "    r\"\\bhasn't\\b\" : \"has not\",\n",
    "    r\"\\bhaven't\\b\" : \"have not\",\n",
    "    r\"\\bhe'd\\b\" : \"he would\",\n",
    "    r\"\\bhe'll\\b\" : \"he will\",\n",
    "    r\"\\bhe's\\b\" : \"he is\",\n",
    "    r\"\\bi'd\\b\" : \"I would\",\n",
    "    r\"\\bi'd\\b\" : \"I had\",\n",
    "    r\"\\bi'll\\b\" : \"I will\",\n",
    "    r\"\\bi'm\\b\" : \"I am\",\n",
    "    r\"\\bisn't\\b\" : \"is not\",\n",
    "    r\"\\bits\\b\" : \"it is\",\n",
    "    r\"\\bit's\\b\" : \"it is\",\n",
    "    r\"\\bit'll\\b\" : \"it will\",\n",
    "    r\"\\bi've\\b\" : \"I have\",\n",
    "    r\"\\blet's\\b\" : \"let us\",\n",
    "    r\"\\bmightn't\\b\" : \"might not\",\n",
    "    r\"\\bmustn't\\b\" : \"must not\",\n",
    "    r\"\\bshan't\\b\" : \"shall not\",\n",
    "    r\"\\bshe'd\\b\" : \"she would\",\n",
    "    r\"\\bshe'll\\b\" : \"she will\",\n",
    "    r\"\\bshe's\\b\" : \"she is\",\n",
    "    r\"\\bshouldn't\\b\" : \"should not\",\n",
    "    r\"\\bthat's\\b\" : \"that is\",\n",
    "    r\"\\bthere's\\b\" : \"there is\",\n",
    "    r\"\\bthey'd\\b\" : \"they would\",\n",
    "    r\"\\bthey'll\\b\" : \"they will\",\n",
    "    r\"\\bthey're\\b\" : \"they are\",\n",
    "    r\"\\bthey've\\b\" : \"they have\",\n",
    "    r\"\\bwe'd\\b\" : \"we would\",\n",
    "    r\"\\bwe're\\b\" : \"we are\",\n",
    "    r\"\\bweren't\\b\" : \"were not\",\n",
    "    r\"\\bwe've\\b\" : \"we have\",\n",
    "    r\"\\bwhat'll\\b\" : \"what will\",\n",
    "    r\"\\bwhat're\\b\" : \"what are\",\n",
    "    r\"\\bwhat's\\b\" : \"what is\",\n",
    "    r\"\\bwhat've\\b\" : \"what have\",\n",
    "    r\"\\bwhere's\\b\" : \"where is\",\n",
    "    r\"\\bwho'd\\b\" : \"who would\",\n",
    "    r\"\\bwho'll\\b\" : \"who will\",\n",
    "    r\"\\bwho're\\b\" : \"who are\",\n",
    "    r\"\\bwho's\\b\" : \"who is\",\n",
    "    r\"\\bwho've\\b\" : \"who have\",\n",
    "    r\"\\bwon't\\b\" : \"will not\",\n",
    "    r\"\\bwouldn't\\b\" : \"would not\",\n",
    "    r\"\\byou'd\\b\" : \"you would\",\n",
    "    r\"\\byou'll\\b\" : \"you will\",\n",
    "    r\"\\byou're\\b\" : \"you are\",\n",
    "    r\"\\byou've\\b\" : \"you have\",\n",
    "    r\"\\b're\\b\" : \" are\",\n",
    "    r\"\\bwasn't\\b\" : \"was not\",\n",
    "    r\"\\bwe'll\\b\" : \"will\",\n",
    "    r\"\\bdidn't\\b\" : \"did not\",\n",
    "    r\"\\btryin'\\b\" : \"trying\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# based on https://www.kaggle.com/prashantkikani/pooled-gru-with-preprocessing\n",
    "emoji_str_repls = {\n",
    "    \"&lt;3\": \" love \",\n",
    "    \":]\" : \" happy \",\n",
    "    \"=)\" : \" happy \",\n",
    "    \"8)\": \" happy \",\n",
    "    \":-)\": \" happy \",\n",
    "    \":)\": \" happy \",\n",
    "    \"(-:\": \" happy \",\n",
    "    \"(:\": \" happy \",\n",
    "    \":&gt;\": \" happy \",\n",
    "    \":')\": \" happy \",\n",
    "    \"(:\" : \" happy \",\n",
    "    \":d\": \" laughing \",\n",
    "    \":dd\": \" laughing \",\n",
    "    \";-)\" : \" wink \",\n",
    "    \";)\": \" wink \",\n",
    "    \":p\": \" playful \",\n",
    "    \":o\" : \" surprise \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \"=(\" : \" sad \",\n",
    "    \"):\" : \" sad \",\n",
    "    \":/\": \" skeptical \",\n",
    "    \":s\": \" skeptical \",\n",
    "    \":-s\": \" skeptical \",\n",
    "    \"^^\": \" nervous \",\n",
    "    \"^_^\": \" nervous \",\n",
    "    \"-_-\" : \" shame \",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spelling_regex_repls = {\n",
    "    # abbreviations\n",
    "    r\"\\bacctg\\b\" : \"acct\",\n",
    "    r\"\\badd'l\\b\" : \"additional\",\n",
    "    r\"\\br\\s\\b\": \"are\",\n",
    "    r\"\\bu\\s\\b\": \"you \",\n",
    "    r\"\\b\\sm\\s\\b \": \"am\",\n",
    "    r\"'cause\\b\" : \"because\",\n",
    "    r\"\\b(ha)+\\b\": \"haha\",\n",
    "    r\"\\b(he)+\\b\": \"haha\",\n",
    "    r\"\\bya+y\\b\": \"yay\",\n",
    "    r\"\\bwa+y\\b\": \"way\",\n",
    "    r\"\\bf'real\\b\" : \"for real\",\n",
    "    r\"\\bgr8\\b\" : \"great\",\n",
    "    r\"\\bintl\\b\" : \"int'l\",\n",
    "    # common misspellings\n",
    "    r\"\\bbailable\\b\" : \"available\",\n",
    "    r\"\\babilty\\b\" : \"ability\",\n",
    "    r\"\\babsolutly\\b\" : \"absolutely\",\n",
    "    r\"\\babsoultely\\b\" : \"absolutely\",\n",
    "    r\"\\bacces\\b\" : \"access\",\n",
    "    r\"\\baccesability\\b\" : \"accessibility\",\n",
    "    r\"\\baccesbility\\b\" : \"accessibility\",\n",
    "    r\"\\baccesibility\\b\" : \"accessibility\",\n",
    "    r\"\\baccessability\\b\" : \"accessibility\",\n",
    "    r\"\\baccessbility\\b\" : \"accessibility\",\n",
    "    r\"\\baccesable\\b\" : \"accessible\",\n",
    "    r\"\\baccesible\\b\" : \"accessible\",\n",
    "    r\"\\baccessable\\b\" : \"accessible\",\n",
    "    r\"\\bacessible\\b\" : \"accessible\",\n",
    "    r\"\\bassessable\\b\" : \"availability\",\n",
    "    r\"\\baccidently\\b\" : \"accidentally\",\n",
    "    r\"\\baccomadate\\b\" : \"accommodate\",\n",
    "    r\"\\baccomdate\\b\" : \"accommodate\",\n",
    "    r\"\\baccomidate\\b\" : \"accommodate\",\n",
    "    r\"\\baccomodate\\b\" : \"accommodate\",\n",
    "    r\"\\baccomadating\\b\" : \"accommodating\",\n",
    "    r\"\\baccomidating\\b\" : \"accommodating\",\n",
    "    r\"\\baccomodating\\b\" : \"accommodating\",\n",
    "    r\"\\baccomadations\\b\" : \"accommodations\",\n",
    "    r\"\\baccomodation\\b\" : \"accommodation\",\n",
    "    r\"\\baccouting\\b\" : \"accounting\",\n",
    "    r\"\\baccross\\b\" : \"across\",\n",
    "    r\"\\badd'l\\b\" : \"additional\",\n",
    "    r\"\\badditonal\\b\" : \"additional\",\n",
    "    r\"\\baddtionally\\b\" : \"additionally\",\n",
    "    r\"\\badminstration\\b\" : \"administration\",\n",
    "    r\"\\badminstrative\\b\" : \"administrative\",\n",
    "    r\"\\badminstrator\\b\" : \"administrator\",\n",
    "    r\"\\badress\\b\" : \"address\",\n",
    "    r\"\\badvancment\\b\" : \"advancement\",\n",
    "    r\"\\badvertized\\b\" : \"advertised\",\n",
    "    r\"\\bafforable\\b\" : \"affordable\",\n",
    "    r\"\\bafordable\\b\" : \"affordable\",\n",
    "    r\"\\bafterall\\b\" : \"after all\",\n",
    "    r\"\\bafterhours\\b\" : \"after hours\",\n",
    "    r\"\\baggresive\\b\" : \"aggressive\",\n",
    "    r\"\\bagressive\\b\" : \"aggressive\",\n",
    "    r\"\\bagressions\\b\" : \"aggressions\",\n",
    "    r\"\\balittle\\b\" : \"a little\",\n",
    "    r\"\\balll\\b\" : \"all\",\n",
    "    r\"\\balloted\\b\" : \"allotted\",\n",
    "    r\"\\ballthough\\b\" : \"although\",\n",
    "    r\"\\balthought\\b\" : \"although\",\n",
    "    r\"\\ballways\\b\" : \"always\",\n",
    "    r\"\\balos\\b\" : \"also\",\n",
    "    r\"\\balot\\b\" : \"a lot\",\n",
    "    r\"\\balotted\\b\" : \"allotted\",\n",
    "    r\"\\bammount\\b\" : \"amount\",\n",
    "    r\"\\bammounts\\b\" : \"amounts\",\n",
    "    r\"\\bamoung\\b\" : \"among\",\n",
    "    r\"\\bamoungst\\b\" : \"amongst\",\n",
    "    r\"\\bannouncment\\b\" : \"announcement\",\n",
    "    r\"\\baparments\\b\" : \"apartments\",\n",
    "    r\"\\bapparrel\\b\" : \"apparel\",\n",
    "    r\"\\bappartment\\b\" : \"apartment\",\n",
    "    r\"\\bappriciate\\b\" : \"appreciate\",\n",
    "    r\"\\bassitance\\b\" : \"assistance\",\n",
    "    r\"\\bassitant\\b\" : \"assistant\",\n",
    "    r\"\\batleast\\b\" : \"at least\",\n",
    "    r\"\\battentative\\b\" : \"attentive\",\n",
    "    r\"\\battrocious\\b\" : \"atrocious\",\n",
    "    r\"\\bavaiable\\b\" : \"available\",\n",
    "    r\"\\bavaible\\b\" : \"available\",\n",
    "    r\"\\bavailabe\\b\" : \"available\",\n",
    "    r\"\\bavailble\\b\" : \"available\",\n",
    "    r\"\\bavailiable\\b\" : \"available\",\n",
    "    r\"\\bavailible\\b\" : \"available\",\n",
    "    r\"\\bavaliable\\b\" : \"available\",\n",
    "    r\"\\bavalible\\b\" : \"available\",\n",
    "    r\"\\bavilable\\b\" : \"available\",\n",
    "    r\"\\bavailiability\\b\" : \"availability\",\n",
    "    r\"\\bavailabiltiy\\b\" : \"availability\",\n",
    "    r\"\\bavailabilty\\b\" : \"availability\",\n",
    "    r\"\\bavailablility\\b\" : \"availability\",\n",
    "    r\"\\bavailablity\\b\" : \"availability\",\n",
    "    r\"\\bavailibility\\b\" : \"availability\",\n",
    "    r\"\\bavaliability\\b\" : \"availability\",\n",
    "    r\"\\bavaliablity\\b\" : \"availability\",\n",
    "    r\"\\bavalibility\\b\" : \"availability\",\n",
    "    r\"\\bactivies\\b\" : \"activities\",\n",
    "    r\"\\bactivites\\b\" : \"activities\",\n",
    "    r\"\\bactualy\\b\" : \"actually\",\n",
    "    r\"\\bacutally\\b\" : \"actually\",\n",
    "    r\"\\bammenities\\b\" : \"amenities\",\n",
    "    r\"\\bantoher\\b\" : \"another\",\n",
    "    r\"\\bassitant\\b\" : \"assistant\",\n",
    "    r\"\\baswell\\b\" : \"as well\",\n",
    "    r\"\\baweful\\b\" : \"awful\",\n",
    "    r\"\\bawfull\\b\" : \"awful\",\n",
    "    r\"\\bawsome\\b\" : \"awesome\",\n",
    "    r\"\\bbeacuse\\b\" : \"because\",\n",
    "    r\"\\bbearly\\b\" : \"barely\",\n",
    "    r\"\\bbeaurocracy\\b\" : \"bureaucracy\",\n",
    "    r\"\\bbeaurocratic\\b\" : \"bureaucratic\",\n",
    "    r\"\\bbecasue\\b\" : \"because\",\n",
    "    r\"\\bbecuase\\b\" : \"because\",\n",
    "    r\"\\bbecuse\\b\" : \"because\",\n",
    "    r\"\\bbefor\\b\" : \"before\",\n",
    "    r\"\\bbeggining\\b\" : \"beginning\",\n",
    "    r\"\\bbegining\\b\" : \"beginning\",\n",
    "    r\"\\bbeleive\\b\" : \"believe\",\n",
    "    r\"\\bbelive\\b\" : \"believe\",\n",
    "    r\"\\bbenificial\\b\" : \"beneficial\",\n",
    "    r\"\\bbenifit\\b\" : \"benefit\",\n",
    "    r'\\bbugetary\\b' : \"budgetary\",\n",
    "    r'\\bbuiding\\b' : \"building\",\n",
    "    r'\\bbuidling\\b' : \"building\",\n",
    "    r'\\bbuisness\\b' : \"business\",\n",
    "    r'\\bbuliding\\b' : \"building\",\n",
    "    r\"\\bbureacracy\\b\" : \"bureaucracy\",\n",
    "    r\"\\bburitto\\b\" : \"burrito\",\n",
    "    r\"\\bbussiness\\b\" : \"business\",\n",
    "    r\"\\bcalender\\b\" : \"calendar\",\n",
    "    r\"\\bcan;t\\b\" : \"can't\",\n",
    "    r\"\\bcasher\\b\" : \"cashier\",\n",
    "    r'\\bcatagories\\b' : \"categories\",\n",
    "    r'\\bcatagory\\b' : \"category\",\n",
    "    r\"\\bcheapter\\b\" : \"cheaper\",\n",
    "    r\"\\bcheeper\\b\" : \"cheaper\",\n",
    "    r'\\bclasss\\b' : \"clas\",\n",
    "    r'\\bclassses\\b' : \"classes\",\n",
    "    r\"\\bcleaniness\\b\" : \"cleanliness\",\n",
    "    r\"\\bcmapus\\b\" : \"campus\",\n",
    "    r'\\bcofee\\b' : \"coffee\",\n",
    "    r'\\bcoffe\\b' : \"coffee\",\n",
    "    r'\\bcollegue\\b' : \"colleague\",\n",
    "    r'\\bcoment\\b' : \"comment\",\n",
    "    r'\\bcoments\\b' : \"comments\",\n",
    "    r'\\bcomming\\b' : \"coming\",\n",
    "    r'\\bcommittment\\b' : \"commitment\",\n",
    "    r'\\bcommment\\b' : \"comment\",\n",
    "    r'\\bcommuication\\b' : \"communication\",\n",
    "    r'\\bcommunter\\b' : \"commuter\",\n",
    "    r'\\bcommunters\\b' : \"commuters\",\n",
    "    r'\\bcomotion\\b' : \"commotion\",\n",
    "    r'\\bcomparision\\b' : \"comparison\",\n",
    "    r'\\bcompatability\\b' : \"compatibility\",\n",
    "    r'\\bcompatable\\b' : \"compatible\",\n",
    "    r'\\bcompetative\\b' : \"competitive\",\n",
    "    r'\\bcompetetive\\b' : \"competitive\",\n",
    "    r'\\bcompetive\\b' : \"competitive\",\n",
    "    r'\\bcompletly\\b' : \"completely\",\n",
    "    r\"\\bcomraderie\\b\" : \"camaraderie\",\n",
    "    r'\\bcomradery\\b' : \"camaraderie\",\n",
    "    r'\\bcomunication\\b' : \"communication\",\n",
    "    r'\\bcomunity\\b' : \"community\",\n",
    "    r'\\bconcious\\b' : \"conscious\",\n",
    "    r'\\bcondusive\\b' : \"conducive\",\n",
    "    r'\\bconection\\b' : \"connection\",\n",
    "    r\"\\bconfortable\\b\" : \"comfortable\",\n",
    "    r'\\bconsistant\\b' : \"consistent\",\n",
    "    r'\\bconsistantly\\b' : \"consistently\",\n",
    "    r'\\bconsistenly\\b' : \"consistently\",\n",
    "    r'\\bcontinously\\b' : \"continuously\",\n",
    "    r'\\bcontruction\\b' : \"construction\",\n",
    "    r'\\bconveinent\\b' : \"convenient\",\n",
    "    r'\\bconveinient\\b' : \"convenient\",\n",
    "    r'\\bconveniant\\b' : \"convenient\",\n",
    "    r'\\bconveniece\\b' : \"convenience\",\n",
    "    r'\\bconveninent\\b' : \"convenient\",\n",
    "    r'\\bconvienance\\b' : \"convenience\",\n",
    "    r'\\bconvienant\\b' : \"convenient\",\n",
    "    r'\\bconvience\\b' : \"convenience\",\n",
    "    r'\\bconvienence\\b' : \"convenience\",\n",
    "    r'\\bconvienent\\b' : \"convenient\",\n",
    "    r'\\bconvienet\\b' : \"convenient\",\n",
    "    r'\\bconvienience\\b' : \"convenience\",\n",
    "    r'\\bconvienient\\b' : \"convenient\",\n",
    "    r'\\bconvient\\b' : \"convenient\",\n",
    "    r'\\bconviently\\b' : \"conveniently\",\n",
    "    r'\\bconvinence\\b' : \"convenience\",\n",
    "    r'\\bconvinent\\b' : \"convenient\",\n",
    "    r'\\bconvinience\\b' : \"convenience\",\n",
    "    r'\\bconvinient\\b' : \"convenient\",\n",
    "    r'\\bcorteous\\b' : \"courteous\",\n",
    "    r'\\bcostodial\\b' : \"custodial\",\n",
    "    r'\\bcoureous\\b' : \"courteous\",\n",
    "    r'\\bcourtis\\b' : \"courteous\",\n",
    "    r'\\bcouteous\\b' : \"courteous\",\n",
    "    r'\\bcovenient\\b' : \"convenient\",\n",
    "    r'\\bcroweded\\b' : \"crowded\",\n",
    "    r'\\bcurteous\\b' : \"courteous\",\n",
    "    r'\\bcurtesy\\b' : \"courtesy\",\n",
    "    r'\\bcurtious\\b' : \"courteous\",\n",
    "    r\"\\bdeaprtment\\b\" : \"department\",\n",
    "    r\"\\bdecission\\b\" : \"decision\",\n",
    "    r'\\bdefinately\\b' : \"definitely\",\n",
    "    r'\\bdefinetely\\b' : \"definitely\",\n",
    "    r'\\bdefinetly\\b' : \"definitely\",\n",
    "    r'\\bdefinitley\\b' : \"definitely\",\n",
    "    r'\\bdefinitly\\b' : \"definitely\",\n",
    "    r'\\bdelievered\\b' : \"delivered\",\n",
    "    r'\\bdeliverers\\b' : \"deliveries\",\n",
    "    r'\\bdeparment\\b' : \"department\",\n",
    "    r'\\bdeparments\\b' : \"department\",\n",
    "    r'\\bdepartement\\b' : \"department\",\n",
    "    r\"\\bdepartment\\(s\\b\" : \"departments\",\n",
    "    r'\\bdepartmet\\b' : \"department\",\n",
    "    r'\\bdepratment\\b' : \"department\",\n",
    "    r\"\\bdeptartment\\b\" : \"department\",\n",
    "    r'\\bdescrimination\\b' : \"discrimination\",\n",
    "    r'\\bdesireable\\b' : \"desirable\",\n",
    "    r\"\\bdiffernt\\b\" : \"different\",\n",
    "    r\"\\bdiffrent\\b\" : \"different\",\n",
    "    r'\\bdinig\\b' : \"dining\",\n",
    "    r'\\bdirverse\\b' : \"diverse\",\n",
    "    r'\\bdisapointed\\b' : \"disappointed\",\n",
    "    r'\\bdisapointing\\b' : \"disappointing\",\n",
    "    r'\\bdisasterous\\b' : \"disastrous\",\n",
    "    r'\\bdisatisfied\\b' : \"dissatisfied\",\n",
    "    r'\\bdisbursment\\b' : \"disbursement\",\n",
    "    r'\\bdisbursments\\b' : \"disbursements\",\n",
    "    r'\\bdiscretely\\b' : \"discreetly\",\n",
    "    r'\\bdiscusting\\b' : \"disgusting\",\n",
    "    r'\\bdisfunctional\\b' : \"dysfunctional\",\n",
    "    r'\\bdispensors\\b' : \"dispensers\",\n",
    "    r'\\bdispersement\\b' : \"disbursement\",\n",
    "    r'\\bdissapointed\\b' : \"disappointed\",\n",
    "    r'\\bdissapointing\\b' : \"disappointing\",\n",
    "    r'\\bdissapointment\\b' : \"disappointment\",\n",
    "    r'\\bdissappointed\\b' : \"disappointed\",\n",
    "    r'\\bdissappointing\\b' : \"disappointing\",\n",
    "    r'\\bdissatified\\b' : \"dissatisfied\",\n",
    "    r'\\bdiveristy\\b' : \"diversity\",\n",
    "    r'\\bdivison\\b' : \"division\",\n",
    "    r'\\bdivsion\\b' : \"division\",\n",
    "    r\"\\bdoens't\\b\" : \"doesn't\",\n",
    "    r\"\\bdoes't\\b\" : \"doesn't\",\n",
    "    r\"\\bdoesn;t\\b\" : \"doesn't\",\n",
    "    r\"\\bdon;t\\b\" : \"don't\",\n",
    "    r'\\bdonot\\b' : \"do not\",\n",
    "    r\"\\bdosen't\\b\" : \"doesn't\",\n",
    "    r\"\\bdosent\\b\" : \"doesn't\",\n",
    "    r'\\bdumbells\\b' : \"dumbbells\",\n",
    "    r'\\bdurring\\b' : \"during\",\n",
    "    r\"\\beatting\\b\" : \"eating\",\n",
    "    r\"\\beduation\\b\" : \"education\",\n",
    "    r'\\beffeciency\\b' : \"efficiency\",\n",
    "    r'\\beffecient\\b' : \"efficient\",\n",
    "    r'\\befficency\\b' : \"efficiency\",\n",
    "    r'\\befficent\\b' : \"efficient\",\n",
    "    r'\\beffiecient\\b' : \"efficient\",\n",
    "    r'\\beimplying\\b' : \"implying\",\n",
    "    r'\\bembarassed\\b' : \"embarrassed\",\n",
    "    r'\\bembarassing\\b' : \"embarrassing\",\n",
    "    r'\\bembarassment\\b' : \"embarrassment\",\n",
    "    r'\\bemploee\\b' : \"employee\",\n",
    "    r'\\bemploye\\b' : \"employee\",\n",
    "    r'\\bemployee\\(s\\b' : \"employees\",\n",
    "    r'\\bemployeed\\b' : \"employed\",\n",
    "    r'\\bemployement\\b' : \"employment\",\n",
    "    r'\\bemployes\\b' : \"employees\",\n",
    "    r'\\bemployess\\b' : \"employees\",\n",
    "    r'\\bemplyee\\b' : \"employee\",\n",
    "    r'\\bemplyees\\b' : \"employees\",\n",
    "    r'\\bempolyees\\b' : \"employees\",\n",
    "    r'\\bencoutered\\b' : \"encountered\",\n",
    "    r'\\benought\\b' : \"enough\",\n",
    "    r'\\benrollement\\b' : \"enrollment\",\n",
    "    r'\\benviorment\\b' : \"environment\",\n",
    "    r'\\benviornment\\b' : \"environment\",\n",
    "    r'\\benvirnment\\b' : \"environment\",\n",
    "    r'\\benviroment\\b' : \"environment\",\n",
    "    r'\\benvironement\\b' : \"environment\",\n",
    "    r'\\bequiped\\b' : \"equipped\",\n",
    "    r'\\bespcially\\b' : \"especially\",\n",
    "    r'\\bespecailly\\b' : \"especially\",\n",
    "    r'\\bespecialy\\b' : \"especially\",\n",
    "    r'\\bespeically\\b' : \"especially\",\n",
    "    r\"\\besthetically\\b\" : \"aesthetically \",\n",
    "    r\"\\bethinicity\\b\" : \"ethnicity\",\n",
    "    r\"\\bevaulation\\b\" : \"evaluation\",\n",
    "    r\"\\beventhough\\b\" : \"even though\",\n",
    "    r'\\beverday\\b' : \"every day\",\n",
    "    r'\\beverthing\\b' : \"everything\",\n",
    "    r'\\beveryones\\b' : \"everyones\",\n",
    "    r'\\beverythings\\b' : \"everythings\",\n",
    "    r'\\beveryway\\b' : \"every way\",\n",
    "    r'\\beveyone\\b' : \"everyone\",\n",
    "    r'\\beveything\\b' : \"everything\",\n",
    "    r'\\bevrything\\b' : \"everything\",\n",
    "    r'\\bexcelent\\b' : \"excellent\",\n",
    "    r'\\bexcellant\\b' : \"excellent\",\n",
    "    r'\\bexellent\\b' : \"excellent\",\n",
    "    r'\\bexhorbitant\\b' : \"exorbitant\",\n",
    "    r'\\bexistance\\b' : \"existence\",\n",
    "    r'\\bexpecially\\b' : \"especially\",\n",
    "    r'\\bexpensice\\b' : \"expensive\",\n",
    "    r'\\bexpereince\\b' : \"experience\",\n",
    "    r'\\bexperiance\\b' : \"experience\",\n",
    "    r'\\bexperince\\b' : \"experience\",\n",
    "    r'\\bexpierence\\b' : \"experience\",\n",
    "    r'\\bexpirence\\b' : \"experience\",\n",
    "    r'\\bexplaination\\b' : \"explanation\",\n",
    "    r'\\bexremely\\b' : \"extremely\",\n",
    "    r'\\bextemely\\b' : \"extremely\",\n",
    "    r'\\bextention\\b' : \"extension\",\n",
    "    r'\\bextermely\\b' : \"extremely\",\n",
    "    r'\\bextreamly\\b' : \"extremely\",\n",
    "    r'\\bextrememly\\b' : \"extremely\",\n",
    "    r'\\bextremly\\b' : \"extremely\",\n",
    "    r\"\\bfacilites\\b\" : \"facilities\",\n",
    "    r'\\bfacilties\\b' : \"facilities\",\n",
    "    r'\\bfacilty\\b' : \"facility\",\n",
    "    r'\\bfaculity\\b' : \"faculty\",\n",
    "    r'\\bfacutly\\b' : \"faculty\",\n",
    "    r'\\bfiancial\\b' : \"financial\",\n",
    "    r\"\\bfinacial\\b\" : \"financial\",\n",
    "    r\"\\bfirendly\\b\" : \"friendly\",\n",
    "    r'\\bflexability\\b' : \"flexibility\",\n",
    "    r'\\bflexibilty\\b' : \"flexibility\",\n",
    "    r'\\bflexiblity\\b' : \"flexibility\",\n",
    "    r\"\\bflourescent\\b\" : \"fluorescent\",\n",
    "    r'\\bfreindly\\b' : \"friendly\",\n",
    "    r'\\bfreqency\\b' : \"frequency\",\n",
    "    r'\\bfreqent\\b' : \"frequent\",\n",
    "    r'\\bfriednly\\b' : \"friendly\",\n",
    "    r'\\bfrusterating\\b' : \"frustrating\",\n",
    "    r'\\bfrusturating\\b' : \"frustrating\",\n",
    "    r'\\bfustrating\\b' : \"frustrating\",\n",
    "    r'\\bgovenor\\b' : \"governor\",\n",
    "    r\"\\bgraffitti\\b\" : \"graffiti\",\n",
    "    r\"\\bgrafitti\\b\" : \"graffiti\",\n",
    "    r\"\\bgreatful\\b\" : \"grateful\",\n",
    "    r\"\\bguarenteed\\b\" : \"guaranteed\",\n",
    "    r\"\\bguidlines\\b\" : \"guidelines\",\n",
    "    r\"\\bguranteed\\b\" : \"guaranteed\",\n",
    "    r\"\\bhappend\\b\" : \"happened\",\n",
    "    r'\\bharrass\\b' : \"harass\",\n",
    "    r'\\bharrassed\\b' : \"harassed\",\n",
    "    r'\\bharrassing\\b' : \"harassing\",\n",
    "    r'\\bharrassment\\b' : \"harassment\",\n",
    "    r\"\\bhavn't\\b\" : \"haven't\",\n",
    "    r'\\bhealtheir\\b' : \"healthier\",\n",
    "    r'\\bhealthly\\b' : \"healthy\",\n",
    "    r'\\bhealtier\\b' : \"healthier\",\n",
    "    r'\\bhealty\\b' : \"healthy\",\n",
    "    r'\\bheathy\\b' : \"healthy\",\n",
    "    r'\\bheirarchy\\b' : \"hierarchy\",\n",
    "    r'\\bhelful\\b' : \"helpful\",\n",
    "    r'\\bhelpfull\\b' : \"helpful\",\n",
    "    r'\\bhelpul\\b' : \"helpful\",\n",
    "    r'\\bhighschool\\b' : \"high school\",\n",
    "    r'\\bhighschools\\b' : \"high schools\",\n",
    "    r'\\bhorendous\\b' : \"horrendous\",\n",
    "    r'\\bhorible\\b' : \"horrible\",\n",
    "    r'\\bhouseing\\b' : \"housing\",\n",
    "    r'\\bi\"m\\b' : \"i'm\",\n",
    "    r'\\bi\"ve\\b' : \"i've\",\n",
    "    r'\\bimplimented\\b' : \"implemented\",\n",
    "    r'\\bimporve\\b' : \"improve\",\n",
    "    r'\\bimposible\\b' : \"impossible\",\n",
    "    r'\\bimprovment\\b' : \"improvement\",\n",
    "    r'\\bimprovments\\b' : \"improvements\",\n",
    "    r'\\bincompetant\\b' : \"incompetent\",\n",
    "    r'\\binconsistant\\b' : \"inconsistent\",\n",
    "    r'\\binconveinent\\b' : \"nconvenient\",\n",
    "    r'\\binconvience\\b' : \"inconvenience\",\n",
    "    r'\\binconvienent\\b' : \"nconvenient\",\n",
    "    r'\\binconvienient\\b' : \"nconvenient\",\n",
    "    r'\\binconvient\\b' : \"nconvenient\",\n",
    "    r'\\binconvinient\\b' : \"nconvenient\",\n",
    "    r'\\bindentify\\b' : \"identify\",\n",
    "    r'\\bindependant\\b' : \"independent\",\n",
    "    r'\\bindividual\\(s\\b' : \"individuals\",\n",
    "    r'\\binforced\\b' : \"enforced\",\n",
    "    r'\\binformaiton\\b' : \"information\",\n",
    "    r'\\binformtion\\b' : \"information\",\n",
    "    r'\\binfront\\b' : \"in front\",\n",
    "    r'\\binnout\\b' : \"in-n-out\",\n",
    "    r'\\binsentive\\b' : \"incentive\",\n",
    "    r'\\binsufficent\\b' : \"insufficient\",\n",
    "    r'\\binterenet\\b' : \"internet\",\n",
    "    r'\\binterent\\b' : \"internet\",\n",
    "    r'\\bintermural\\b' : \"intramural\",\n",
    "    r'\\bintramurals\\b' : \"intramurals\",\n",
    "    r'\\binvironment\\b' : \"environment\",\n",
    "    r'\\bissue\\(s\\b' : \"issues\",\n",
    "    r'\\bit;s\\b' : \"it's\",\n",
    "    r'\\bitem\\(s\\b' : \"items\",\n",
    "    r\"\\bjob\\(s\\b\" : \"jobs\",\n",
    "    r'\\bknowledable\\b' : \"knowledgeable\",\n",
    "    r'\\bknowledeable\\b' : \"knowledgeable\",\n",
    "    r'\\bknowledegable\\b' : \"knowledgeable\",\n",
    "    r'\\bknowledgable\\b' : \"knowledgeable\",\n",
    "    r'\\bknowledgably\\b' : \"knowledgeably\",\n",
    "    r'\\bknowledgeably\\b' : \"knowledgeably\",\n",
    "    r'\\bknowledgeble\\b' : \"knowledgeable\",\n",
    "    r'\\bknowlegable\\b' : \"knowledgeable\",\n",
    "    r'\\bknowlegeable\\b' : \"knowledgeable\",\n",
    "    r'\\bliek\\b' : \"like\",\n",
    "    r'\\blieke\\b' : \"like\",\n",
    "    r'\\blimted\\b' : \"limited\",\n",
    "    r'\\bmaintainance\\b' : \"maintenance\",\n",
    "    r'\\bmaintaince\\b' : \"maintenance\",\n",
    "    r'\\bmaintainence\\b' : \"maintenance\",\n",
    "    r'\\bmaintanance\\b' : \"maintenance\",\n",
    "    r'\\bmaintance\\b' : \"maintenance\",\n",
    "    r'\\bmaintanence\\b' : \"maintenance\",\n",
    "    r'\\bmaintenace\\b' : \"maintenance\",\n",
    "    r'\\bmaintenances\\b' : \"maintenance\",\n",
    "    r'\\bmaintence\\b' : \"maintenance\",\n",
    "    r'\\bmaintenece\\b' : \"maintenance\",\n",
    "    r'\\bmaintenence\\b' : \"maintenance\",\n",
    "    r'\\bmaitenance\\b' : \"maintenance\",\n",
    "    r'\\bmanager\\(s\\b' : \"managers\",\n",
    "    r'\\bmanagment\\b' : \"management\",\n",
    "    r'\\bmanangement\\b' : \"management\",\n",
    "    r'\\bmangement\\b' : \"management\",\n",
    "    r'\\bmangers\\b' : \"managers\",\n",
    "    r'\\bmanuever\\b' : \"maneuver\",\n",
    "    r'\\bmintues\\b' : \"minutes\",\n",
    "    r'\\bmoblie\\b' : \"mobile\",\n",
    "    r'\\bmulitple\\b' : \"multiple\",\n",
    "    r'\\bn\\?a\\b' : \"n/a\",\n",
    "    r'\\bna\\b' : \"n/a\",\n",
    "    r'\\bneccessary\\b' : \"necessary\",\n",
    "    r'\\bnecesary\\b' : \"necessary\",\n",
    "    r'\\bneedes\\b' : \"needs\",\n",
    "    r'\\bneeed\\b' : \"need\",\n",
    "    r'\\bnonexistant\\b' : \"nonexistent\",\n",
    "    r'\\bnothig\\b' : \"nothing\",\n",
    "    r'\\bnothjng\\b' : \"nothing\",\n",
    "    r'\\bnoticable\\b' : \"noticeable\",\n",
    "    r'\\bobsurd\\b' : \"absurd\",\n",
    "    r'\\bocassional\\b' : \"occasional\",\n",
    "    r'\\boccassion\\b' : \"occasion\",\n",
    "    r'\\boccassional\\b' : \"occasional\",\n",
    "    r'\\boccassionally\\b' : \"occasionally\",\n",
    "    r'\\boccassions\\b' : \"occasions\",\n",
    "    r'\\boccations\\b' : \"occasions\",\n",
    "    r'\\boccurances\\b' : \"occurrences\",\n",
    "    r'\\boccured\\b' : \"occurred\",\n",
    "    r'\\boccuring\\b' : \"occurring\",\n",
    "    r'\\boccurr\\b' : \"occur\",\n",
    "    r'\\bofcourse\\b' : \"of course\",\n",
    "    r'\\bofferred\\b' : \"offered\",\n",
    "    r'\\bopinon\\b' : \"opinion\",\n",
    "    r'\\bopitions\\b' : \"options\",\n",
    "    r'\\boportunities\\b' : \"opportunities\",\n",
    "    r'\\bopperation\\b' : \"operation\",\n",
    "    r'\\boppertunities\\b' : \"opportunities\",\n",
    "    r'\\boppinion\\b' : \"opinion\",\n",
    "    r'\\bopportunites\\b' : \"opportunities\",\n",
    "    r'\\bopportunties\\b' : \"opportunities\",\n",
    "    r'\\boppotunities\\b' : \"opportunities\",\n",
    "    r'\\boppurtunities\\b' : \"opportunities\",\n",
    "    r'\\boppurtunity\\b' : \"opportunity\",\n",
    "    r'\\borgnized\\b' : \"organized\",\n",
    "    r'\\boutragous\\b' : \"outrageous\",\n",
    "    r'\\bpage\\(s\\b' : \"pages\",\n",
    "    r'\\bpakages\\b' : \"packages\",\n",
    "    r'\\bparkibg\\b' : \"parking\",\n",
    "    r'\\bparkig\\b' : \"parking\",\n",
    "    r'\\bparkign\\b' : \"parking\",\n",
    "    r'\\bparkinglots\\b' : \"parking lots\",\n",
    "    r'\\bpartime\\b' : \"part-time\",\n",
    "    r'\\bparttime\\b' : \"part-time\",\n",
    "    r'\\bpatroling\\b' : \"patrolling\",\n",
    "    r'\\bpeopel\\b' : \"people\",\n",
    "    r'\\bpermitt\\b' : \"permit\",\n",
    "    r'\\bperson\\(s\\b' : \"persons\",\n",
    "    r'\\bpersonel\\b' : \"personnel\",\n",
    "    r'\\bpersonell\\b' : \"personnel\",\n",
    "    r'\\bpharamcy\\b' : \"pharmacy\",\n",
    "    r'\\bpleasent\\b' : \"pleasant\",\n",
    "    r'\\bplently\\b' : \"plenty\",\n",
    "    r'\\bplesant\\b' : \"pleasant\",\n",
    "    r'\\bpositon\\b' : \"position\",\n",
    "    r'\\bposses\\b' : \"possess\",\n",
    "    r'\\bpossition\\b' : \"position\",\n",
    "    r'\\bpostion\\b' : \"position\",\n",
    "    r'\\bpostions\\b' : \"positions\",\n",
    "    r'\\bpostition\\b' : \"position\",\n",
    "    r'\\bpostive\\b' : \"positive\",\n",
    "    r'\\bpractioner\\b' : \"practitioner\",\n",
    "    r'\\bpractioners\\b' : \"practitioners\",\n",
    "    r'\\bprefered\\b' : \"preferred\",\n",
    "    r'\\bpreferrably\\b' : \"preferably\",\n",
    "    r'\\bpreform\\b' : \"perform\",\n",
    "    r'\\bpreforming\\b' : \"performing\",\n",
    "    r'\\bpricess\\b' : \"prices\",\n",
    "    r'\\bpriciples\\b' : \"principles\",\n",
    "    r'\\bpricy\\b' : \"pricey\",\n",
    "    r'\\bprking\\b' : \"parking\",\n",
    "    r'\\bproceedures\\b' : \"procedures\",\n",
    "    r'\\bprocurment\\b' : \"procurement\",\n",
    "    r'\\bprofessionaly\\b' : \"professionally\",\n",
    "    r'\\bproffessional\\b' : \"professional\",\n",
    "    r'\\bproffit\\b' : \"profit\",\n",
    "    r'\\bprofitt\\b' : \"profit\",\n",
    "    r'\\bprogam\\b' : \"program\",\n",
    "    r'\\bpromissed\\b' : \"promised\",\n",
    "    r'\\bpublically\\b' : \"publicly\",\n",
    "    r'\\bqucik\\b' : \"quick\",\n",
    "    r'\\bquestion\\(s\\b' : \"questions\",\n",
    "    r'\\bquestionaire\\b' : \"questionnaire\",\n",
    "    r'\\breall\\b' : \"really\",\n",
    "    r'\\brealy\\b' : \"really\",\n",
    "    r'\\breccomend\\b' : \"recommend\",\n",
    "    r'\\breccommend\\b' : \"recommend\",\n",
    "    r'\\breceieve\\b' : \"receive\",\n",
    "    r'\\breciept\\b' : \"receipt\",\n",
    "    r'\\breciepts\\b' : \"receipts\",\n",
    "    r'\\brecieve\\b' : \"receive\",\n",
    "    r'\\brecieved\\b' : \"received\",\n",
    "    r'\\brecieves\\b' : \"receives\",\n",
    "    r'\\brecieving\\b' : \"receiving\",\n",
    "    r'\\brecived\\b' : \"received\",\n",
    "    r'\\brecomend\\b' : \"recommend\",\n",
    "    r'\\brecomended\\b' : \"recommended\",\n",
    "    r'\\brediculous\\b' : \"ridiculous\",\n",
    "    r'\\brediculously\\b' : \"ridiculously\",\n",
    "    r'\\brefered\\b' : \"referred\",\n",
    "    r'\\brefering\\b' : \"referring\",\n",
    "    r'\\bregeants\\b' : \"regents\",\n",
    "    r'\\bregistar\\b' : \"regisrtar\",\n",
    "    r'\\bregistars\\b' : \"regisrtars\",\n",
    "    r'\\bregulary\\b' : \"regularly\",\n",
    "    r'\\breimbursment\\b' : \"reimbursement\",\n",
    "    r'\\breponse\\b' : \"response\",\n",
    "    r'\\breponsive\\b' : \"responsive\",\n",
    "    r'\\brepresentitive\\b' : \"representative\",\n",
    "    r'\\breserach\\b' : \"research\",\n",
    "    r'\\bresonable\\b' : \"reasonable\",\n",
    "    r'\\bresouces\\b' : \"resources\",\n",
    "    r'\\bresourses\\b' : \"resources\",\n",
    "    r'\\bresponsed\\b' : \"responded\",\n",
    "    r'\\bresponsibilites\\b' : \"responsibilites\",\n",
    "    r'\\bresponsiblities\\b' : \"responsibilites\",\n",
    "    r'\\bresponsiblity\\b' : \"responsibility\",\n",
    "    r'\\brestaraunts\\b' : \"restaurants\",\n",
    "    r'\\brestraunts\\b' : \"restaurants\",\n",
    "    r'\\brestuarant\\b' : \"restaurant\",\n",
    "    r'\\brestuarants\\b' : \"restaurants\",\n",
    "    r'\\bresturant\\b' : \"restaurant\",\n",
    "    r'\\bresturants\\b' : \"restaurants\",\n",
    "    r'\\bridiculus\\b' : \"ridiculous\",\n",
    "    r'\\briduculous\\b' : \"ridiculous\",\n",
    "    r'\\broomate\\b' : \"roommate\",\n",
    "    r'\\broomates\\b' : \"roommates\",\n",
    "    r'\\bsaleries\\b' : \"salaries\",\n",
    "    r'\\bsandwhich\\b' : \"sandwich\",\n",
    "    r'\\bsandwhiches\\b' : \"sandwiches\",\n",
    "    r'\\bsandwitches\\b' : \"sandwiches\",\n",
    "    r'\\bsatifaction\\b' : \"satisfaction\",\n",
    "    r'\\bsatified\\b' : \"satisfisatisfieded\",\n",
    "    r'\\bsattelite\\b' : \"satellite\",\n",
    "    r'\\bsceience\\b' : \"science\",\n",
    "    r'\\bschedual\\b' : \"schedule\",\n",
    "    r'\\bseemless\\b' : \"seamless\",\n",
    "    r'\\bselction\\b' : \"selection\",\n",
    "    r'\\bsenority\\b' : \"seniority\",\n",
    "    r'\\bsensative\\b' : \"sensitive\",\n",
    "    r'\\bsensored\\b' : \"censored\",\n",
    "    r'\\bseperate\\b' : \"separate\",\n",
    "    r'\\bseperation\\b' : \"separation\",\n",
    "    r'\\bserivce\\b' : \"service\",\n",
    "    r'\\bserivces\\b' : \"services\",\n",
    "    r'\\bserive\\b' : \"service\",\n",
    "    r'\\bserives\\b' : \"services\",\n",
    "    r'\\bservicesi\\b' : \"services\",\n",
    "    r'\\bservidces\\b' : \"services\",\n",
    "    r'\\bservive\\b' : \"survive\",\n",
    "    r'\\bservives\\b' : \"survives\",\n",
    "    r'\\bseverly\\b' : \"severely\",\n",
    "    r'\\bsevice\\b' : \"service\",\n",
    "    r'\\bsevices\\b' : \"services\",\n",
    "    r'\\bshcool\\b' : \"school\",\n",
    "    r'\\bshoud\\b' : \"should\",\n",
    "    r'\\bshoudl\\b' : \"should\",\n",
    "    r'\\bshutttle\\b' : \"shuttle\",\n",
    "    r'\\bsimiliar\\b' : \"similar\",\n",
    "    r'\\bsomeitmes\\b' : \"sometimes\",\n",
    "    r'\\bsomeone\\(s\\b' : \"someones\",\n",
    "    r'\\bsomeones\\b' : \"someones\",\n",
    "    r'\\bsometiems\\b' : \"sometimes\",\n",
    "    r'\\bsomone\\b' : \"someone\",\n",
    "    r'\\bsomthing\\b' : \"something\",\n",
    "    r'\\bsophmore\\b' : \"sophomore\",\n",
    "    r'\\bspecialy\\b' : \"especially\",\n",
    "    r'\\bstafff\\b' : \"staff\",\n",
    "    r'\\bstatment\\b' : \"statement\",\n",
    "    r'\\bstong\\b' : \"strong\",\n",
    "    r'\\bstongly\\b' : \"strongly\",\n",
    "    r'\\bstoping\\b' : \"stopping\",\n",
    "    r'\\bstrabucks\\b' : \"starbucks\",\n",
    "    r'\\bstressfull\\b' : \"stressful\",\n",
    "    r'\\bstructure\\(s\\b' : \"structures\",\n",
    "    r'\\bstucture\\b' : \"structure\",\n",
    "    r'\\bstuctures\\b' : \"structures\",\n",
    "    r'\\bstuden\\b' : \"student\",\n",
    "    r'\\bstudent\\(s\\b' : \"students\",\n",
    "    r'\\bstudetns\\b' : \"students\",\n",
    "    r'\\bstudnet\\b' : \"student\",\n",
    "    r'\\bstudnets\\b' : \"students\",\n",
    "    r'\\bsucess\\b' : \"success\",\n",
    "    r'\\bsudent\\b' : \"student\",\n",
    "    r'\\bsudents\\b' : \"students\",\n",
    "    r'\\bsuperintendant\\b' : \"superintendent\",\n",
    "    r'\\bsuperviser\\b' : \"supervisor\",\n",
    "    r'\\bsupervisor\\(s\\b' : \"supervisors\",\n",
    "    r'\\bsupervisores\\b' : \"supervisors\",\n",
    "    r'\\bsuport\\b' : \"support\",\n",
    "    r'\\bsupples\\b' : \"supplies\",\n",
    "    r'\\bsuppossed\\b' : \"supposed\",\n",
    "    r'\\bsuprised\\b' : \"surprised\",\n",
    "    r'\\bsuvey\\b' : \"survey\",\n",
    "    r'\\bsytem\\b' : \"system\",\n",
    "    r'\\bthats\\b' : \"that's\",\n",
    "    r\"\\bthe're\\b\" : \"they're\",\n",
    "    r'\\btheives\\b' : \"thieves\",\n",
    "    r'\\bthiefs\\b' : \"thieves\",\n",
    "    r'\\bthreating\\b' : \"threatening\",\n",
    "    r'\\bthroughly\\b' : \"thoroughly\",\n",
    "    r'\\bthrought\\b' : \"throughout\",\n",
    "    r'\\bthroughtout\\b' : \"throughout\",\n",
    "    r'\\btodays\\b' : \"today's\",\n",
    "    r'\\btraing\\b' : \"training\",\n",
    "    r'\\btrainning\\b' : \"training\",\n",
    "    r'\\btranfers\\b' : \"transfers\",\n",
    "    r'\\btransfered\\b' : \"transferred\",\n",
    "    r'\\btransfering\\b' : \"transferring\",\n",
    "    r'\\btransporation\\b' : \"transportation\",\n",
    "    r'\\btransportaion\\b' : \"transportation\",\n",
    "    r'\\btransportations\\b' : \"transportations\",\n",
    "    r'\\btransportion\\b' : \"transportation\",\n",
    "    r'\\btrashbags\\b' : \"trash bags\",\n",
    "    r'\\btrashcans\\b' : \"trash cans\",\n",
    "    r'\\btremedously\\b' : \"tremendously\",\n",
    "    r'\\btshirt\\b' : \"t-shirt\",\n",
    "    r'\\btshirts\\b' : \"t-shirts\",\n",
    "    r'\\btution\\b' : \"tuition\",\n",
    "    r'\\btutition\\b' : \"tuition\",\n",
    "    r'\\bunaccessible\\b' : \"inaccessible\",\n",
    "    r'\\bunconvenient\\b' : \"inconvenient\",\n",
    "    r'\\bunecessary\\b' : \"unnecessary\",\n",
    "    r'\\bunflexible\\b' : \"inflexible\",\n",
    "    r'\\bunforseen\\b' : \"unforeseen\",\n",
    "    r'\\buniverisity\\b' : \"university\",\n",
    "    r'\\buniveristy\\b' : \"university\",\n",
    "    r'\\buniverity\\b' : \"university\",\n",
    "    r'\\bunknowledgeable\\b' : \"unknowledgable\",\n",
    "    r'\\bunneccessary\\b' : \"unnecessary\",\n",
    "    r'\\bunrealiable\\b' : \"unreliable\",\n",
    "    r'\\buntill\\b' : \"until\",\n",
    "    r'\\bunversity\\b' : \"university\",\n",
    "    r'\\buseability\\b' : \"usability\",\n",
    "    r'\\busefull\\b' : \"useful\",\n",
    "    r'\\bususally\\b' : \"usually\",\n",
    "    r'\\bvaccum\\b' : \"vacuum\",\n",
    "    r'\\bvaccuum\\b' : \"vacuum\",\n",
    "    r'\\bvaction\\b' : \"vacation\",\n",
    "    r'\\bvacume\\b' : \"vacuum\",\n",
    "    r'\\bvariaty\\b' : \"variety\",\n",
    "    r'\\bvarities\\b' : \"varieties\",\n",
    "    r'\\bvarity\\b' : \"variety\",\n",
    "    r'\\bvegeterian\\b' : \"vegetarian\",\n",
    "    r'\\bvegitarian\\b' : \"vegetarian\",\n",
    "    r'\\bvegitarians\\b' : \"vegetarians\",\n",
    "    r'\\bvegtables\\b' : \"vegetables\",\n",
    "    r'\\bventillation\\b' : \"ventilation\",\n",
    "    r'\\bveriety\\b' : \"variety\",\n",
    "    r'\\bvisted\\b' : \"visited\",\n",
    "    r'\\bvistor\\b' : \"visitor\",\n",
    "    r'\\bvistors\\b' : \"visitors\",\n",
    "    r'\\bweeekends\\b' : \"weekends\",\n",
    "    r'\\bwierd\\b' : \"weird\",\n",
    "    r'\\bwirless\\b' : \"wireless\",\n",
    "    r'\\bwithdrawl\\b' : \"withdrawal\",\n",
    "    r'\\bwoudl\\b' : \"would\",\n",
    "    r\"\\bwoudn't\\b\" : \"wouldn't\",\n",
    "    r\"\\bthier\\b\" : \"their\",\n",
    "    r\"\\bappartments\\b\" : \"apartments\",\n",
    "    r\"\\bbenifits\\b\" : \"benefits\",\n",
    "    r\"\\bexistant\\b\" : \"existent\",\n",
    "    r\"\\bsaftey\\b\" : \"safety\",\n",
    "    r'\\bdon\"t\\b' : \"don't\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weirdchar_str_repls = {\n",
    "    \"#39;\" : \"'\",   \n",
    "    'amp;' : '&',   \n",
    "    '#146;' : \"'\",   \n",
    "    'nbsp;' : ' ',   \n",
    "    '#36;' : '$',   \n",
    "    '\\\\n' : \"\\n\",   \n",
    "    'quot;' : \"'\",   \n",
    "    '’' : \"'\",   \n",
    "    \"´\" : \"'\",\n",
    "    \"`\" : \"'\",\n",
    "    '`' : \"'\", \n",
    "    '´' : \"'\", \n",
    "    '“' : '\"',   \n",
    "    '”' : '\"',   \n",
    "    '<br />' : \"\\n\",   \n",
    "    '\\\\\"' : '\"',   \n",
    "    '<unk>' : 'u_n',   \n",
    "    ' @.@ ' : '.',   \n",
    "    ' @-@ ' : '-',   \n",
    "    '\\\\' : ' \\\\ ',   \n",
    "    '•' : '-'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# does regex replace making the substitution the same case\n",
    "def re_replace(word, replacement, text):\n",
    "    def func(match):\n",
    "        g = match.group()\n",
    "        if g.islower(): return replacement.lower()\n",
    "        if g.istitle(): return replacement.title()\n",
    "        if g.isupper(): return replacement.upper()\n",
    "        return replacement      \n",
    "    \n",
    "    return re.sub(word, func, text, flags=re.I)\n",
    "\n",
    "# define regex and string replacements\n",
    "re_repls = {}  # e.g., { **spelling_regex_repls } \n",
    "str_repls = {} # e.g., { **weirdchar_str_repls }\n",
    "\n",
    "def make_replacements(t:str) -> str:\n",
    "    # replace based on regexs (keeping case) and then strings\n",
    "    for k, v in re_repls.items(): t = re_replace(k, v, t)\n",
    "    for k, v in str_repls.items(): t = t.replace(k, t)\n",
    "    return t\n",
    "\n",
    "# ensure am|pm is considered it own token (7:00pm > 7:00 pm, 7am-10pm > 7 am - 10 pm))\n",
    "def fix_ampm(t:str) -> str:\n",
    "    re_ampm = re.compile(r'(\\d+)(am|pm|am\\-|pm\\-|a\\.m\\.|p\\.m\\.|a\\.m\\.\\-|p\\.m\\.\\-)')    \n",
    "    return re_ampm.sub(r'\\1 \\2 ', t)\n",
    "\n",
    "# try to handle places where a new sentence doesn't begin with a space (e.g., I like dogs.I like cats)\n",
    "# without breaking apart things like urls and emails\n",
    "def fix_sentence_ends(t:str) -> str:\n",
    "    re_sentend = re.compile(r'(?<!www)\\.((?!com|edu|org|net|m\\b)[a-zA-Z]+)(?!(@|\\.(com|edu|org|net)))\\b') \n",
    "    return re_sentend.sub(r'. \\1 ', t)\n",
    "\n",
    "# separate hyphen|tilde if it is at beginning of letter/digit\n",
    "def fix_hyphenated_words(t:str) -> str:\n",
    "    re_hypword = re.compile(r'\\s(\\-+|~+)([a-zA-Z0-9])')\n",
    "    return re_hypword.sub(r' \\1 \\2', t)\n",
    "\n",
    "\n",
    "# prepend custom tokenization rules to defaults\n",
    "custom_tok_rules = defaults.text_pre_rules + [make_replacements, fix_ampm, fix_sentence_ends, fix_hyphenated_words]\n",
    "\n",
    "# use this customized Tokenizer for qualitative data\n",
    "tokenizer = Tokenizer(pre_rules=custom_tok_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various default, LM, and classification paths\n",
    "PATH = Path('../data')\n",
    "CLEAN_DATA_PATH = Path('../data/clean')\n",
    "\n",
    "LM_PATH = PATH/'lm'\n",
    "CLS_PATH = PATH/'classification'\n",
    "STANDARD_THEME_PATH = CLS_PATH/'standard_themes'\n",
    "STANDARD_THEME_CSS_PATH = STANDARD_THEME_PATH/'css'\n",
    "\n",
    "(LM_PATH/'models').mkdir(parents=True, exist_ok=True)\n",
    "(LM_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "(STANDARD_THEME_CSS_PATH/'models').mkdir(parents=True, exist_ok=True)\n",
    "(STANDARD_THEME_CSS_PATH/'tmp').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic columns\n",
    "lm_dtypes = { \n",
    "    'Id': int, 'QuestionAnsID': int, 'AnswerText': str, 'AnswerText_NonEnglish': str, 'Language': str,\n",
    "    \n",
    "    'SurveyID': int, 'SurveyTypeID': int, 'BenchmarkSurveyType': str, 'ClientId': str,'RspID': int,\n",
    "    \n",
    "    'QuestionCategoryAbbr': str, 'QuestionText': str, 'QuestionClass': str, \n",
    "    \n",
    "    'QuestionCategoryID': float, 'QuestionReportAbbr': str, 'QuestionCategoryLabel': str, \n",
    "    'BenchmarkLevel1': str, 'BenchmarkLevel2': str, 'BenchmarkLevel3': str, 'ClientBenchmarkLevel': str,\n",
    "    \n",
    "    'GroupCode': float, 'GroupID': str, \n",
    "    'GroupLevel1Code': float, 'GroupLevel1Name': str,\n",
    "    'GroupLevel2Code': float, 'GroupLevel2Name': str,\n",
    "    'GroupLevel3Code': float, 'GroupLevel3Name': str,\n",
    "    'GroupLevel4Code': float, 'GroupLevel4Name': str,\n",
    "    'GroupLevel5Code': float, 'GroupLevel5Name': str,\n",
    "    'GroupLevel6Code': float, 'GroupLevel6Name': str,\n",
    "    'GroupLevel7Code': float, 'GroupLevel7Name': str,\n",
    "    'GroupLevel8Code': float, 'GroupLevel8Name': str,\n",
    "}\n",
    "\n",
    "lm_dtypes_sc = { convert_to_snakecase(k):v for k,v in lm_dtypes.items() }\n",
    "\n",
    "# standard css themes\n",
    "standard_theme_css_dtypes = { \n",
    "    'accessible_to_customers': int,\n",
    "    'consistency_in_policies_information': int,\n",
    "    'cost_fees': int,\n",
    "    'courteous_professional_staff': int,\n",
    "    'effective_communications': int,\n",
    "    'effectively_uses_websites_online_documentation': int,\n",
    "    'helpful_staff': int,\n",
    "    'knowledgeable_staff': int,\n",
    "    'moving_in_a_positive_direction': int,\n",
    "    'overall_satisfaction': int,\n",
    "    'process_improvement': int,\n",
    "    'provides_effective_advice_guidance': int,\n",
    "    'provides_training_on_processes_applications': int,\n",
    "    'resolves_problems_effectively': int,\n",
    "    'responds_to_requests_within_an_acceptable_time': int,\n",
    "    'understands_my_needs_and_requirements': int\n",
    "}\n",
    "\n",
    "# date columns\n",
    "date_cols = []\n",
    "    \n",
    "\n",
    "STANDARD_THEME_CSS_LABELS = list(standard_theme_css_dtypes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(LM_PATH/'vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "The classifier is basically a linear layer custom head on top of the LM backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 24000\n",
    "\n",
    "bptt, em_sz, nh, nl = 70, 400, 1150, 3\n",
    "bsz = 80\n",
    "wd = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define what text columns to use (can be multiple)\n",
    "corpus_cols = ['answer_text'] \n",
    "\n",
    "# define how to identify the text we are using for the LM\n",
    "corpus_suf = '' #'_cleaned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(STANDARD_THEME_CSS_PATH/'train.csv')\n",
    "valid_df = pd.read_csv(STANDARD_THEME_CSS_PATH/'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any rows whre the \"corpus_cols\" are nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=corpus_cols, inplace=True)\n",
    "valid_df.dropna(subset=corpus_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_THEME_CSS_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 11/15/2018 - currently have to put all labels into a single column\n",
    "# train_df['labels'] = train_df[SENT_LABELS[1:]].apply(lambda row: ' '.join(row.columns[row.values == 1]), axis=1)\n",
    "# valid_df['labels'] = valid_df[SENT_LABELS[1:]].apply(lambda row: ' '.join(row.columns[row.values == 1], axis=1)\n",
    "\n",
    "train_df['labels'] = train_df[STANDARD_THEME_CSS_LABELS].apply(\n",
    "    lambda x: ' '.join(x.index[x.astype(bool)]), axis=1)\n",
    "valid_df['labels'] = valid_df[STANDARD_THEME_CSS_LABELS].apply(\n",
    "    lambda x: ' '.join(x.index[x.astype(bool)]), axis=1)\n",
    "\n",
    "train_df[['labels'] + STANDARD_THEME_CSS_LABELS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_processor = [\n",
    "    TokenizeProcessor(tokenizer=tokenizer, chunksize=chunksize),\n",
    "    NumericalizeProcessor(vocab=vocab)\n",
    "]\n",
    "\n",
    "data_clas = (ItemLists(path=STANDARD_THEME_CSS_PATH,\n",
    "                     train=TextList.from_df(\n",
    "                         train_df, path=STANDARD_THEME_CSS_PATH, cols=corpus_cols, processor=cls_processor),\n",
    "                     valid=TextList.from_df(\n",
    "                         valid_df, path=STANDARD_THEME_CSS_PATH, cols=corpus_cols, processor=cls_processor)\n",
    "                    )\n",
    "             .label_from_df(cols='labels', classes=STANDARD_THEME_CSS_LABELS, label_delim=' ')\n",
    "             .databunch(bs=bsz)\n",
    "          )\n",
    "\n",
    "data_clas.save(f'data_cls_standard_theme_css.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(STANDARD_THEME_CSS_PATH, f'data_cls_standard_theme_css.pkl', bs=bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.train_ds.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_clas.train_ds.x[0])\n",
    "print(data_clas.train_ds.y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_clas.train_ds), len(data_clas.train_ds.vocab.itos))\n",
    "print(len(data_clas.valid_ds), len(data_clas.valid_ds.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(data_clas.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(it)\n",
    "print(batch[0].size())\n",
    "print(batch[1].size())\n",
    "print(batch[0].size(), batch[0].type(), batch[1].size(), batch[1].type(), bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "' '.join([ data_clas.train_ds.vocab.itos[idx] for idx in batch[0][0,:] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a forward or backwards run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backwards = True\n",
    "\n",
    "m_suf = '_multilabel' #'_cleaned'\n",
    "m_pre = 'bwd_' if (backwards) else 'fwd_'\n",
    "\n",
    "data_clas = load_data(STANDARD_THEME_CSS_PATH, f'data_cls_standard_theme_css.pkl', bs=bsz, backwards=backwards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the classifier (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, y_range=None):\n",
    "        super().__init__()\n",
    "        self.y_range = y_range\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x, raw_outputs, outputs = input\n",
    "        x = torch.sigmoid(x)\n",
    "        if (self.y_range):\n",
    "            x = x * (self.y_range[1] - self.y_range[0])\n",
    "            x = x + self.y_range[0]\n",
    "        \n",
    "        return x, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup the dropouts for the model - these values have been chosen after experimentation. If you need to update them for custom LMs, you can change the weighting factor (0.7 here) based on the amount of data you have. For more data, you can reduce dropout factor and for small datasets, you can reduce overfitting by choosing a higher dropout factor. *No other dropout value requires tuning*\n",
    "\n",
    "We first tune the last embedding layer so that the missing tokens initialized with mean weights get tuned properly. So we freeze everything except the last layer.\n",
    "\n",
    "We also keep track of the *accuracy* metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1\n",
    "start = 0.1\n",
    "\n",
    "def fscore(preds, targs):\n",
    "    return metrics_util.best_fscore(preds, targs, beta, start=start)\n",
    "    \n",
    "def opt_th(preds, targs):\n",
    "    return metrics_util.best_fthresh(preds, targs, beta=beta, start=start)\n",
    "\n",
    "def multilbl_accuracy(preds, targs):\n",
    "    return metrics_util.multi_accuracy(preds, targs, beta=beta, start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: learn.purge(); learn = None; torch.cuda.empty_cache();\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = None; gc.collect()\n",
    "learn = text_classifier_learner(data_clas, arch=AWD_LSTM, pretrained=False,\n",
    "                                drop_mult=0.5, bptt=bptt, lin_ftrs=[50], ps=[0.1],\n",
    "                                alpha=2., beta=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.add_module('2', MultiLabelClassifier())\n",
    "\n",
    "learn.clip = 25.\n",
    "learn.loss_func = F.binary_cross_entropy\n",
    "learn.metrics = [opt_th, fscore, multilbl_accuracy]\n",
    "\n",
    "learn.model_dir = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_cb = partial(SaveModelCallback, monitor='fscore', mode='max', name=f'{m_pre}cls_bestmodel{m_suf}')\n",
    "# best_model_cb = partial(SaveModelCallback, monitor='val_loss', mode='min', name=f'{lm_pre}cls_bestmodel{exp_suffix}')\n",
    "\n",
    "learn.callback_fns.append(best_model_cb)\n",
    "# learn.callback_fns.append(RocAucEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = STANDARD_THEME_CSS_PATH/f'models/{m_pre}cls_bestmodel{m_suf}*'\n",
    "!rm {best_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from /lm/models -> class/models (both fwd and bwd weights)\n",
    "! cp {LM_PATH/'models/*_lm_enc.pth'} {STANDARD_THEME_CSS_PATH/'models/'}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load_encoder(f'{m_pre}lm_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-1\n",
    "wd = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(lr/1000, wd=wd)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set learning rates and fit our IMDB LM. We first run one epoch to tune the last layer which contains the embedding weights. This should help the missing tokens in the wikitext103 learn better weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "learn.fit_one_cycle(1, lr, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{m_pre}cls_last_ft{m_suf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will load the best when training ends automaticall ... #learn = learn.load(f'{lm_pre}cls_last_ft{exp_suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(5e-2/(2.6**4),5e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{m_pre}cls_last2_ft{m_suf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will load the best when training ends automaticall ... #learn = learn.load(f'{lm_pre}cls_last2_ft{exp_suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{m_pre}cls_last3_ft{m_suf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will load the best when training ends automaticall ... \n",
    "# learn = learn.load(f'{lm_pre}cls_last3_ft{exp_suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(20, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{m_pre}cls{m_suf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(file=f'{m_pre}export_clas{m_suf}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn = load_learner(STANDARD_THEME_CSS_PATH, file=f'{m_pre}export_clas{m_suf}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn.data.single_ds.y.classes = STANDARD_THEME_CSS_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn.predict('The pay is too low and parking stinks on campus.  Where is my salary increase?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review final validation loss for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load(f'{m_pre}cls_bestmodel{m_suf}')\n",
    "probs, targs, loss = learn.get_preds(DatasetType.Valid, with_loss=True)\n",
    "\n",
    "print(f'Validation Loss: {loss.mean()}')\n",
    "print(f'Validation Loss (per label): {loss.mean(dim=0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(STANDARD_THEME_CSS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\"There are not enough people to do the work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models, csvs, to zip and download (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r models.zip {LM_PATH}/models/ {CLS_PATH}/models  -x {LM_PATH}/models/lstm_wt103/\\*\n",
    "\n",
    "# FileLink('models.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip verbatims-csvs.zip {PATH}/verbatims.csv {PATH}/verbatims-entities.csv {PATH}/verbatims-meta.csv\n",
    "\n",
    "# FileLink('verbatims-csvs.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sentiment for our validation dataset, including the actual document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions for a single model using the learner's model and data loaders\n",
    "learn.load(f'{m_pre}cls_bestmodel{m_suf}')\n",
    "learn.model.cuda(1)\n",
    "probs, targs, docs = get_cls_predictions(learn, DatasetType.Valid, vocab)\n",
    "\n",
    "probs.shape, targs.shape, len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.valid_ds.y.items.shape, data_clas.valid_ds.y.c, data_clas.valid_ds.y.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine optimal threshold based on desired f-score\n",
    "threshold_f05 = metrics_util.best_fthresh(probs, targs, beta=0.5, start=0.1, end=.3).item()\n",
    "threshold_f1 = metrics_util.best_fthresh(probs, targs, beta=1, start=0.1, end=.3).item()\n",
    "threshold_f2 = metrics_util.best_fthresh(probs, targs, beta=2, start=0.1, end=.3).item()\n",
    "\n",
    "threshold_f05, threshold_f1, threshold_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fbeta(probs, targs, thresh=threshold_f1, beta=1, sigmoid=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = metrics.fbeta_score(targs, (probs > threshold_f1), beta=1, average='samples')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = ((probs > threshold_f1).byte() == targs.byte()).float().mean()\n",
    "preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine accuracy based on optimal threshold\n",
    "val_acc_f05 = accuracy_thresh(probs, targs, threshold_f05, sigmoid=False).item()\n",
    "val_acc_f1 = accuracy_thresh(probs, targs, threshold_f1, sigmoid=False).item()\n",
    "val_acc_f2 = accuracy_thresh(probs, targs, threshold_f2, sigmoid=False).item()\n",
    "\n",
    "val_acc_f05, val_acc_f1, val_acc_f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "print (sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape, targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_targs = targs.flatten() # targs[:,0]\n",
    "eval_probs = probs.flatten() # probs[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Accuracy\n",
    "\n",
    "The percentage of correct predictions.  Answers the question, *\"Overall, how often is the classifier correct?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In multilabel classification, this function computes subset accuracy: \n",
    "# the set of labels predicted for a sample must exactly match ALL the corresponding set of labels in y_true.\n",
    "print(metrics.accuracy_score(targs, (probs > threshold_f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(eval_targs, (eval_probs > threshold_f1).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Accuracy\n",
    " \n",
    "The accuracy achieved by always predicting the most frequent class.  Answers the question, *\"What would the accuracy be by always predicting the most frequent case?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_classes, u_counts = np.unique(eval_targs, return_counts=True)\n",
    "most_freq_class, most_freq_class_count = u_classes[np.argmax(u_counts)], np.max(u_counts)\n",
    "print(most_freq_class, most_freq_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_class_count / len(eval_targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohen's kappa\n",
    "\n",
    "This measure is intended to compare labelings by different human annotators (not a classifier vs. ground truth)\n",
    "\n",
    "Kappa socres are between -1 and 1 ( >= .8 is generally considered good agreement; <= 0 means no agreement ... e.g., practically random labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.cohen_kappa_score(eval_targs, (eval_probs > threshold_f1).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Describes the performance of a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, \n",
    "                          title='Confusion matrix', cmap=plt.cm.Blues, print_info=False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        if (print_info): print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        if (print_info): print('Confusion matrix, without normalization')\n",
    "\n",
    "    if (print_info): print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(eval_targs, (eval_probs > threshold_f1).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_confusion_matrix(cm, classes=u_classes,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_confusion_matrix(cm, classes=u_classes, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(eval_targs, (eval_probs > threshold_f1).float(), [0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw probability distribution\n",
    "\n",
    "Useful to see how the threshold can be adjusted to increase sensitivity or specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(eval_probs, bins=10)\n",
    "# plt.xlim(0,1)\n",
    "plt.title('Histogram of predicted probabilities')\n",
    "plt.xlabel('Predicted probability of IsVeryPositive')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrates how you can **decrease** the threshold for predicting label in order to **increase the sensitivity** of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curves and Area Under the Curve (AUC)\n",
    "\n",
    "***ROC Curve*** answers the question, *\"How would sensitivity and specificity be affected by various thresholds without changing the threshold?\"*  It is a way **to visualize the performance of a binary classifier.**\n",
    "\n",
    "The ROC curve can help you **choose a threshold** that balances sensitivity and specificity based on your particular business case.\n",
    "\n",
    "ROC curves visualize all possible classification thresholds whereas misclassification rate only represents your error rate for a single threshold.\n",
    "\n",
    "A classifier that does a good job at separating the classes will have a ROC curve that hugs the upper left corner of the plot.  Converseley, a classifier the does a poor job separating the classes will have a ROC curve that is close to the diagonal line (0,0 -> 1,1).  That diagonal line represents a classifier that does no better than random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(eval_targs, eval_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "plt.xlim = ([0.0, 1.0])\n",
    "plt.ylim = ([0.0, 1.0])\n",
    "plt.title('ROC curve for all labels')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity/Recall)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***AUC*** = the percentage of the ROC plot that is underneath the curve.  \n",
    "\n",
    "AUC summarizes the performance of a classifier in a **single number**.  It says, *\"If you randomly chose one positive and one negative observation, what is the likelihood that your classifier will assign a higher predicted probability to the positive observation.\"*\n",
    "\n",
    "**An AUC of ~ 0.8 is very good while an AUC of ~ 0.5 represents a poor classifier.**\n",
    "\n",
    "The ROC curve and AUC are insensitive to whether your predicted probabilities are properly calibrated to actually represent probabilities of class membership (e.g., it works if predicted probs range from 0.9 to 1 instead of 0 to 1).  All the AUC metric cares about is how well your classifier separated the two classes\n",
    "\n",
    "Notes:\n",
    "1.  AUC is useful even when there is **high class imbalance** (unlike classification accuracy)\n",
    "2.  AUC is useful even when predicted probabilities are not properly calibrated (e.g., not between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.roc_auc_score(eval_targs, eval_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at things label by label ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_metrics = {\n",
    "    'thresholds': { 'f-beta05': threshold_f05, 'f-beta1': threshold_f1, 'f-beta2': threshold_f2 }\n",
    "}\n",
    "\n",
    "for idx, lbl in enumerate(STANDARD_THEME_CSS_LABELS):\n",
    "    lbl_name, lbl_idx, lbl_targs, lbl_probs = lbl, idx, targs[:,idx], probs[:, idx]\n",
    "    \n",
    "    label_metrics[lbl_name] = {}\n",
    "    label_metrics[lbl_name]['accuracies'] = {}\n",
    "    label_metrics[lbl_name]['cohen_kappas'] = {}\n",
    "    label_metrics[lbl_name]['confusion_matrices'] = {}\n",
    "    label_metrics[lbl_name]['roc'] = {}\n",
    "    label_metrics[lbl_name]['report'] = {}\n",
    "    \n",
    "    # get null accuracy (accuracy we'd get if we simply predicted the most common class)\n",
    "    u_classes, u_counts = np.unique(lbl_targs, return_counts=True)\n",
    "    most_freq_class, most_freq_class_count = u_classes[np.argmax(u_counts)], np.max(u_counts)\n",
    "    label_metrics[lbl_name]['null_accuracy'] = most_freq_class_count / len(lbl_targs)\n",
    "    \n",
    "    # get raw probability distribution\n",
    "    label_metrics[lbl_name]['probability_distribution'] = np.histogram(lbl_probs)\n",
    "    \n",
    "    # roc/auc curve metrics\n",
    "    label_metrics[lbl_name]['roc_auc'] = metrics.roc_auc_score(lbl_targs, lbl_probs)\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(lbl_targs, lbl_probs)\n",
    "    label_metrics[lbl_name]['roc']['fpr'] = fpr\n",
    "    label_metrics[lbl_name]['roc']['tpr'] = tpr\n",
    "    label_metrics[lbl_name]['roc']['thresholds'] = thresholds\n",
    "    \n",
    "    for k,v in label_metrics['thresholds'].items():\n",
    "        label_metrics[lbl_name]['accuracies'][k] = metrics.accuracy_score(lbl_targs, (lbl_probs > v))\n",
    "        label_metrics[lbl_name]['cohen_kappas'][k] = metrics.cohen_kappa_score(lbl_targs, (lbl_probs > v))\n",
    "        label_metrics[lbl_name]['confusion_matrices'][k] = metrics.confusion_matrix(lbl_targs, (lbl_probs > v))\n",
    "        \n",
    "        precision, recall, fbeta_score, support = metrics.precision_recall_fscore_support(lbl_targs, (lbl_probs > v))\n",
    "        label_metrics[lbl_name]['report'][k] = {}\n",
    "        label_metrics[lbl_name]['report'][k]['precision'] = precision\n",
    "        label_metrics[lbl_name]['report'][k]['recall'] = recall\n",
    "        label_metrics[lbl_name]['report'][k]['fbeta_score'] = fbeta_score\n",
    "        label_metrics[lbl_name]['report'][k]['support'] = support\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for lbl in label_metrics.keys():\n",
    "    if (lbl == 'thresholds'): continue\n",
    "    \n",
    "    print(f'{lbl.upper()}\\n')\n",
    "    \n",
    "    print(f'Null Accuracy:\\t{label_metrics[lbl][\"null_accuracy\"]}')\n",
    "    print(f'AUC Score:\\t{label_metrics[lbl][\"roc_auc\"]}')\n",
    "    print('')\n",
    "    \n",
    "    print(''.join([ f'\\t\\t{threshold}({np.round(v, 4)})' for threshold, v in label_metrics['thresholds'].items() ]))\n",
    "    \n",
    "    print('Accuracy:\\t', end='')\n",
    "    for threshold, v in label_metrics['thresholds'].items():\n",
    "        print(f'{label_metrics[lbl][\"accuracies\"][threshold]}\\t', end='')\n",
    "    print('')\n",
    "    \n",
    "    print('Cohen\\'s Kappa:\\t', end='')\n",
    "    for threshold, v in label_metrics['thresholds'].items():\n",
    "        print(f'{label_metrics[lbl][\"cohen_kappas\"][threshold]}\\t', end='')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Classification Reports:')\n",
    "    for k in label_metrics[lbl]['report'].keys():\n",
    "        print(f'{k}')\n",
    "        print(f'{\"\":<20}' + ''.join([ f'{sub_key:<20}' for sub_key in label_metrics[lbl]['report'][k].keys() ]))\n",
    "        \n",
    "        for i in range(2):\n",
    "            print(f'{i:<20}' + ''.join([ f'{np.round(v[i],4):<20}' \n",
    "                                      for v in label_metrics[lbl]['report'][k].values() ]))\n",
    "        \n",
    "        print(f'{\"avg/total\":<20}' + ''.join([ f'{ np.round(v.mean(),4) if (sub_key != \"support\") else np.round(v.sum(),4):<20}' \n",
    "                                     for sub_key, v in label_metrics[lbl]['report'][k].items() ]))\n",
    "        print('')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Confusion Matrices:')\n",
    "    for threshold, v in label_metrics['thresholds'].items():\n",
    "        cm = label_metrics[lbl]['confusion_matrices'][threshold]\n",
    "        \n",
    "        # Plot non-normalized confusion matrix\n",
    "        fig = plt.figure(figsize=(12,8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plot_confusion_matrix(cm, classes=[0,1], \n",
    "                              title=f'Confusion matrix, without normalization ({threshold}: {np.round(v,4)})')\n",
    "\n",
    "        # Plot normalized confusion matrix\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plot_confusion_matrix(cm, classes=[0,1], normalize=True, \n",
    "                              title=f'Normalized confusion matrix ({threshold}: {np.round(v,4)})')\n",
    "\n",
    "        fig.subplots_adjust(wspace=0.5)\n",
    "        plt.show()\n",
    "    print('\\n')\n",
    "    \n",
    "    print('ROC Curve:')\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(label_metrics[lbl]['roc']['fpr'], label_metrics[lbl]['roc']['tpr'])\n",
    "    plt.xlim = ([0.0, 1.0])\n",
    "    plt.ylim = ([0.0, 1.0])\n",
    "    plt.title(f'ROC curve for {lbl}')\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity/Recall)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print('Predicted Probability Distribution:')\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.xlim = ([0.0, 1.0])\n",
    "    plt.bar(label_metrics[lbl]['probability_distribution'][1][:-1], \n",
    "            label_metrics[lbl]['probability_distribution'][0], width=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*100)\n",
    "    print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble forwards and backwards passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    learn_fwd.purge(); learn_fwd = None;\n",
    "    learn_bwd.purge(); learn_bwd = None;\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except: pass\n",
    "\n",
    "\n",
    "bsz = 80\n",
    "m_suf = '_multilabel'\n",
    "\n",
    "learn_fwd = load_learner(STANDARD_THEME_CSS_PATH, file=f'fwd_export_clas{m_suf}.pkl')\n",
    "data_fwd = load_data(STANDARD_THEME_CSS_PATH, f'data_cls_standard_theme_css.pkl', bs=bsz)\n",
    "learn_fwd.data = data_fwd\n",
    "\n",
    "learn_bwd = load_learner(STANDARD_THEME_CSS_PATH, file=f'bwd_export_clas{m_suf}.pkl')\n",
    "data_bwd = load_data(STANDARD_THEME_CSS_PATH, f'data_cls_standard_theme_css.pkl', bs=bsz, backwards=True)\n",
    "learn_bwd.data = data_bwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_fwd, lbl_fwd, loss_fwd = learn_fwd.get_preds(ordered=True, with_loss=True)\n",
    "probs_bwd, lbl_bwd, loss_bwd = learn_bwd.get_preds(ordered=True, with_loss=True)\n",
    "\n",
    "probs_fwd.shape, probs_bwd.shape, loss_fwd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fwd.mean(), probs_bwd.mean(), (loss_fwd.mean() + probs_bwd.mean()) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_final = (probs_fwd + probs_bwd) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine optimal threshold based on desired f-score\n",
    "threshold_f05 = metrics_util.best_fthresh(probs_fwd, lbl_fwd, beta=0.5, start=0.1, end=.3).item()\n",
    "threshold_f1 = metrics_util.best_fthresh(probs_fwd, lbl_fwd, beta=1, start=0.1, end=.3).item()\n",
    "threshold_f2 = metrics_util.best_fthresh(probs_fwd, lbl_fwd, beta=2, start=0.1, end=.3).item()\n",
    "\n",
    "threshold_f05, threshold_f1, threshold_f2\n",
    "\n",
    "# determine accuracy based on optimal threshold\n",
    "val_acc_f05 = accuracy_thresh(probs_fwd, lbl_fwd, threshold_f05, sigmoid=False).item()\n",
    "val_acc_f1 = accuracy_thresh(probs_fwd, lbl_fwd, threshold_f1, sigmoid=False).item()\n",
    "val_acc_f2 = accuracy_thresh(probs_fwd, lbl_fwd, threshold_f2, sigmoid=False).item()\n",
    "\n",
    "print('Fowards Only\\n-------------')\n",
    "print(f'f05:\\tOptimal threshold = {threshold_f05}\\t(Accuracy = {val_acc_f05})')\n",
    "print(f'f1:\\tOptimal threshold = {threshold_f1}\\t\\t(Accuracy = {val_acc_f1})')\n",
    "print(f'f2:\\tOptimal threshold = {threshold_f2}\\t(Accuracy = {val_acc_f2})')\n",
    "\n",
    "print(f'\\nAccuracy: {accuracy_thresh(probs_fwd, lbl_fwd, sigmoid=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine optimal threshold based on desired f-score\n",
    "threshold_f05 = metrics_util.best_fthresh(probs_bwd, lbl_fwd, beta=0.5, start=0.1, end=.3).item()\n",
    "threshold_f1 = metrics_util.best_fthresh(probs_bwd, lbl_fwd, beta=1, start=0.1, end=.3).item()\n",
    "threshold_f2 = metrics_util.best_fthresh(probs_bwd, lbl_fwd, beta=2, start=0.1, end=.3).item()\n",
    "\n",
    "threshold_f05, threshold_f1, threshold_f2\n",
    "\n",
    "# determine accuracy based on optimal threshold\n",
    "val_acc_f05 = accuracy_thresh(probs_bwd, lbl_fwd, threshold_f05, sigmoid=False).item()\n",
    "val_acc_f1 = accuracy_thresh(probs_bwd, lbl_fwd, threshold_f1, sigmoid=False).item()\n",
    "val_acc_f2 = accuracy_thresh(probs_bwd, lbl_fwd, threshold_f2, sigmoid=False).item()\n",
    "\n",
    "print('Backwards Only\\n-------------')\n",
    "print(f'f05:\\tOptimal threshold = {threshold_f05} (Accuracy = {val_acc_f05})')\n",
    "print(f'f1:\\tOptimal threshold = {threshold_f1} (Accuracy = {val_acc_f1})')\n",
    "print(f'f2:\\tOptimal threshold = {threshold_f2} (Accuracy = {val_acc_f2})')\n",
    "\n",
    "print(f'\\nAccuracy: {accuracy_thresh(probs_bwd, lbl_fwd, sigmoid=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine optimal threshold based on desired f-score\n",
    "threshold_f05 = metrics_util.best_fthresh(probs_final, lbl_fwd, beta=0.5, start=0.1, end=.3).item()\n",
    "threshold_f1 = metrics_util.best_fthresh(probs_final, lbl_fwd, beta=1, start=0.1, end=.3).item()\n",
    "threshold_f2 = metrics_util.best_fthresh(probs_final, lbl_fwd, beta=2, start=0.1, end=.3).item()\n",
    "\n",
    "threshold_f05, threshold_f1, threshold_f2\n",
    "\n",
    "# determine accuracy based on optimal threshold\n",
    "val_acc_f05 = accuracy_thresh(probs_final, lbl_fwd, threshold_f05, sigmoid=False).item()\n",
    "val_acc_f1 = accuracy_thresh(probs_final, lbl_fwd, threshold_f1, sigmoid=False).item()\n",
    "val_acc_f2 = accuracy_thresh(probs_final, lbl_fwd, threshold_f2, sigmoid=False).item()\n",
    "\n",
    "print('Ensemble\\n-------------')\n",
    "print(f'f05:\\tOptimal threshold = {threshold_f05} (Accuracy = {val_acc_f05})')\n",
    "print(f'f1:\\tOptimal threshold = {threshold_f1} (Accuracy = {val_acc_f1})')\n",
    "print(f'f2:\\tOptimal threshold = {threshold_f2} (Accuracy = {val_acc_f2})')\n",
    "\n",
    "print(f'\\nAccuracy: {accuracy_thresh(probs_final, lbl_fwd, sigmoid=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_valid_loss = (loss_fwd.mean() + probs_bwd.mean()) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference (ad-hoc documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(STANDARD_THEME_CSS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    'The parking situation REALLY sucks around here.  It needs to be fixed',\n",
    "    'I LOVE working at UCSD!!!  It is wonderful',\n",
    "    \"\"\"Some staff are just uninformed.There is no support for solo-individual study (no closed off rooms).\n",
    "        Once a guy (quite tall) walked in into the girl's restroom and used the stalls standing up. \n",
    "        There was no line in the guy's restroom. This happened when I done and was going to walk out. \n",
    "        I was extremely uncomfortable\"\"\",\n",
    "    \"I love UCSD!!! It is a terrible place to work!\",\n",
    "    \"I was really uncomfortable to express my opinion!!!\"\n",
    "]\n",
    "\n",
    "doc_probs, doc_preds, doc_toks = get_cls_doc_predictions(learn.model, vocab, tokenizer, test_comments, \n",
    "                                                         threshold=threshold_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_probs), len(doc_probs), len(doc_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d_probs, d_preds, d_toks in zip(doc_probs, doc_preds, doc_toks):\n",
    "    print(f'> {\" \".join([t for t in d_toks])}\\nProbabilities:\\t{d_probs}\\nPredictions:\\t{d_preds}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference (batch ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "yyyymmdd = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "m_suf = '_multilabel'\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbatims_df = pd.read_csv(LM_PATH/'all.csv', dtype={**lm_dtypes}, parse_dates=[])\n",
    "\n",
    "inf_df = verbatims_df[verbatims_df.BenchmarkSurveyType.str.startswith('CSS')].copy()\n",
    "inf_df.reset_index(drop=True, inplace=True)\n",
    "print(len(inf_df))\n",
    "\n",
    "corpus_cols = ['AnswerText']  # ['question_text', 'answer_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_pool(raw_outputs):\n",
    "    last_rnn_layer = raw_outputs[-1]\n",
    "    bsz = last_rnn_layer.shape[0] \n",
    "    \n",
    "    avg_pool = F.adaptive_avg_pool1d(last_rnn_layer.permute(0,2,1), 1).view(bsz, -1)\n",
    "    max_pool = F.adaptive_max_pool1d(last_rnn_layer.permute(0,2,1), 1).view(bsz, -1)\n",
    "    last_outp = last_rnn_layer[:,-1,:]\n",
    "\n",
    "    return torch.cat([last_outp, max_pool, avg_pool], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_results(backwards:bool=False, m_suf:str='multilabel'):\n",
    "    \n",
    "    model_prefix = 'bwd' if backwards else 'fwd'\n",
    "    \n",
    "    # 1. grab learner, procs, and data\n",
    "    inf_learn = load_learner(STANDARD_THEME_CSS_PATH, file=f'{model_prefix}_export_clas_{m_suf}.pkl')\n",
    "    txt_procs = inf_learn.data.train_ds.processor\n",
    "    inf_data = TextList.from_df(inf_df, cols=corpus_cols, processor=txt_procs).split_none().label_empty()\n",
    "    \n",
    "    # 2. define a suitable dataloader\n",
    "    collate_fn = partial(pad_collate, pad_first=True, backwards=backwards)\n",
    "    sampler = SortSampler(inf_data.train.x, key=[len(t) for t in inf_data.train.x.items].__getitem__)\n",
    "    dl = DeviceDataLoader.create(inf_data.train, bs=128, sampler=sampler, collate_fn=collate_fn, device=device)\n",
    "    \n",
    "    # 3. get probs and document vectors\n",
    "    inf_learn.model = inf_learn.model.to(device)\n",
    "    inf_learn.model = inf_learn.model.eval()\n",
    "    \n",
    "    test_probs, doc_vecs, concat_doc_vecs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for index, (xb, yb) in enumerate(dl):\n",
    "            if index % 1000 == 0:  print(index)\n",
    "\n",
    "            # reset hidden state (if you don't do this you will OOM)\n",
    "            inf_learn.model.reset()\n",
    "\n",
    "            # why \"detach\"? the computation of gradients wrt the weights of netG can be fully \n",
    "            # avoided in the backward pass if the graph is detached where it is.\n",
    "            probs, raw_outputs, outputs = inf_learn.model(xb)\n",
    "\n",
    "            test_probs.append(to_detach(probs))\n",
    "            doc_vecs.append(to_detach(raw_outputs[-1][:,-1,:]))\n",
    "            concat_doc_vecs.append(to_detach(concat_pool(raw_outputs)))\n",
    "\n",
    "    all_probs = torch.cat(test_probs)\n",
    "    all_vecs = torch.cat(doc_vecs)\n",
    "    all_concat_vecs = torch.cat(concat_doc_vecs)\n",
    "\n",
    "    # 4. ensure results are returned in order\n",
    "    if hasattr(dl, 'sampler'):\n",
    "        sampler_idxs = [i for i in dl.sampler]\n",
    "        reverse_sampler = np.argsort(sampler_idxs)\n",
    "\n",
    "        all_probs = all_probs[reverse_sampler]\n",
    "        all_vecs = all_vecs[reverse_sampler]\n",
    "        all_concat_vecs = all_concat_vecs[reverse_sampler]\n",
    "        \n",
    "    # 5. return ordered results\n",
    "    inf_learn, inf_data = None, None; gc.collect()\n",
    "    \n",
    "    return all_probs, all_vecs, all_concat_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "probs_fwd, vecs_fwd, concat_vecs_fwd = get_classification_results(backwards=False)\n",
    "probs_bwd, vecs_bwd, concat_vecs_bwd = get_classification_results(backwards=True)\n",
    "\n",
    "probs_final = (probs_fwd + probs_bwd) / 2\n",
    "\n",
    "print(probs_final.shape)\n",
    "print(probs_fwd.shape, vecs_fwd.shape, concat_vecs_fwd.shape)\n",
    "print(probs_bwd.shape, vecs_bwd.shape, concat_vecs_bwd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the probabilities of each label to `inf_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_labels = ['prob_' + lbl for lbl in STANDARD_THEME_CSS_LABELS]\n",
    "probs_df = pd.DataFrame(probs_final.numpy(), columns=prob_labels)\n",
    "probs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df_filtered.update(probs_df)\n",
    "final_df = pd.concat([inf_df, probs_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in predictions based on f1 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl in STANDARD_THEME_CSS_LABELS:\n",
    "    final_df[f'pred_{lbl}'] = (final_df[f'prob_{lbl}'] > threshold_f1).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include found thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['threshold_f05'] = threshold_f05\n",
    "final_df['threshold_f1'] = threshold_f1\n",
    "final_df['threshold_f2'] = threshold_f2\n",
    "\n",
    "final_df['val_acc_f05'] = val_acc_f05\n",
    "final_df['val_acc_f1'] = val_acc_f1\n",
    "final_df['val_acc_f2'] = val_acc_f2\n",
    "\n",
    "final_df['val_loss'] = final_valid_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "final_df.to_csv(STANDARD_THEME_CSS_PATH/f'{yyyymmdd}_ensemble_predictions{m_suf}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "np.save(STANDARD_THEME_CSS_PATH/f'{yyyymmdd}_fwd_concat_docvecs_d400{m_suf}.npy', concat_vecs_fwd.numpy())  \n",
    "np.save(STANDARD_THEME_CSS_PATH/f'{yyyymmdd}_fwd_docvecs_d400{m_suf}.npy', vecs_fwd.numpy())\n",
    "\n",
    "np.save(STANDARD_THEME_CSS_PATH/f'{yyyymmdd}_bwd_concat_docvecs_d400{m_suf}.npy', concat_vecs_bwd.numpy())  \n",
    "np.save(STANDARD_THEME_CSS_PATH/f'{yyyymmdd}_bwd_docvecs_d400{m_suf}.npy', vecs_bwd.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.iloc[0].threshold_f05, final_df.iloc[0].threshold_f1, final_df.iloc[0].threshold_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(learn.layer_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[ print(f'{lg}\\n') for lg in learn.layer_groups ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
