{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wired-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp verbatims/standard_themes_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developing-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "engaging-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-penny",
   "metadata": {},
   "source": [
    "# Verbatims - Standard Themes: Meta\n",
    "\n",
    "> This module defines the training configuration object, data preparation, training, optimization, and inference code for our multimodal classification/regression task that, given a verbatim and a related standard theme, predicts the sentiment of the verbatim relative to the theme (regression) and whether it should be used as an example of that theme (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aggressive-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/wgilliam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import os, datetime, gc\n",
    "import sklearn.metrics as skm\n",
    "from fastai.text.all import *\n",
    "from fastai import metrics as fa_metrics\n",
    "from transformers import *\n",
    "\n",
    "import optuna\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *\n",
    "from blurr.modeling.core import *\n",
    "from blurr.modeling.all import MultiTargetLoss\n",
    "\n",
    "from tritonlytics_ai.utils import *\n",
    "\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "visible-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "# pandas and plotting config\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "moderate-reaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch 1.7.1+cu110\n",
      "Using fastai 2.2.7\n",
      "Using transformers 4.3.3\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from fastai import __version__ as fa_version\n",
    "from torch import __version__ as pt_version\n",
    "from transformers import __version__ as hft_version\n",
    "\n",
    "print(f'Using pytorch {pt_version}')\n",
    "print(f'Using fastai {fa_version}')\n",
    "print(f'Using transformers {hft_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affected-fishing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-banks",
   "metadata": {},
   "source": [
    "## Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "equipped-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "train_config = {\n",
    "    'm_pre': '',\n",
    "    'm_suf': '_multilabel_hf',\n",
    "    'base_model_name': 'verbatim_standard_theme_meta',\n",
    "    \n",
    "    'orig_txt_cols': ['theme', 'answer_text'],\n",
    "    'txt_cols': ['_theme_', '_text_'],\n",
    "    'max_seq_length': 256,\n",
    "    'batch_size': 8,\n",
    "    'corpus_suf': '_multitask',\n",
    "    'train_data': STANDARD_THEME_META_PATH/'train.csv',\n",
    "    'valid_data': STANDARD_THEME_META_PATH/'test.csv',\n",
    "    \n",
    "    'opt_beta': 0.5, \n",
    "    'opt_beta_average': 'binary',\n",
    "    'opt_beta_sample_weight': None,\n",
    "    'opt_start': 0.08, \n",
    "    'opt_end': 0.7,\n",
    "    \n",
    "    'save_model_monitor': 'valid_loss', \n",
    "    'save_model_comp': np.less,\n",
    "    'learner_path': STANDARD_THEME_META_PATH\n",
    "}\n",
    "\n",
    "train_config.update({\n",
    "    'cache_data_path': STANDARD_THEME_META_PATH/f\"data_{train_config['base_model_name']}.pkl\",\n",
    "    'save_model_filename': f\"{train_config['m_pre']}{train_config['base_model_name']}{train_config['m_suf']}_bestmodel\",\n",
    "    'export_filename': f\"{train_config['m_pre']}{train_config['base_model_name']}{train_config['m_suf']}_export.pkl\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "comprehensive-server",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m_pre': '',\n",
       " 'm_suf': '_multilabel_hf',\n",
       " 'base_model_name': 'verbatim_standard_theme_meta',\n",
       " 'orig_txt_cols': ['theme', 'answer_text'],\n",
       " 'txt_cols': ['_theme_', '_text_'],\n",
       " 'max_seq_length': 256,\n",
       " 'batch_size': 8,\n",
       " 'corpus_suf': '_multitask',\n",
       " 'train_data': Path('data/classification/standard_themes/meta/train.csv'),\n",
       " 'valid_data': Path('data/classification/standard_themes/meta/test.csv'),\n",
       " 'opt_beta': 0.5,\n",
       " 'opt_beta_average': 'binary',\n",
       " 'opt_beta_sample_weight': None,\n",
       " 'opt_start': 0.08,\n",
       " 'opt_end': 0.7,\n",
       " 'save_model_monitor': 'valid_loss',\n",
       " 'save_model_comp': <ufunc 'less'>,\n",
       " 'learner_path': Path('data/classification/standard_themes/meta'),\n",
       " 'cache_data_path': Path('data/classification/standard_themes/meta/data_verbatim_standard_theme_meta.pkl'),\n",
       " 'save_model_filename': 'verbatim_standard_theme_meta_multilabel_hf_bestmodel',\n",
       " 'export_filename': 'verbatim_standard_theme_meta_multilabel_hf_export.pkl'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-appliance",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-illustration",
   "metadata": {},
   "source": [
    "Prepare the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "familiar-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_config['train_data'])\n",
    "valid_df = pd.read_csv(train_config['valid_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "corrected-booking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_ans_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_text_non_english</th>\n",
       "      <th>language</th>\n",
       "      <th>survey_id</th>\n",
       "      <th>survey_type_id</th>\n",
       "      <th>benchmark_survey_type</th>\n",
       "      <th>client_id</th>\n",
       "      <th>rsp_id</th>\n",
       "      <th>question_category_abbr</th>\n",
       "      <th>question_text</th>\n",
       "      <th>question_class</th>\n",
       "      <th>question_category_id</th>\n",
       "      <th>question_report_abbr</th>\n",
       "      <th>question_category_label</th>\n",
       "      <th>benchmark_level1</th>\n",
       "      <th>benchmark_level2</th>\n",
       "      <th>benchmark_level3</th>\n",
       "      <th>client_benchmark_level</th>\n",
       "      <th>group_code</th>\n",
       "      <th>group_id</th>\n",
       "      <th>group_level1_code</th>\n",
       "      <th>group_level1_name</th>\n",
       "      <th>group_level2_code</th>\n",
       "      <th>group_level2_name</th>\n",
       "      <th>group_level3_code</th>\n",
       "      <th>group_level3_name</th>\n",
       "      <th>group_level4_code</th>\n",
       "      <th>group_level4_name</th>\n",
       "      <th>group_level5_code</th>\n",
       "      <th>group_level5_name</th>\n",
       "      <th>group_level6_code</th>\n",
       "      <th>group_level6_name</th>\n",
       "      <th>group_level7_code</th>\n",
       "      <th>group_level7_name</th>\n",
       "      <th>group_level8_code</th>\n",
       "      <th>group_level8_name</th>\n",
       "      <th>standard_theme_id</th>\n",
       "      <th>theme</th>\n",
       "      <th>url_friendly_theme</th>\n",
       "      <th>theme_display_order</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>is_example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>660454</td>\n",
       "      <td>93069</td>\n",
       "      <td>\"Academics at UC ANR value my contributions.\"\\r\\n\"Staff members at UC ANR value my contributions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>396</td>\n",
       "      <td>47</td>\n",
       "      <td>SAW</td>\n",
       "      <td>UCANR</td>\n",
       "      <td>480552</td>\n",
       "      <td>None</td>\n",
       "      <td>Please provide any additional feedback regarding the work environment at UC ANR. Your comments w...</td>\n",
       "      <td>Verbatim-Comments</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>Comments re Work Environment at UC ANR</td>\n",
       "      <td>Comments</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>250400.0</td>\n",
       "      <td>6984</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>UC Agriculture &amp; Natural Resources</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>AVP Programs and Initiatives</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>Strategic Institutes and Statewide Programs</td>\n",
       "      <td>250400.0</td>\n",
       "      <td>Statewide IPM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>Have Voice within my Institution/Valued Member of my Institution</td>\n",
       "      <td>HaveVoiceWithinMyInstitutionValuedMemberOfMyInstitution</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>589692</td>\n",
       "      <td>2576</td>\n",
       "      <td>*The MSO of this department consistently takes unfair advantage of power dynamics to intimidate ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>212</td>\n",
       "      <td>9</td>\n",
       "      <td>SAW</td>\n",
       "      <td>UCSD</td>\n",
       "      <td>447156</td>\n",
       "      <td>C&amp;B</td>\n",
       "      <td>If you would like to elaborate on any of your answers to the conduct and behavioral questions ab...</td>\n",
       "      <td>Verbatim</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>Conduct &amp; Behavioral - Comments</td>\n",
       "      <td>Conduct &amp; Behavioral</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10104.0</td>\n",
       "      <td>3437</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>UC San Diego</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>ACADEMIC AFFAIRS</td>\n",
       "      <td>10002.0</td>\n",
       "      <td>DIVISIONS/SCHOOLS</td>\n",
       "      <td>10003.0</td>\n",
       "      <td>ARTS &amp; HUMANITIES</td>\n",
       "      <td>10104.0</td>\n",
       "      <td>MUSIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>Supervisor Effectiveness/Resolves Staff Issues</td>\n",
       "      <td>SupervisorEffectivenessResolvesStaffIssues</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  question_ans_id                                                                                          answer_text  answer_text_non_english language  survey_id  survey_type_id benchmark_survey_type client_id  rsp_id question_category_abbr                                                                                        question_text     question_class  question_category_id                    question_report_abbr question_category_label benchmark_level1 benchmark_level2 benchmark_level3  client_benchmark_level  group_code  group_id  group_level1_code                   group_level1_name  group_level2_code             group_level2_name  group_level3_code                            group_level3_name  group_level4_code  group_level4_name  group_level5_code group_level5_name  group_level6_code group_level6_name  group_level7_code group_level7_name  group_level8_code  group_level8_name  standard_theme_id                                                             theme  \\\n",
       "0  660454            93069  \"Academics at UC ANR value my contributions.\"\\r\\n\"Staff members at UC ANR value my contributions...                      NaN  English        396              47                   SAW     UCANR  480552                   None  Please provide any additional feedback regarding the work environment at UC ANR. Your comments w...  Verbatim-Comments                1141.0  Comments re Work Environment at UC ANR                Comments             None             None             None                     3.0    250400.0      6984           999999.0  UC Agriculture & Natural Resources           200000.0  AVP Programs and Initiatives           250000.0  Strategic Institutes and Statewide Programs           250400.0      Statewide IPM                NaN               NaN                NaN               NaN                NaN               NaN                NaN                NaN                 10  Have Voice within my Institution/Valued Member of my Institution   \n",
       "1  589692             2576  *The MSO of this department consistently takes unfair advantage of power dynamics to intimidate ...                      NaN  English        212               9                   SAW      UCSD  447156                    C&B  If you would like to elaborate on any of your answers to the conduct and behavioral questions ab...           Verbatim                1240.0         Conduct & Behavioral - Comments    Conduct & Behavioral             None             None             None                     3.0     10104.0      3437           999999.0                        UC San Diego            10000.0              ACADEMIC AFFAIRS            10002.0                            DIVISIONS/SCHOOLS            10003.0  ARTS & HUMANITIES            10104.0             MUSIC                NaN               NaN                NaN               NaN                NaN                NaN                 19                    Supervisor Effectiveness/Resolves Staff Issues   \n",
       "\n",
       "                                        url_friendly_theme  theme_display_order  avg_sentiment  is_example  \n",
       "0  HaveVoiceWithinMyInstitutionValuedMemberOfMyInstitution                    1            2.0           0  \n",
       "1               SupervisorEffectivenessResolvesStaffIssues                    1            1.0           0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "resident-recording",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_example.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-processing",
   "metadata": {},
   "source": [
    "Remove any rows where the \"txt_col\" are nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smaller-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=train_config['orig_txt_cols'], inplace=True)\n",
    "valid_df.dropna(subset=train_config['orig_txt_cols'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-think",
   "metadata": {},
   "source": [
    "Add a labels column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "specified-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# list(set(train_df.theme.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-intelligence",
   "metadata": {},
   "source": [
    "Consolodate datasets and add a standard column for the text data to be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aggressive-terry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13413"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['is_valid'] = False\n",
    "valid_df['is_valid'] = True\n",
    "\n",
    "df = pd.concat([train_df, valid_df])\n",
    "df[train_config['txt_cols']] = df[train_config['orig_txt_cols']]\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "combined-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_train_data(train_config_updates={}):\n",
    "    config = {**train_config, **train_config_updates}\n",
    "    \n",
    "    train_df = pd.read_csv(config['train_data'])\n",
    "    train_df.dropna(subset=config['orig_txt_cols'], inplace=True)\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    train_df['is_valid'] = False\n",
    "    train_df[train_config['txt_cols']] = train_df[train_config['orig_txt_cols']]\n",
    "    \n",
    "    if ('valid_data' in config and config['valid_data'] is not None):\n",
    "        valid_df = pd.read_csv(config['valid_data'])\n",
    "        valid_df.dropna(subset=config['orig_txt_cols'], inplace=True)\n",
    "        valid_df.reset_index(drop=True, inplace=True)\n",
    "        valid_df['is_valid'] = True\n",
    "        valid_df[train_config['txt_cols']] = valid_df[train_config['orig_txt_cols']]\n",
    "        \n",
    "        return pd.concat([train_df, valid_df])\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "focused-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_train_data()\n",
    "test(len(df), 0, operator.ge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-argentina",
   "metadata": {},
   "source": [
    "Using the mid-level `DataBlocks` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "opened-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"facebook/bart-base\" #\"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cooked-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model), \n",
    "    RegressionBlock(),\n",
    "    CategoryBlock()\n",
    ")\n",
    "\n",
    "def get_x(inp): return ': '.join(inp[train_config['txt_cols']].values)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=get_x, \n",
    "                   get_y=[ColReader('avg_sentiment'), ColReader('is_example')], \n",
    "                   splitter=ColSplitter(col='is_valid'), \n",
    "                   n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "determined-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "set_seed(TL_RAND_SEED)\n",
    "dls = dblock.dataloaders(df, bs=train_config['batch_size'], num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "identical-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Target vocab has (2 items)\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print(f'The Target vocab has ({len(dls.vocab)} items)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "standing-declaration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inputs vocab (50265 items), and the targets (2 items)\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print((f'The inputs vocab ({dls.before_batch[0].hf_tokenizer.vocab_size} items), '\n",
    "       f'and the targets ({len(dls.vocab)} items)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "headed-laugh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " torch.Size([8, 465]),\n",
       " torch.Size([8]),\n",
       " torch.Size([8]),\n",
       " 'torch.cuda.FloatTensor',\n",
       " 'torch.cuda.LongTensor')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "b = dls.one_batch()\n",
    "len(b), b[0]['input_ids'].shape, b[1].shape, b[2].shape, b[1].type(), b[2].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "indian-portable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Supervisor Effectiveness/Resolves Staff Issues: In the Enterprise Network and Telecommunications group of ITS, the environment continues to be toxic, retaliatory, abusive, and discriminatory as in past years. Under the direction of James Seddon, supervisor Malerie Samadi harasses and talks down to employees. Manipulates and edits official documentation to make staff look and give lower performance appraisal ratings or retaliate against them. Also, use the same practice to provide their friends with higher performance ratings and award them with higher merit increases.  Senior management continues to harbor this behavior without any consequence and or accountability. James and Malerie exploit minorities and give preferential treatment to personnel hired by them. Regularly abuse the power that the University of California gives them and exercise nepotism because they are both product of such practices.\\r\\nOther supervisors like Ynez Hicks also participates in the same practices of despotism, nepotism, and favoritism. \\r\\nIt is unacceptable that this behavior and practices continue to be used by management without any accountability.  Principles of the community are ignored continuously daily. When this type of situation is brought up to Human Resources, they ignore them because no one is enforcing the UC Principles. James Seddon and Malerie Samadi in the Datacom group don't care about staff promotion, compensation, and well being. They only care about themselves and their friends. Hiring practices are unfair; they manipulate the process so they can hire barely qualified personnel into experienced positions. Existing staff is overworked because newer personnel can not pull their weight, yet Malerie and James make it look like they are in their appraisals. They mentally abuse staff and minimize their work performance. Recently one of the team member past away while working at home. A stroke caused by the stress and the pressure that Malerie was putting on David Ramirez. She used him to get her promotion to supervisor and make her look good in front of others.\\r\\nBoth James and Malerie are the perfect examples of bad management. Somehow they continue to occupy their positions, and Senior Management doesn't do anything about it.  \\r\\nThe lowest level the Datacom team has been in years, all because of James and Malerie's arrogance and lack of ethics.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Have Voice within my Institution/Valued Member of my Institution: It is tough to answer the questions this year. \\r\\nI used to know my resources, and feel positively about training. I do not feel the same since ESR. RSC has limited job functionality because OFC was rolled-out without appropriate reporting and or cost transfers. We have spent many frustrating hours trying to figure out how to access the systems and functions that are up and running (Path). We are left on the front lines with faculty and departments with inadequate tools to help.\\r\\nThe issues above do not make me feel valued as a member of the UC San Diego community or like leadership listens. It feels like no one even stopped to think about what a fund manager does, and how they will be able to continue those functions in the new systems. \\r\\nI realize now it is hard for me to comment on inclusion. I think we do a good job, or at least try to, but I am not in a group that would have ever felt excluded based on my background or orientation.  \\r\\nAmount of work remains an issue. The job expands to the hours allotted. We could do a lot better, more thorough and thoughtful work if we had some space to breath and think. Instead we are running from one fire to the next. Many areas had to learn 1 new ESR system. We had to learn ALL of them. It's WAY too much. On top of already having more than enough work to fill every minute of a 40 hr week. And we have learned that somewhere in the ESR process, it was decided (without our knowledge/input/concurrence) that we would take on more duties.  Yay.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-audit",
   "metadata": {},
   "source": [
    "Codify building training dls with optional caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enabling-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_train_x(inp, txt_cols): return 'theme: ' + ' comment: '.join(inp[txt_cols].values)\n",
    "\n",
    "def get_train_dls(df, hf_arch, hf_config, hf_tokenizer, hf_model, \n",
    "                  train_config_updates={}, use_cache=False):\n",
    "    \n",
    "    config = {**train_config, **train_config_updates}\n",
    "    cache_path = config['cache_data_path'] if ('cache_data_path' in config) else None\n",
    "    \n",
    "    if (use_cache and cache_path is not None):\n",
    "        if (os.path.isfile(cache_path)): \n",
    "            dls = torch.load(cache_path)\n",
    "            dls.bs = config['batch_size']\n",
    "            return dls\n",
    "    \n",
    "    blocks = (\n",
    "        HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=config['max_seq_length']), \n",
    "        RegressionBlock(),\n",
    "        CategoryBlock()\n",
    "    )\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=partial(get_train_x, txt_cols=config['txt_cols']),\n",
    "                   get_y=[ColReader('avg_sentiment'), ColReader('is_example')], \n",
    "                   splitter=ColSplitter(col='is_valid'), \n",
    "                   n_inp=1)\n",
    "    \n",
    "    set_seed(TL_RAND_SEED)\n",
    "    dls = dblock.dataloaders(df, bs=config['batch_size'], num_workers=0)\n",
    "    if (cache_path is not None): torch.save(dls, config['cache_data_path'])\n",
    "        \n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-bhutan",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "effective-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_train_data()\n",
    "dls = get_train_dls(df, hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "test_eq(dls.bs, train_config['batch_size'])\n",
    "test_eq(2, len(dls.vocab))\n",
    "\n",
    "b = dls.one_batch()\n",
    "test_eq(len(b), 3)\n",
    "test_eq(b[1].shape[0], dls.bs)    # = regression task\n",
    "test_eq(b[2].shape[0], dls.bs)    # = classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "german-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_train_dls(df, hf_arch, hf_config, hf_tokenizer, hf_model, use_cache=True)\n",
    "\n",
    "test_eq(dls.bs, train_config['batch_size'])\n",
    "test_eq(2, len(dls.vocab))\n",
    "\n",
    "b = dls.one_batch()\n",
    "test_eq(len(b), 3)\n",
    "test_eq(b[1].shape[0], dls.bs)    # = regression task\n",
    "test_eq(b[2].shape[0], dls.bs)    # = classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cultural-corps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theme: Supervisor Effectiveness/Resolves Staff Issues comment: In the Enterprise Network and Telecommunications group of ITS, the environment continues to be toxic, retaliatory, abusive, and discriminatory as in past years. Under the direction of James Seddon, supervisor Malerie Samadi harasses and talks down to employees. Manipulates and edits official documentation to make staff look and give lower performance appraisal ratings or retaliate against them. Also, use the same practice to provide their friends with higher performance ratings and award them with higher merit increases.  Senior management continues to harbor this behavior without any consequence and or accountability. James and Malerie exploit minorities and give preferential treatment to personnel hired by them. Regularly abuse the power that the University of California gives them and exercise nepotism because they are both product of such practices.\\r\\nOther supervisors like Ynez Hicks also participates in the same practices of despotism, nepotism, and favoritism. \\r\\nIt is unacceptable that this behavior and practices continue to be used by management without any accountability.  Principles of the community are ignored continuously daily. When this type of situation is brought up to Human Resources, they ignore them because no one is enforcing the UC Principles. James Seddon and</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>theme: Favoritism/Cliques comment: In the Enterprise Network and Telecommunications group of ITS, the environment continues to be toxic, retaliatory, abusive, and discriminatory as in past years. Under the direction of James Seddon, supervisor Malerie Samadi harasses and talks down to employees. Manipulates and edits official documentation to make staff look and give lower performance appraisal ratings or retaliate against them. Also, use the same practice to provide their friends with higher performance ratings and award them with higher merit increases.  Senior management continues to harbor this behavior without any consequence and or accountability. James and Malerie exploit minorities and give preferential treatment to personnel hired by them. Regularly abuse the power that the University of California gives them and exercise nepotism because they are both product of such practices.\\r\\nOther supervisors like Ynez Hicks also participates in the same practices of despotism, nepotism, and favoritism. \\r\\nIt is unacceptable that this behavior and practices continue to be used by management without any accountability.  Principles of the community are ignored continuously daily. When this type of situation is brought up to Human Resources, they ignore them because no one is enforcing the UC Principles. James Seddon and Malerie Sam</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-retirement",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-cooling",
   "metadata": {},
   "source": [
    "Configure our **metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alike-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# define metrics\n",
    "def sentiment_mse(preds, *targs):\n",
    "    return fa_metrics.mse(preds[0], targs[0])\n",
    "\n",
    "def is_example_acc(preds, *targs):\n",
    "    return fa_metrics.accuracy(preds[1], targs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "technological-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_metrics(train_config_updates={}):\n",
    "    config = {**train_config, **train_config_updates}\n",
    "    return [ sentiment_mse, is_example_acc ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-cologne",
   "metadata": {},
   "source": [
    "Configure our **loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "confirmed-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 148.02469135802468]\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "is_example_weights = list(np.max(train_df.is_example.value_counts()) /train_df.is_example.value_counts())\n",
    "print(is_example_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "practical-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# define our custom multi-target loss\n",
    "loss_func = MultiTargetLoss(loss_classes=[MSELossFlat, CrossEntropyLossFlat],\n",
    "                            loss_classes_kwargs=[{}, {'weight': FloatTensor(is_example_weights).to('cuda:1')}],\n",
    "                            weights=[1, 0.1], \n",
    "                            reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dying-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_loss_func(dls, train_df=None, use_weighted=False):\n",
    "    loss_classes_kwargs = [{}, {}]\n",
    "    \n",
    "    if (use_weighted and train_df is not None):\n",
    "        is_example_weights = list(np.max(train_df.is_example.value_counts()) /train_df.is_example.value_counts())\n",
    "        loss_classes_kwargs[1] = {'weight': FloatTensor(is_example_weights).to(dls.device)}\n",
    "    \n",
    "    loss_func = MultiTargetLoss(loss_classes=[MSELossFlat, CrossEntropyLossFlat],\n",
    "                                loss_classes_kwargs=loss_classes_kwargs,\n",
    "                                weights=[1, 0.1], \n",
    "                                reduction='mean')\n",
    "        \n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "reasonable-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_loss_func = get_loss_func(dls, train_df, use_weighted=False)\n",
    "test_is(type(tst_loss_func), MultiTargetLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dirty-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_loss_func = get_loss_func(dls, train_df, use_weighted=True)\n",
    "test_is(type(tst_loss_func), MultiTargetLoss)\n",
    "test_eq(len(tst_loss_func.loss_funcs[1].func.weight), len(dls.c))\n",
    "test_eq(dls.device, tst_loss_func.loss_funcs[1].func.weight.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-champagne",
   "metadata": {},
   "source": [
    "Configure our **callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecological-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Meta_MM_HF_BaseModelCallback(HF_BaseModelCallback):\n",
    "    def __init__(self, cls_idx=0):\n",
    "        super().__init__()\n",
    "        self.cls_idx = cls_idx\n",
    "        \n",
    "    def after_pred(self):\n",
    "        super().after_pred()\n",
    "        if (self.learn.pred[0].dim() == 3):\n",
    "            self.learn.pred = (self.learn.pred[0][:,self.cls_idx,:], self.learn.pred[1][:,self.cls_idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "filled-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cbs(train_config_updates={}, add_save_model_cb=True):\n",
    "    config = {**train_config, **train_config_updates}\n",
    "    fit_cbs = []\n",
    "    \n",
    "    best_model_cb = SaveModelCallback(monitor=config['save_model_monitor'], \n",
    "                                      comp=config['save_model_comp'], \n",
    "                                      fname=config['save_model_filename'],\n",
    "                                      reset_on_fit=False)\n",
    "    \n",
    "    if (add_save_model_cb): fit_cbs.append(best_model_cb)\n",
    "    \n",
    "    return [Meta_MM_HF_BaseModelCallback], fit_cbs # (learn_cbs, fit_cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-rover",
   "metadata": {},
   "source": [
    "Configure our **final model** by updating our hf_model.classifier for multi-modal tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "touched-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# class Meta_MM(Module):\n",
    "#     def __init__(self, in_features=768, inner_dim=768, p=0.0): \n",
    "#         super().__init__()\n",
    "#         self.dense = nn.Linear(in_features, inner_dim)\n",
    "#         self.dropout = nn.Dropout(p=p)\n",
    "#         self.pred_is_example = nn.Linear(inner_dim, 2, bias=False)\n",
    "#         self.pred_avg_sentiment = nn.Linear(inner_dim, 1, bias=False)\n",
    "#         self.pred_avg_sent_range = SigmoidRange(1., 5.1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.dense(x)\n",
    "#         x = torch.tanh(x)\n",
    "#         x = self.dropout(x)\n",
    "#         is_example = self.pred_is_example(x)\n",
    "#         avg_sentiment = self.pred_avg_sent_range(self.pred_avg_sentiment(x))\n",
    "        \n",
    "#         return avg_sentiment, is_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cloudy-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Meta_MM(Module):\n",
    "    def __init__(self, in_features=50): \n",
    "        super().__init__()\n",
    "        self.pred_is_example = nn.Linear(in_features, 2, bias=False)\n",
    "        self.pred_avg_sentiment = nn.Linear(in_features, 1, bias=False)\n",
    "        self.pred_avg_sent_range = SigmoidRange(1., 5.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        is_example = self.pred_is_example(x)\n",
    "        avg_sentiment = self.pred_avg_sent_range(self.pred_avg_sentiment(x))\n",
    "        \n",
    "        return avg_sentiment, is_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "wrong-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "last_layer = list(hf_model.named_children())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "political-slope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classification_head'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "last_layer_name = last_layer[0]; last_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "floating-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "in_features = hf_model._modules[last_layer_name].dense.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "significant-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# hf_model._modules[last_layer_name] = Meta_MM(in_features=in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "steady-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# in_features = list(last_layer[1].children())[0].in_features; in_features\n",
    "# mm_model = nn.Sequential(list(hf_model.named_children())[:-1][0][1])\n",
    "# mm_model.add_module(name='classifier', module=MM(in_features=768))\n",
    "# hf_model._modules['classifier'] = hf_model._modules.pop(last_layer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-jimmy",
   "metadata": {},
   "source": [
    "Configure our **Learner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ideal-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "try: del learn\n",
    "except: pass\n",
    "finally: gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "preliminary-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_learner(hf_model, dls, train_df=None, use_weighted_loss=False, use_fp16=True,\n",
    "                opt_func=partial(Adam, mom=0.9, sqr_mom=0.98, eps=1e-6, wd=0.1),\n",
    "                add_save_model_cb=True, train_config_updates={}):\n",
    "    \n",
    "    config = {**train_config, **train_config_updates}\n",
    "    \n",
    "    # swap out classifier for our Meta_MM module\n",
    "    last_layer = list(hf_model.named_children())[-1]\n",
    "    last_layer_name = last_layer[0]\n",
    "    in_features = hf_model._modules[last_layer_name].dense.in_features\n",
    "    hf_model._modules[last_layer_name] = Meta_MM(in_features=in_features)\n",
    "    \n",
    "    # build learner\n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    loss_func = get_loss_func(dls, train_df, use_weighted_loss)\n",
    "    learn_cbs, fit_cbs = get_cbs(config, add_save_model_cb=add_save_model_cb)\n",
    "    learn_metrics = get_metrics(config)\n",
    "\n",
    "    set_seed(TL_RAND_SEED)\n",
    "    learn = Learner(dls, model, loss_func=loss_func, opt_func=opt_func, \n",
    "                    metrics=learn_metrics, cbs=learn_cbs, splitter=hf_splitter, path=config['learner_path'])\n",
    "    \n",
    "    if (use_fp16): learn = learn.to_fp16()\n",
    "    learn.create_opt() # -> will create your layer groups based on your \"splitter\" function\n",
    "    learn.freeze()\n",
    "    \n",
    "    return learn, fit_cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "operating-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config_updates={'save_model_filename': f\"exp_{train_config['save_model_filename']}\" }\n",
    "                      \n",
    "learn, fit_cbs = get_learner(hf_model, dls, train_df, train_config_updates=train_config_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "sudden-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "small-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "sunrise-allah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.003981071710586548, 1.0964781722577754e-06)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_min, lr_steep = learn.lr_find(); lr_min, lr_steep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "excess-luther",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003981071710586548"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lr_min; lr_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "baking-sister",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>sentiment_mse</th>\n",
       "      <th>is_example_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.451275</td>\n",
       "      <td>0.455106</td>\n",
       "      <td>0.451222</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>03:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.45510587096214294.\n"
     ]
    }
   ],
   "source": [
    "set_seed(TL_RAND_SEED)\n",
    "learn.fit_one_cycle(1, lr_max=lr, cbs=fit_cbs)\n",
    "# learn.fit_flat_cos(10, lr_max=lr_min, cbs=fit_cbs, pct_start=0.72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "subject-bicycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3.311311274956097e-07, 0.002511886414140463)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "lr_min, lr_steep = learn.lr_find(); lr_min, lr_steep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "mechanical-legislature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.311311274956097e-07"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lr_min; lr_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "paperback-chest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>sentiment_mse</th>\n",
       "      <th>is_example_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.444798</td>\n",
       "      <td>0.453417</td>\n",
       "      <td>0.449536</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.503554</td>\n",
       "      <td>0.447229</td>\n",
       "      <td>0.443352</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.477169</td>\n",
       "      <td>0.443810</td>\n",
       "      <td>0.439936</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.400236</td>\n",
       "      <td>0.442045</td>\n",
       "      <td>0.438180</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.396645</td>\n",
       "      <td>0.442302</td>\n",
       "      <td>0.438444</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.495828</td>\n",
       "      <td>0.441175</td>\n",
       "      <td>0.437319</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.480493</td>\n",
       "      <td>0.440538</td>\n",
       "      <td>0.436687</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.409209</td>\n",
       "      <td>0.440267</td>\n",
       "      <td>0.436419</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.430810</td>\n",
       "      <td>0.440109</td>\n",
       "      <td>0.436261</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.422115</td>\n",
       "      <td>0.440022</td>\n",
       "      <td>0.436175</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.45341694355010986.\n",
      "Better model found at epoch 1 with valid_loss value: 0.44722941517829895.\n",
      "Better model found at epoch 2 with valid_loss value: 0.44380953907966614.\n",
      "Better model found at epoch 3 with valid_loss value: 0.4420449733734131.\n",
      "Better model found at epoch 5 with valid_loss value: 0.4411749243736267.\n",
      "Better model found at epoch 6 with valid_loss value: 0.4405377507209778.\n",
      "Better model found at epoch 7 with valid_loss value: 0.44026699662208557.\n",
      "Better model found at epoch 8 with valid_loss value: 0.44010862708091736.\n",
      "Better model found at epoch 9 with valid_loss value: 0.44002217054367065.\n"
     ]
    }
   ],
   "source": [
    "set_seed(TL_RAND_SEED)\n",
    "learn.fit_one_cycle(10, lr_max=slice(lr/10, lr), cbs=fit_cbs)\n",
    "# learn.fit_flat_cos(5, lr_max=slice(lr_min/10, lr_min), cbs=fit_cbs, pct_start=0.72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adjustable-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('data/classification/standard_themes/meta/models/exp_verbatim_standard_theme_meta_multilabel_hf.pth')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save(f\"exp_{train_config['m_pre']}{train_config['base_model_name']}{train_config['m_suf']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-reservoir",
   "metadata": {},
   "source": [
    "Look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "heavy-weekend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "      <th>category</th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theme: Advancement and Training Opportunities comment: 1.\\tSensitivity and bias training for SLBO management  The selection and search for the person to fill Evelyns position seemed inequitable.  Evelyn having a say in her successor may not have been against campus hiring policies but did not seem appropriate since she had inside knowledge of some of the candidates and a possible bias for or against other candidates.  \\r\\n  I applied for the position because my applying was suggested by another member of Evelyns team that was not on the committee.\\r\\nAnna, a member of the committee, was also under the leadership of Evelyn.  Anna, who now has been promoted into Jamies positions would not be in Jamies position if he had not gotten Evelyns position.  In addition, there was no one in the room from HR ensuring that the interviews were fair and equal.  I think that this Cluster is large enough to warrant additional oversite with the filling of positions. \\r\\nThough it may not matter to some but within the SLBO all of senior management (HR remains neutral) is now male while 99 %  of the rest of the staff is female.  \\r\\n2.\\t</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>(2.078125,)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>theme: Fear of Retaliation, Negative Consequences comment: Staff no longer receive COLAs, like the faculty do--we are instead told that our COLA is a merit increase and expected to believe this.\\r\\n\\r\\nThis is incredibly demoralizing and many talented staff with institutional longevity are considered leaving UCSD for higher compensation elsewhere, at least as far as I am able to report by personal conversation. The turnover loss to the department if this were to happen would be substantial. Experienced career staff have knowledge that they can generalize from to solve the unique problems that arise frequently in the university setting. New staff cannot always provide these solutions to faculty.\\r\\n\\r\\nFurther our'merit increases' were not announced in a timely manner and apparently do not include July, that is, they are effective Aug 1. I do not think our faculty would accept this!  It is true that the faculty attract fee-paying students to campus but the institution truly is a three legged stool--charitably I do not think our department could be run with faculty alone.\\r\\n\\r\\nAsides from compensation, which is an UCOP / EVCAA issue, Mathematics is a good department</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>(2.109375,)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "broadband-queue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'valid_loss': 0.44002217054367065,\n",
       " 'sentiment_mse': 0.43617454171180725,\n",
       " 'is_example_acc': 0.9932935833930969}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = dict(zip(learn.recorder.metric_names[2:], learn.validate())); scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "liquid-piece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/tritonlytics-ai/lib/python3.8/site-packages/fastai/learner.py:56: UserWarning: Saved filed doesn't contain an optimizer state.\n",
      "  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.44002217054367065\n"
     ]
    }
   ],
   "source": [
    "learn = learn.load(f\"exp_{train_config['save_model_filename']}\")\n",
    "probs, targs, loss = learn.get_preds(dl=dls.valid, with_loss=True)\n",
    "\n",
    "print(f'Validation Loss: {loss.mean()}')\n",
    "# print(f'Validation Loss (per label): {loss.mean(dim=0)}') # ... no longer works (see forum comment from sylvain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "motivated-celebration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0039]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.2305]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.3672]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9053]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8281]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.9570]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8750]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.0312]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.9785]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9111]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4297]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([3.1777]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3047]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8398]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8066]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2969]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8945]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8691]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7959]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([1.8809]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3965]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.2559]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.9492]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1113]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1289]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8770]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2969]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.0664]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0977]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8838]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2324]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.3906]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([3.8457]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8193]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9658]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7461]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.0176]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([3.6758]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.4238]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9746]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3418]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0391]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0117]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2266]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2266]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.9473]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.7559]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9082]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2910]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3281]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([3.7930]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([3.5742]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3340]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8652]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0508]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8691]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.1484]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9043]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8789]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.4805]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([1.9336]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3125]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9121]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0312]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1367]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0352]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8604]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0020]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2070]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9434]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([2.7734]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.1484]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8691]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5586]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8750]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0898]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9395]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9668]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9336]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9326]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8936]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.0527]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8926]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3848]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0820]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1211]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0801]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9121]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0918]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.4336]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6133]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1035]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.6797]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.1172]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8906]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9824]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3516]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([1.9668]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9844]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.3203]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.2910]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8359]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.0781]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.9102]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0625]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3262]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0117]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8555]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.3047]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.0078]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0352]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.3848]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9131]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3516]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9883]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.1406]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.7559]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1172]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1719]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1875]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4590]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3105]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8008]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2363]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0156]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9824]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.5391]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9482]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2168]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.6094]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9385]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1602]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.8164]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9502]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2188]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.7949]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.7871]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1602]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0703]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3906]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9199]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6172]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4297]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3867]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0391]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0371]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7441]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0645]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0977]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2227]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9531]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8242]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9414]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2773]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0859]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4297]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6055]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.1406]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.2500]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8750]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0664]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9551]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.0273]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9160]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0332]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.5488]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.0273]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.2969]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1562]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9658]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9453]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.1172]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.7891]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.0312]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.7773]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1914]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.1250]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.0703]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1660]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8467]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.1797]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2109]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1230]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.6113]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1367]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7344]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0820]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3066]) tensor(3.) tensor(0) tensor(1)\n",
      "tensor([4.4297]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.2539]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0938]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([2.2188]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([1.9092]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0488]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8594]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.3047]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.1191]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2969]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9141]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8057]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1953]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1055]) tensor(2.) tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0430]) tensor(2.) tensor(0) tensor(1)\n",
      "tensor([2.0078]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.0156]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.2500]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0352]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1855]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8281]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.1328]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.6973]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.6230]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0293]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2676]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9766]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8262]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.4805]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.7598]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3359]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9756]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.7812]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1602]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4648]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.3281]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8848]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8066]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6133]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9834]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8750]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.2617]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.0977]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9521]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4297]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.2969]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7891]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0703]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7676]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9961]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0586]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9482]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8047]) tensor(2.) tensor(0) tensor(1)\n",
      "tensor([2.0898]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7832]) tensor(1.) tensor(0) tensor(1)\n",
      "tensor([2.0508]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9160]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0723]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8057]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.7617]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.2969]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8340]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9922]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.4453]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3984]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.7383]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.4316]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9590]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1172]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0391]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.7617]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.8496]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9434]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0586]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4844]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9668]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1211]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0742]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8809]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2051]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9297]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.5000]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0293]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2891]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9746]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0703]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0898]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8770]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0977]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9287]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.6328]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2031]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8994]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1445]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9590]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8154]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.6875]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9814]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.4297]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.6348]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9268]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0684]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.2148]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9375]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1113]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0254]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8750]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2773]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1250]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0664]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.3066]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8896]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9912]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9668]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0215]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2344]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9277]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9648]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.0234]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8086]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4727]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0684]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0938]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8613]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3320]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.7734]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.7188]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9082]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9219]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2812]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0859]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.9922]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9805]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9648]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([1.9805]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5586]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5117]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1328]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2539]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.4180]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.9395]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9180]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8906]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4766]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.0391]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([2.1289]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8281]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([1.7500]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9883]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9785]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8135]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0820]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9990]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([1.9512]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2344]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3691]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0859]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1777]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1406]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9824]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.9297]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0703]) tensor(2.) tensor(0) tensor(1)\n",
      "tensor([1.8652]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9277]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([2.0195]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0996]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9277]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.9102]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2715]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4375]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.3301]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8555]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2422]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.1523]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3281]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2734]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8906]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1211]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0176]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9844]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.9043]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9551]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9434]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0645]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.2441]) tensor(3.5000) tensor(0) tensor(0)\n",
      "tensor([3.6738]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8672]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.4727]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8838]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.4023]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([1.9727]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7578]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0996]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9551]) tensor(3.) tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9004]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([4.4375]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.2500]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9199]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0586]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6387]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8682]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0391]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8867]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2656]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9219]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0742]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0234]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0586]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1641]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.3906]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.3086]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1914]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.7031]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1289]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4609]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.4336]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0879]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7480]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6016]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2695]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0195]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2305]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.7168]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9180]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2109]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.1855]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.3652]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.5566]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2246]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9814]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8027]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.1094]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2852]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2402]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0469]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2070]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9199]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9629]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.7305]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.0840]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2773]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9062]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1016]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.1484]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0664]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.2188]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2051]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0332]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5566]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([3.2559]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.5254]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.2500]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.7920]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2031]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8291]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1523]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2773]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.1641]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.4688]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0586]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.7676]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3027]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0957]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0059]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.2188]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.7148]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9014]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0195]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9863]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0078]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3672]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.9961]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8340]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0156]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2500]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0312]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9531]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9072]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9248]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.7773]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3555]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([1.9336]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4844]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.5234]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([1.8848]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9863]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0176]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0957]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3926]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0332]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.2422]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3828]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.3516]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9062]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8750]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.5703]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8477]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6621]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.9336]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8242]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.1484]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9941]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1055]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8545]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.7930]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.1328]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.2227]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.6797]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9023]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4609]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.3711]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1445]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1289]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.5000]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0703]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3262]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2344]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.8105]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9785]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.1016]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([3.3887]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.9814]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8643]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1445]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2656]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.0938]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.6875]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1758]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.2598]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1445]) tensor(2.) tensor(0) tensor(1)\n",
      "tensor([1.7656]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([2.2031]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.4336]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0352]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.8516]) tensor(4.5000) tensor(0) tensor(0)\n",
      "tensor([1.9092]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9980]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0781]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.7930]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9443]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8398]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1602]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0859]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9287]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.4922]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.1016]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3867]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.7227]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.4023]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.7207]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9863]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5664]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8711]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3008]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9531]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1914]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.7393]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9668]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.4414]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7920]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2773]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0352]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0117]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8242]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9609]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8691]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3477]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1250]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0234]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.3066]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([4.4141]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.5156]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0234]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.8320]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8398]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0703]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0332]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3398]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1133]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3047]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9590]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9336]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0938]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1719]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1758]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9258]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0664]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7500]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8105]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.5039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2266]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.8145]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8984]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4570]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([3.0781]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9824]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6133]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0352]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1484]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3828]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.4863]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1445]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7871]) tensor(1.) tensor(0) tensor(1)\n",
      "tensor([2.2461]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8809]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9424]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9209]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.0781]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.0566]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5859]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.7871]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1367]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1035]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8379]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9453]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0117]) tensor(2.) tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.8672]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8105]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.4414]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8750]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.0547]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([3.7051]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9824]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0020]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0254]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4688]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8730]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0645]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.2871]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0781]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9424]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9805]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3242]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0742]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1758]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.0215]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9688]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3301]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.7070]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9326]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9062]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9082]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3828]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0527]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1035]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8770]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1035]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8125]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1172]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3398]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.7266]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0742]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8945]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0625]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2969]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.1855]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9248]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0762]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3223]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.8984]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([3.0137]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.7900]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1504]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.8047]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3125]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1191]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5273]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8789]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0996]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9414]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8535]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.6992]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9766]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1250]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9814]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8613]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1211]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8848]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9590]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8418]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.5469]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8086]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3047]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9141]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9570]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3594]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0625]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9746]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8711]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.2773]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1562]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9160]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.2188]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.6582]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6250]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9844]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.5625]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.9414]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9199]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9688]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8809]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3438]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.1055]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3984]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7578]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8545]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8809]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.3203]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8848]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.4219]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0195]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8203]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2324]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3418]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.7949]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.4844]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9697]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.0117]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9395]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2031]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1562]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8574]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9697]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0117]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.1797]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8828]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4004]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0469]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9707]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.1562]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.1367]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.9688]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4336]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1055]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.3672]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.3125]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1914]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8896]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0352]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9805]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0430]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.2520]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.2891]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.3281]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2344]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([3.5469]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.4844]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1777]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.3906]) tensor(5.) tensor(0) tensor(1)\n",
      "tensor([2.0254]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0469]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3164]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0840]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2617]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3066]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2207]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8926]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7422]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1953]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.9629]) tensor(4.5000) tensor(0) tensor(0)\n",
      "tensor([1.9258]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8926]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1758]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9902]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8848]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.7090]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2500]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9062]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3105]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8613]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8242]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3633]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.3516]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2324]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3086]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1680]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0176]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1660]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4688]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8828]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0020]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0742]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.2051]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9961]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0176]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.7695]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0781]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.0547]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.5117]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0820]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3008]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0508]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4922]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8633]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3086]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.4844]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3711]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(1.) tensor(0) tensor(1)\n",
      "tensor([1.9609]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9297]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1641]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.0117]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.2969]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.1270]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([3.1250]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8203]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.5762]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([3.1348]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9902]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.1172]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.4688]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8730]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2773]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.9785]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4824]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.8047]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0781]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.3281]) tensor(4.5000) tensor(0) tensor(0)\n",
      "tensor([2.7324]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2930]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0195]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0781]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0508]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([2.5703]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3672]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.3730]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9072]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1797]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9395]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.2383]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8672]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.2656]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3320]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9238]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.8594]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9121]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9434]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([1.8330]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0898]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0938]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0430]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9268]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([2.4648]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.4297]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([3.9570]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9883]) tensor(2.) tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0977]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7754]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.4531]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9414]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8789]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.8613]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1680]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.1602]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9590]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9941]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8525]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.7461]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2695]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1016]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.3984]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.4648]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7363]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.2461]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9824]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0078]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9219]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.0059]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0645]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.1758]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.2422]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8379]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9531]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9688]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1055]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.7764]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8477]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1914]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0391]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9688]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.6270]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3027]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0273]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.5371]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.9551]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8496]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.5039]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8447]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8115]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9004]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4844]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.0488]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9668]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3281]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.2148]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9375]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0703]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1250]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1133]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2812]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9219]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([4.1602]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.2402]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([1.9922]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1523]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8604]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.5918]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([4.4414]) tensor(4.5000) tensor(0) tensor(0)\n",
      "tensor([4.3242]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1641]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8477]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.4414]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.4453]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0469]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9834]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9961]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2500]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.0293]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2207]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.3145]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9902]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.8477]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9238]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.0938]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([3.1562]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9277]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.0176]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0176]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.3594]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.9424]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8477]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.2305]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.1094]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.5840]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1328]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9092]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8984]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9375]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8994]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2422]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9531]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3750]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8262]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3203]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1641]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0840]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2578]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1445]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8643]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8906]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.3633]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1367]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1680]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0078]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.3770]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.4297]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([3.5371]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3105]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0195]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3535]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2148]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0371]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.1094]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.4668]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0156]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0254]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0430]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8398]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9902]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.5820]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0820]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9004]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.4629]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2891]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9629]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1230]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9639]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3633]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.3438]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0156]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9805]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8477]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0586]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4766]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.4062]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.8770]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8184]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0156]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0801]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0332]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9648]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2422]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.5234]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1641]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9805]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0605]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0742]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0742]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([1.8340]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5918]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.4629]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1602]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1523]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1289]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0840]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0078]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0820]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([1.9697]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0352]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2461]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0312]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0664]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7646]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8086]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9961]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8037]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.8262]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2285]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3125]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0488]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.3906]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2852]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0488]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6641]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0078]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1738]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8877]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6797]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8770]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.5957]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1055]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6602]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1406]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([4.3867]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1504]) tensor(2.5000) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.5762]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.5742]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1367]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1523]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8955]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.3672]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.4453]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.8926]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2969]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9238]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9922]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1875]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8359]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.7363]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.5195]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.0430]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.1660]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9863]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2109]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([2.3340]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2812]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.6230]) tensor(1.) tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.8496]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.9805]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0801]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9297]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.6133]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9277]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3555]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.9629]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.0078]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2070]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7793]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9102]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1367]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1230]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8955]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.4141]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2656]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8457]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1328]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.1680]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.8672]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.2930]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2559]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1445]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0332]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4375]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8516]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.9844]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8711]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.1484]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1133]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9932]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0469]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.5078]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8164]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5898]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1406]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.3125]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.2871]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.6777]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9863]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9727]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0605]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1445]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0273]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1895]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0820]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3320]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2285]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.7949]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2969]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0312]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.4805]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2383]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0430]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9609]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3438]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.9434]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.5391]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7227]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.1875]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0195]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4883]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.5039]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1719]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0078]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1328]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.6348]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1367]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1172]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8867]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2461]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8408]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9355]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9990]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9570]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8613]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9453]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0176]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.9102]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.3223]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0469]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9385]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6016]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.3906]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2344]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([4.2266]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1680]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8936]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0566]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.5703]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9053]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5625]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([2.0391]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9785]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4766]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.8984]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.5449]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8770]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0430]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0527]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8975]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9756]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2422]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9619]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1055]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5039]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8945]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2930]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0977]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7607]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0742]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2383]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5898]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5742]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.7617]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9961]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1973]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0645]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8672]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([3.9277]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.8379]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([4.3516]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.0898]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9316]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9287]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8125]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9121]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7891]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0781]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9668]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0547]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.6426]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0898]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2148]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.3086]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8125]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2383]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1113]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.7461]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.2969]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.4297]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.3438]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9727]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0898]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8105]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1992]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2168]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.4375]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1172]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.1641]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9385]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9883]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9551]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8652]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1797]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2695]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9277]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.9785]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7637]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9395]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1094]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9014]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.0742]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9668]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.4746]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4297]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3828]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.4219]) tensor(4.5000) tensor(0) tensor(0)\n",
      "tensor([2.0938]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8701]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.9746]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.8828]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([3.0840]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8770]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4336]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2188]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6562]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3984]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8418]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0020]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2188]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8672]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1641]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.1680]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0430]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0430]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8809]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.6172]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0352]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.1289]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.9180]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0508]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3125]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([1.8545]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5859]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([1.9844]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0059]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8535]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([1.8906]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6992]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8965]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9004]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.4531]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([4.3359]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.4336]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0312]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2031]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2031]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.3750]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.0508]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([1.9658]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.3047]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0957]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9844]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9766]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9385]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5977]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.8438]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.7930]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([4.3438]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1172]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9199]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9229]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.2695]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9785]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8447]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0039]) tensor(1.5000) tensor(0) tensor(0)\n",
      "tensor([4.1992]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1992]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([3.4844]) tensor(4.) tensor(0) tensor(0)\n",
      "tensor([2.1875]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9199]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2715]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.5586]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.6660]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.0117]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9883]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.2500]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([1.9658]) tensor(2.) tensor(0) tensor(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7773]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.4316]) tensor(3.) tensor(0) tensor(0)\n",
      "tensor([2.]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.8574]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([2.0605]) tensor(1.) tensor(0) tensor(0)\n",
      "tensor([2.7051]) tensor(5.) tensor(0) tensor(0)\n",
      "tensor([2.2793]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9785]) tensor(2.) tensor(0) tensor(0)\n",
      "tensor([1.9785]) tensor(2.) tensor(0) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for prob_sent, prob_example, targ_sent, targ_example in zip(*probs,*targs):\n",
    "    print(prob_sent, targ_sent, torch.argmax(prob_example, dim=-1), targ_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-custody",
   "metadata": {},
   "source": [
    "Export model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "distant-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(fname=f\"exp_{train_config['export_filename']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dramatic-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn = load_learner(f\"{train_config['learner_path']}/exp_{train_config['export_filename']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "spiritual-clear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adequate_staffing',\n",
       " 'advancement_and_training_opportunities',\n",
       " 'appropriate_stress_work_assigned_equitably',\n",
       " 'benefits',\n",
       " 'better_ways_recognized_participate_in_decisions']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STANDARD_THEME_SAW_LABELS[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "handled-advocate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(((1.9047832489013672), '0'),\n",
       "  (#2) [tensor([1.9048]),tensor(0)],\n",
       "  (#2) [tensor([1.9048]),tensor([0.9926, 0.0074])])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_learn.blurr_predict('theme: Benefits comment: We are not paid enough and the benefits are horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "clear-valuation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(((2.09375), '0'),\n",
       "  (#2) [tensor([2.0938]),tensor(0)],\n",
       "  (#2) [tensor([2.0938]),tensor([0.9974, 0.0026])])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict(\"theme: Adequate Staffing comment: We don't have enough people to get the job done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-reminder",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "labeled-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "try: del learn; del inf_learn\n",
    "except: pass\n",
    "finally: gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "czech-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"facebook/bart-base\" #\"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "identical-stationery",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/tritonlytics-ai/lib/python3.8/site-packages/fastai/learner.py:56: UserWarning: Saved filed doesn't contain an optimizer state.\n",
      "  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1342, 1]),\n",
       " torch.Size([1342, 2]),\n",
       " torch.Size([1342]),\n",
       " torch.Size([1342]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions for a single model using the learner's model and data loaders\n",
    "set_seed(TL_RAND_SEED)\n",
    "learn, fit_cbs = get_learner(hf_model, \n",
    "                             dls, \n",
    "                             train_df=train_df, \n",
    "                             use_weighted_loss=False, \n",
    "                             use_fp16=True,\n",
    "                             train_config_updates={})\n",
    "\n",
    "learn = learn.load(f\"exp_{train_config['save_model_filename']}\")\n",
    "learn.model.cuda(1)\n",
    "probs, targs  = learn.get_preds()\n",
    "\n",
    "probs[0].shape, probs[1].shape, targs[0].shape, targs[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-french",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Lets look at validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "characteristic-analyst",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "is_example_prob_true = torch.softmax(probs[1], dim=-1)[:,1]\n",
    "# is_example_prob_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "empirical-pharmacology",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# determine optimal threshold based on desired f-score\n",
    "average, sample_weight = train_config['opt_beta_average'], train_config['opt_beta_sample_weight']\n",
    "    \n",
    "f05 = OptimalMultiThresholdMetrics(beta=0.5, start=0.05, end=.5, sigmoid=False, \n",
    "                                   average=average, sample_weight=sample_weight)\n",
    "f1 = OptimalMultiThresholdMetrics(beta=1, start=0.05, end=.5, sigmoid=False, \n",
    "                                   average=average, sample_weight=sample_weight)\n",
    "f2 = OptimalMultiThresholdMetrics(beta=2, start=0.05, end=.5, sigmoid=False, \n",
    "                                   average=average, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "found-airplane",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27, 0.27, 0.27)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_f05 = f05.opt_th(is_example_prob_true, targs[1])\n",
    "threshold_f1 = f1.opt_th(is_example_prob_true, targs[1])\n",
    "threshold_f2 = f2.opt_th(is_example_prob_true, targs[1])\n",
    "\n",
    "threshold_f05, threshold_f1, threshold_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "australian-values",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.010590415673815197, 0.016824395373291272, 0.04089979550102249)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f05_score = f05.opt_fscore(is_example_prob_true, targs[1])\n",
    "f1_score = f1.opt_fscore(is_example_prob_true, targs[1])\n",
    "f2_score = f2.opt_fscore(is_example_prob_true, targs[1])\n",
    "\n",
    "f05_score, f1_score, f2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "monthly-portuguese",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010590415673815197"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure we are getting the same f1 score as sklearn\n",
    "res = skm.fbeta_score(targs[1], (is_example_prob_true > threshold_f05), beta=.5, \n",
    "                      average='binary', sample_weight=None, zero_division=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "adaptive-integrity",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9932935833930969, 0.9932935833930969, 0.9932935833930969)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine accuracy based on optimal threshold\n",
    "val_acc_f05 = accuracy_multi(torch.argmax(probs[1], dim=-1), targs[1], threshold_f05, sigmoid=False).item()\n",
    "val_acc_f1 = accuracy_multi(torch.argmax(probs[1], dim=-1), targs[1], threshold_f1, sigmoid=False).item()\n",
    "val_acc_f2 = accuracy_multi(torch.argmax(probs[1], dim=-1), targs[1], threshold_f2, sigmoid=False).item()\n",
    "\n",
    "val_acc_f05, val_acc_f1, val_acc_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "iraqi-church",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30327868461608887"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure we are getting the same f1 accuracy\n",
    "preds = ((is_example_prob_true > threshold_f05).byte() == targs[1].byte()).float().mean()\n",
    "preds.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-tract",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Overall metrics - is_example_prob_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "latin-somerset",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eval_targs = targs[1].flatten() # targs[:,0]\n",
    "eval_probs = is_example_prob_true.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-creator",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Classification Accuracy\n",
    "\n",
    "The percentage of correct predictions.  Answers the question, **\"Overall, how often is the classifier correct?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "palestinian-consultancy",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30327868852459017\n"
     ]
    }
   ],
   "source": [
    "# In multilabel classification, this function computes subset accuracy: \n",
    "# the set of labels predicted for a sample must exactly match ALL the corresponding set of labels in y_true.\n",
    "print(skm.accuracy_score(eval_targs, (eval_probs > threshold_f05).float(), sample_weight=sample_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-colleague",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Null Accuracy\n",
    " \n",
    "The accuracy achieved by always predicting the most frequent class.  Answers the question, **\"What would the accuracy be by always predicting the most frequent case?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "interested-valley",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1333\n"
     ]
    }
   ],
   "source": [
    "u_classes, u_counts = np.unique(eval_targs, return_counts=True)\n",
    "most_freq_class, most_freq_class_count = u_classes[np.argmax(u_counts)], np.max(u_counts)\n",
    "print(most_freq_class, most_freq_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "religious-motivation",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9932935916542474"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_class_count / len(eval_targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-document",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Cohen's kappa\n",
    "\n",
    "This measure is intended to compare labelings by different human annotators (not a classifier vs. ground truth)\n",
    "\n",
    "Kappa socres are between -1 and 1 ( >= .8 is generally considered good agreement; <= 0 means no agreement ... e.g., practically random labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "photographic-transcript",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0035861591409735993\n"
     ]
    }
   ],
   "source": [
    "print(skm.cohen_kappa_score(eval_targs, (eval_probs > threshold_f05).float(), \n",
    "                            weights=None, sample_weight=sample_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-quick",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Describes the performance of a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "false-documentation",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, \n",
    "                          title='Confusion matrix', cmap=plt.cm.Blues, print_info=False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        if (print_info): print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        if (print_info): print('Confusion matrix, without normalization')\n",
    "\n",
    "    if (print_info): print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "expected-aurora",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cm = skm.confusion_matrix(eval_targs, (eval_probs > threshold_f05).float(), sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "working-gabriel",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_confusion_matrix(cm, classes=u_classes,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_confusion_matrix(cm, classes=u_classes, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "operational-tourist",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.30      0.46      1333\n",
      "           1       0.01      0.89      0.02         9\n",
      "\n",
      "    accuracy                           0.30      1342\n",
      "   macro avg       0.50      0.59      0.24      1342\n",
      "weighted avg       0.99      0.30      0.46      1342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(skm.classification_report(eval_targs, (eval_probs > threshold_f05).float(), \n",
    "                                labels=[0,1], \n",
    "                                sample_weight=sample_weight, \n",
    "                                zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-marketplace",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### ROC curves and Area Under the Curve (AUC)\n",
    "\n",
    "***ROC Curve*** answers the question, *\"How would sensitivity and specificity be affected by various thresholds without changing the threshold?\"*  It is a way **to visualize the performance of a binary classifier.**\n",
    "\n",
    "The ROC curve can help you **choose a threshold** that balances sensitivity and specificity based on your particular business case.\n",
    "\n",
    "ROC curves visualize all possible classification thresholds whereas misclassification rate only represents your error rate for a single threshold.\n",
    "\n",
    "A classifier that does a good job at separating the classes will have a ROC curve that hugs the upper left corner of the plot.  Converseley, a classifier the does a poor job separating the classes will have a ROC curve that is close to the diagonal line (0,0 -> 1,1).  That diagonal line represents a classifier that does no better than random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "phantom-specialist",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = skm.roc_curve(eval_targs, eval_probs, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "addressed-casting",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "plt.xlim = ([0.0, 1.0])\n",
    "plt.ylim = ([0.0, 1.0])\n",
    "plt.title('ROC curve for all labels')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity/Recall)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-values",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***AUC*** = the percentage of the ROC plot that is underneath the curve.  \n",
    "\n",
    "AUC summarizes the performance of a classifier in a **single number**.  It says, *\"If you randomly chose one positive and one negative observation, what is the likelihood that your classifier will assign a higher predicted probability to the positive observation.\"*\n",
    "\n",
    "**An AUC of ~ 0.8 is very good while an AUC of ~ 0.5 represents a poor classifier.**\n",
    "\n",
    "The ROC curve and AUC are insensitive to whether your predicted probabilities are properly calibrated to actually represent probabilities of class membership (e.g., it works if predicted probs range from 0.9 to 1 instead of 0 to 1).  All the AUC metric cares about is how well your classifier separated the two classes\n",
    "\n",
    "Notes:\n",
    "1.  AUC is useful even when there is **high class imbalance** (unlike classification accuracy)\n",
    "2.  AUC is useful even when predicted probabilities are not properly calibrated (e.g., not between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "instrumental-crawford",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6912144702842377\n"
     ]
    }
   ],
   "source": [
    "print(skm.roc_auc_score(eval_targs, eval_probs, average='weighted', sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-support",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Overall metrics - sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "imposed-elimination",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43617442"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse\n",
    "skm.mean_squared_error(targs[0], probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "frank-census",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6604350251936353"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rmse\n",
    "math.sqrt(skm.mean_squared_error(targs[0], probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "expired-stevens",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4809825"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mae\n",
    "skm.mean_absolute_error(targs[0], probs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-shoot",
   "metadata": {},
   "source": [
    "## Final results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "treated-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "try: del inf_learn\n",
    "except: pass\n",
    "finally: gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "northern-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn = load_learner(fname=f\"{train_config['learner_path']}/exp_{train_config['export_filename']}\")\n",
    "inf_learn.loss_func.loss_funcs[1].func = inf_learn.loss_func.loss_funcs[1].func.to(dls.device)\n",
    "dls = get_train_dls(df, hf_arch, hf_config, hf_tokenizer, hf_model, train_config_updates={})\n",
    "inf_learn.dls = dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "parallel-shareware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1342, 1]),\n",
       " torch.Size([1342, 2]),\n",
       " torch.Size([1342]),\n",
       " torch.Size([1342]),\n",
       " torch.Size([1342]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs, targs, loss = inf_learn.get_preds(with_loss=True, reorder=True)\n",
    "probs[0].shape, probs[1].shape, targs[0].shape, targs[1].shape, loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "invalid-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine optimal threshold based on desired f-score\n",
    "f05 = OptimalMultiThresholdMetrics(beta=0.5, start=0.05, end=.5, sigmoid=False, \n",
    "                                   average=average, sample_weight=sample_weight)\n",
    "f1 = OptimalMultiThresholdMetrics(beta=1, start=0.05, end=.5, sigmoid=False, \n",
    "                                   average=average, sample_weight=sample_weight)\n",
    "f2 = OptimalMultiThresholdMetrics(beta=2, start=0.05, end=.5, sigmoid=False, \n",
    "                                   average=average, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "incident-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_example_prob_true = torch.softmax(probs[1], dim=-1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "herbal-equality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27 0.27 0.27\n",
      "Fowards Only\n",
      "-------------\n",
      "f05:\tOptimal threshold = 0.27\t(Accuracy = 0.2973174452781677)\n",
      "f1:\tOptimal threshold = 0.27\t(Accuracy = 0.2973174452781677)\n",
      "f2:\tOptimal threshold = 0.27\t(Accuracy = 0.2973174452781677)\n",
      "\n",
      "Accuracy: 0.9932935833930969\n"
     ]
    }
   ],
   "source": [
    "# determine optimal threshold based on desired f-score\n",
    "threshold_f05 = f05.opt_th(is_example_prob_true, targs[1])\n",
    "threshold_f1 = f1.opt_th(is_example_prob_true, targs[1])\n",
    "threshold_f2 = f2.opt_th(is_example_prob_true, targs[1])\n",
    "\n",
    "print(threshold_f05, threshold_f1, threshold_f2)\n",
    "\n",
    "# determine accuracy based on optimal threshold\n",
    "val_acc_f05 = accuracy_multi(is_example_prob_true, targs[1], threshold_f05, sigmoid=False).item()\n",
    "val_acc_f1 = accuracy_multi(is_example_prob_true, targs[1], threshold_f1, sigmoid=False).item()\n",
    "val_acc_f2 = accuracy_multi(is_example_prob_true, targs[1], threshold_f2, sigmoid=False).item()\n",
    "\n",
    "print('Fowards Only\\n-------------')\n",
    "print(f'f05:\\tOptimal threshold = {threshold_f05}\\t(Accuracy = {val_acc_f05})')\n",
    "print(f'f1:\\tOptimal threshold = {threshold_f1}\\t(Accuracy = {val_acc_f1})')\n",
    "print(f'f2:\\tOptimal threshold = {threshold_f2}\\t(Accuracy = {val_acc_f2})')\n",
    "\n",
    "print(f'\\nAccuracy: {accuracy_multi(is_example_prob_true, targs[1], sigmoid=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "changed-accordance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.48010173, 0.43516338, 0.6596691433707621)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = skm.mean_absolute_error(targs[0], probs[0])\n",
    "mse = skm.mean_squared_error(targs[0], probs[0])\n",
    "rmse = math.sqrt(skm.mean_squared_error(targs[0], probs[0]))\n",
    "\n",
    "mae, mse, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "living-kuwait",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43901023268699646"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_valid_loss = loss.mean().item(); final_valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-wallpaper",
   "metadata": {},
   "source": [
    "## Build our training loop for hyperparam optimization and final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "authorized-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "try: del learn; del dls\n",
    "except: pass\n",
    "finally: gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "according-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train(params, trial=None, yyyymmdd = datetime.today().strftime(\"%Y%m%d\"), train_config_updates={}):\n",
    "    \n",
    "    config = {**train_config, **train_config_updates}    \n",
    "    m_pre, m_suf, base_model_name = config['m_pre'], config['m_suf'], config['base_model_name']\n",
    "    full_model_name = f'{m_pre}{base_model_name}{m_suf}'\n",
    "\n",
    "    # 1. grab our huggingface objects\n",
    "    task = HF_TASKS_AUTO.SequenceClassification\n",
    "    hf_config = AutoConfig.from_pretrained(params[\"pretrained_model_name\"])\n",
    "    \n",
    "    if (f'{params[\"pretrained_model_name\"]}_config_overrides' in params):\n",
    "        hf_config.update(params[f'{params[\"pretrained_model_name\"]}_config_overrides'])\n",
    "    else:\n",
    "        config_overrides = { k:v for k,v in params.items() if (k in hf_config.to_dict()) }\n",
    "        hf_config.update(config_overrides)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(params[\"pretrained_model_name\"], \n",
    "                                                                                   task=task, \n",
    "                                                                                   config=hf_config)\n",
    "\n",
    "    # 2. build our dls and learner\n",
    "    df = get_train_data(train_config_updates=config)\n",
    "    train_df, valid_df = df[df.is_valid == False], df[df.is_valid == True]\n",
    "    \n",
    "    set_seed(TL_RAND_SEED)\n",
    "    dls = get_train_dls(df, hf_arch, hf_config, hf_tokenizer, hf_model, train_config_updates=config, use_cache=False)\n",
    "    \n",
    "    set_seed(TL_RAND_SEED)\n",
    "    learn, fit_cbs = get_learner(hf_model, \n",
    "                                 dls, \n",
    "                                 train_df=None, \n",
    "                                 use_weighted_loss=params[\"use_weighted_loss\"], \n",
    "                                 use_fp16=params[\"use_fp16\"],\n",
    "                                 add_save_model_cb=params['save_model'],\n",
    "                                 train_config_updates=config)\n",
    "    \n",
    "    if (trial is not None): learn.add_cb(FastAIPruningCallbackv2(trial=trial, monitor=params['optimize_for']))\n",
    "    \n",
    "    # 3. train\n",
    "    with learn.no_logging(): \n",
    "        set_seed(TL_RAND_SEED)\n",
    "        learn.fit_one_cycle(params[\"n_frozen_epochs\"], lr_max=params[\"frozen_lr\"], cbs=fit_cbs)\n",
    "        \n",
    "        learn.unfreeze()\n",
    "        set_seed(TL_RAND_SEED)\n",
    "        learn.fit_one_cycle(params[\"n_unfrozen_epochs\"], \n",
    "                            lr_max=slice(params[\"unfrozen_lr_min\"], params[\"unfrozen_lr_max\"]), \n",
    "                            cbs=fit_cbs)\n",
    "        \n",
    "        # export model for inference (SavedModelCallback already saves the best model if save_mode=True)\n",
    "        if (trial is None): learn.export(fname=f\"{yyyymmdd}_{config['export_filename']}\")\n",
    "        \n",
    "    # 4. evaluate\n",
    "    scores = dict(zip(learn.recorder.metric_names[2:], learn.validate()))\n",
    "    \n",
    "    try:\n",
    "        if (trial is not None): return scores[params['optimize_for']]\n",
    "        \n",
    "        probs, targs, losses = learn.get_preds(dl=dls.valid, with_loss=True)\n",
    "\n",
    "        # determine optimal threshold based on desired f-score\n",
    "        average, sample_weight = config['opt_beta_average'], config['opt_beta_sample_weight']\n",
    "\n",
    "        f05 = OptimalMultiThresholdMetrics(beta=0.5, start=0.05, end=.5, sigmoid=False, \n",
    "                                           average=average, sample_weight=sample_weight)\n",
    "        f1 = OptimalMultiThresholdMetrics(beta=1, start=0.05, end=.5, sigmoid=False, \n",
    "                                           average=average, sample_weight=sample_weight)\n",
    "        f2 = OptimalMultiThresholdMetrics(beta=2, start=0.05, end=.5, sigmoid=False, \n",
    "                                           average=average, sample_weight=sample_weight)\n",
    "\n",
    "        is_example_prob_true = torch.softmax(probs[1], dim=-1)[:,1]\n",
    "        scores['is_example_f05'], scores['is_example_f1'], scores['is_example_f2'] = {}, {}, {}\n",
    "\n",
    "        scores['is_example_f05']['threshold'] = f05.opt_th(is_example_prob_true, targs[1])\n",
    "        scores['is_example_f1']['threshold'] = f1.opt_th(is_example_prob_true, targs[1])\n",
    "        scores['is_example_f2']['threshold'] = f2.opt_th(is_example_prob_true, targs[1])\n",
    "\n",
    "        scores['is_example_f05']['score'] = f05.opt_fscore(is_example_prob_true, targs[1])\n",
    "        scores['is_example_f1']['score'] = f1.opt_fscore(is_example_prob_true, targs[1])\n",
    "        scores['is_example_f2']['score'] = f2.opt_fscore(is_example_prob_true, targs[1])\n",
    "\n",
    "        scores['sentiment'] = {\n",
    "            'mae': skm.mean_absolute_error(targs[0], probs[0]).item(),\n",
    "            'mse': skm.mean_squared_error(targs[0], probs[0]).item(),\n",
    "            'rmse': math.sqrt(skm.mean_squared_error(targs[0], probs[0]).item())\n",
    "        }\n",
    "\n",
    "        # save scores from validation set if mode == training\n",
    "        with open(f\"{config['learner_path']}/{yyyymmdd}_{full_model_name}_train_scores.json\", 'w') as f:\n",
    "            json.dump(scores, f, indent=4)\n",
    "\n",
    "        # save train/validation probs, targs, losses for review\n",
    "        test_dl = dls.test_dl(df, with_labels=True)\n",
    "        probs, targs, losses = learn.get_preds(dl=test_dl, with_loss=True)\n",
    "        is_example_prob_true = torch.softmax(probs[1], dim=-1)[:,1]\n",
    "\n",
    "        probs_df = pd.DataFrame(np.concatenate((probs[0].numpy(), is_example_prob_true[:,None]), axis=-1), \n",
    "                                columns=['pred_sentiment', 'prob_is_example'])\n",
    "        targs_df = pd.DataFrame(np.concatenate((targs[0].numpy()[:,None], targs[1].numpy()[:,None]), axis=-1), \n",
    "                                columns= ['targ_sentiment', 'targ_is_example'])\n",
    "        losses_df = pd.DataFrame(losses.numpy(), columns=['loss'])\n",
    "        \n",
    "        final_df = pd.concat([df.reset_index(), probs_df, targs_df, losses_df], axis=1)\n",
    "\n",
    "        final_df.to_csv(f\"{config['learner_path']}/{yyyymmdd}_{full_model_name}_train_results.csv\", index=False)\n",
    "        return scores, final_df\n",
    "    \n",
    "    finally:\n",
    "        # cleanup\n",
    "        del learn; del dls \n",
    "        del hf_arch; del hf_config; del hf_tokenizer; del hf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-stereo",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "spread-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def after_trial_cleanup(study, trial):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "simple-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def objective(trial, yyyymmdd = datetime.today().strftime(\"%Y%m%d\"), train_config_updates={}):\n",
    "    opt_params = {\n",
    "        'pretrained_model_name': trial.suggest_categorical(\"pretrained_model_name\", [\"facebook/bart-base\"]),\n",
    "        \n",
    "        'save_model': trial.suggest_categorical(\"save_model\", [True, False]), \n",
    "        'use_weighted_loss': trial.suggest_categorical(\"use_weighted_loss\", [True, False]),\n",
    "        'use_fp16': trial.suggest_categorical(\"use_fp16\", [True]),\n",
    "        'n_frozen_epochs': trial.suggest_int(\"n_frozen_epochs\", 1, 3),\n",
    "        'n_unfrozen_epochs': trial.suggest_int(\"n_unfrozen_epochs\", 0, 10),\n",
    "        'frozen_lr': trial.suggest_loguniform(\"frozen_lr\", 1e-4, 1e-3),\n",
    "        'unfrozen_lr_max': trial.suggest_loguniform(\"unfrozen_lr_max\", 1e-7, 1e-6),\n",
    "        'unfrozen_lr_min': trial.suggest_loguniform(\"unfrozen_lr_min\", 1e-9, 1e-7),\n",
    "        'optimize_for': 'valid_loss',\n",
    "        \n",
    "        'facebook/bart-base_config_overrides': {\n",
    "            'activation_dropout': trial.suggest_discrete_uniform('activation_dropout', 0.0, 0.3, 0.05),\n",
    "            'attention_dropout': trial.suggest_discrete_uniform('attention_dropout', 0.0, 0.3, 0.05),\n",
    "            'classif_dropout': trial.suggest_discrete_uniform('classif_dropout', 0.0, 0.3, 0.05),\n",
    "            'dropout': trial.suggest_discrete_uniform('dropout', 0.0, 0.3, 0.05)\n",
    "        },\n",
    "        'roberta-base_config_overrides': {\n",
    "            'attention_probs_dropout_prob': trial.suggest_discrete_uniform('attention_probs_dropout_prob', 0.0, 0.3, 0.05),\n",
    "            'hidden_dropout_prob': trial.suggest_discrete_uniform('hidden_dropout_prob', 0.0, 0.3, 0.05)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    score = train(opt_params, trial=trial, yyyymmdd=yyyymmdd, train_config_updates=train_config_updates)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-centre",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "renewable-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyyymmdd = datetime.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner() if True else optuna.pruners.NopPruner()\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "\n",
    "train_config_updates = {}\n",
    "study.optimize(partial(objective, yyyymmdd=yyyymmdd, train_config_updates=train_config_updates), \n",
    "               n_trials=30, \n",
    "               callbacks=[after_trial_cleanup])#, timeout=600)\n",
    "\n",
    "end = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre, m_suf = train_config['m_pre'], train_config['m_suf']\n",
    "full_model_name = f\"{m_pre}{train_config['base_model_name']}{m_suf}\"\n",
    "   \n",
    "pruned_trials = [ t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED ]\n",
    "complete_trials = [ t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE ]\n",
    "\n",
    "print('Study statistics: ')\n",
    "print('  Number of finished trials: ', len(study.trials))\n",
    "print('  Number of pruned trials: ', len(pruned_trials))\n",
    "print('  Number of complete trials: ', len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value: ', trial.value)\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items(): print('    {}: {}'.format(key, value))\n",
    "print('  User attrs:')\n",
    "for key, value in trial.user_attrs.items(): print('    {}: {}'.format(key, value))\n",
    "    \n",
    "best_params = study.best_params\n",
    "best_params['fbeta_score'] = study.best_value\n",
    "\n",
    "with open(f\"{train_config['learner_path']}/{yyyymmdd}_{full_model_name}_best_trial_params.json\", 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df.to_csv(f\"{train_config['learner_path']}/{yyyymmdd}_{full_model_name}_trial_results.csv\", index=False)\n",
    "\n",
    "print(f'total time is {(end - start).total_seconds()} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-installation",
   "metadata": {},
   "source": [
    "### Training with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adverse-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pre, m_suf = train_config['m_pre'], train_config['m_suf']\n",
    "full_model_name = f\"{m_pre}{train_config['base_model_name']}{m_suf}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fewer-discussion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(f\"{train_config['learner_path']}/{yyyymmdd}_{full_model_name}_best_trial_params.json\") as f: \n",
    "    best_params = json.load(f)\n",
    "    \n",
    "train_yyyymmdd = datetime.today().strftime(\"%Y%m%d\")\n",
    "scores, train_res_df = train(params=best_params, yyyymmdd=train_yyyymmdd, train_config_updates={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sophisticated-content",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid_loss': 0.36898770928382874,\n",
       " 'sentiment_mse': 0.3652612566947937,\n",
       " 'is_example_acc': 0.9932935833930969,\n",
       " 'is_example_f05': {'threshold': 0.29000000000000004,\n",
       "  'score': 0.1724137931034483},\n",
       " 'is_example_f1': {'threshold': 0.28, 'score': 0.17142857142857146},\n",
       " 'is_example_f2': {'threshold': 0.28, 'score': 0.24193548387096775},\n",
       " 'sentiment': {'mae': 0.4495120048522949,\n",
       "  'mse': 0.3652612864971161,\n",
       "  'rmse': 0.6043685022377623}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "formed-combat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>question_ans_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_text_non_english</th>\n",
       "      <th>language</th>\n",
       "      <th>survey_id</th>\n",
       "      <th>survey_type_id</th>\n",
       "      <th>benchmark_survey_type</th>\n",
       "      <th>client_id</th>\n",
       "      <th>rsp_id</th>\n",
       "      <th>question_category_abbr</th>\n",
       "      <th>question_text</th>\n",
       "      <th>question_class</th>\n",
       "      <th>question_category_id</th>\n",
       "      <th>question_report_abbr</th>\n",
       "      <th>question_category_label</th>\n",
       "      <th>benchmark_level1</th>\n",
       "      <th>benchmark_level2</th>\n",
       "      <th>benchmark_level3</th>\n",
       "      <th>client_benchmark_level</th>\n",
       "      <th>group_code</th>\n",
       "      <th>group_id</th>\n",
       "      <th>group_level1_code</th>\n",
       "      <th>group_level1_name</th>\n",
       "      <th>group_level2_code</th>\n",
       "      <th>group_level2_name</th>\n",
       "      <th>group_level3_code</th>\n",
       "      <th>group_level3_name</th>\n",
       "      <th>group_level4_code</th>\n",
       "      <th>group_level4_name</th>\n",
       "      <th>group_level5_code</th>\n",
       "      <th>group_level5_name</th>\n",
       "      <th>group_level6_code</th>\n",
       "      <th>group_level6_name</th>\n",
       "      <th>group_level7_code</th>\n",
       "      <th>group_level7_name</th>\n",
       "      <th>group_level8_code</th>\n",
       "      <th>group_level8_name</th>\n",
       "      <th>standard_theme_id</th>\n",
       "      <th>theme</th>\n",
       "      <th>url_friendly_theme</th>\n",
       "      <th>theme_display_order</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>is_example</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>_theme_</th>\n",
       "      <th>_text_</th>\n",
       "      <th>pred_sentiment</th>\n",
       "      <th>prob_is_example</th>\n",
       "      <th>targ_sentiment</th>\n",
       "      <th>targ_is_example</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>660454</td>\n",
       "      <td>93069</td>\n",
       "      <td>\"Academics at UC ANR value my contributions.\"\\r\\n\"Staff members at UC ANR value my contributions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>396</td>\n",
       "      <td>47</td>\n",
       "      <td>SAW</td>\n",
       "      <td>UCANR</td>\n",
       "      <td>480552</td>\n",
       "      <td>None</td>\n",
       "      <td>Please provide any additional feedback regarding the work environment at UC ANR. Your comments w...</td>\n",
       "      <td>Verbatim-Comments</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>Comments re Work Environment at UC ANR</td>\n",
       "      <td>Comments</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>250400.0</td>\n",
       "      <td>6984</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>UC Agriculture &amp; Natural Resources</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>AVP Programs and Initiatives</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>Strategic Institutes and Statewide Programs</td>\n",
       "      <td>250400.0</td>\n",
       "      <td>Statewide IPM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>Have Voice within my Institution/Valued Member of my Institution</td>\n",
       "      <td>HaveVoiceWithinMyInstitutionValuedMemberOfMyInstitution</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Have Voice within my Institution/Valued Member of my Institution</td>\n",
       "      <td>\"Academics at UC ANR value my contributions.\"\\r\\n\"Staff members at UC ANR value my contributions...</td>\n",
       "      <td>2.003906</td>\n",
       "      <td>0.272173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>589692</td>\n",
       "      <td>2576</td>\n",
       "      <td>*The MSO of this department consistently takes unfair advantage of power dynamics to intimidate ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>212</td>\n",
       "      <td>9</td>\n",
       "      <td>SAW</td>\n",
       "      <td>UCSD</td>\n",
       "      <td>447156</td>\n",
       "      <td>C&amp;B</td>\n",
       "      <td>If you would like to elaborate on any of your answers to the conduct and behavioral questions ab...</td>\n",
       "      <td>Verbatim</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>Conduct &amp; Behavioral - Comments</td>\n",
       "      <td>Conduct &amp; Behavioral</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10104.0</td>\n",
       "      <td>3437</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>UC San Diego</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>ACADEMIC AFFAIRS</td>\n",
       "      <td>10002.0</td>\n",
       "      <td>DIVISIONS/SCHOOLS</td>\n",
       "      <td>10003.0</td>\n",
       "      <td>ARTS &amp; HUMANITIES</td>\n",
       "      <td>10104.0</td>\n",
       "      <td>MUSIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>Supervisor Effectiveness/Resolves Staff Issues</td>\n",
       "      <td>SupervisorEffectivenessResolvesStaffIssues</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Supervisor Effectiveness/Resolves Staff Issues</td>\n",
       "      <td>*The MSO of this department consistently takes unfair advantage of power dynamics to intimidate ...</td>\n",
       "      <td>1.587891</td>\n",
       "      <td>0.281652</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>589041</td>\n",
       "      <td>1877</td>\n",
       "      <td>Hiring and staffing pools in the research administration series have been very weak and it is di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>212</td>\n",
       "      <td>9</td>\n",
       "      <td>SAW</td>\n",
       "      <td>UCSD</td>\n",
       "      <td>449832</td>\n",
       "      <td>SAT</td>\n",
       "      <td>If you would like to elaborate on your responses above, or if you have any additional feedback r...</td>\n",
       "      <td>Verbatim</td>\n",
       "      <td>114.0</td>\n",
       "      <td>Comments re Work Environment at UCSD</td>\n",
       "      <td>Satisfaction with UC San Diego</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99119.0</td>\n",
       "      <td>4731</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>UC San Diego</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>VICE CHANCELLOR HEALTH SCIENCES</td>\n",
       "      <td>93000.0</td>\n",
       "      <td>SCHOOL OF MEDICINE</td>\n",
       "      <td>96000.0</td>\n",
       "      <td>DEAN'S OFFICE</td>\n",
       "      <td>99119.0</td>\n",
       "      <td>VC OPERATIONS LEADERSHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>Internal Processes Effective</td>\n",
       "      <td>InternalProcessesEffective</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Internal Processes Effective</td>\n",
       "      <td>Hiring and staffing pools in the research administration series have been very weak and it is di...</td>\n",
       "      <td>1.970703</td>\n",
       "      <td>0.270193</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>683498</td>\n",
       "      <td>1877</td>\n",
       "      <td>My supervisor is great but her supervisors are not. Our department is not respectful of the need...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>401</td>\n",
       "      <td>9</td>\n",
       "      <td>SAW</td>\n",
       "      <td>UCSD</td>\n",
       "      <td>495891</td>\n",
       "      <td>SAT</td>\n",
       "      <td>If you would like to elaborate on your responses above, or if you have any additional feedback r...</td>\n",
       "      <td>Verbatim</td>\n",
       "      <td>114.0</td>\n",
       "      <td>Comments re Work Environment at UCSD</td>\n",
       "      <td>Satisfaction with UC San Diego</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>700001.0</td>\n",
       "      <td>3828</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>UC San Diego</td>\n",
       "      <td>700000.0</td>\n",
       "      <td>ADVANCEMENT</td>\n",
       "      <td>700019.0</td>\n",
       "      <td>ALUMNI</td>\n",
       "      <td>700001.0</td>\n",
       "      <td>ALUMNI RELATIONS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>Supervisor Effectiveness/Resolves Staff Issues</td>\n",
       "      <td>SupervisorEffectivenessResolvesStaffIssues</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Supervisor Effectiveness/Resolves Staff Issues</td>\n",
       "      <td>My supervisor is great but her supervisors are not. Our department is not respectful of the need...</td>\n",
       "      <td>2.398438</td>\n",
       "      <td>0.271427</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>683028</td>\n",
       "      <td>1877</td>\n",
       "      <td>The inefficiencies and chronic shortages of resources (IT/computer related/eqipment) is so frust...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>401</td>\n",
       "      <td>9</td>\n",
       "      <td>SAW</td>\n",
       "      <td>UCSD</td>\n",
       "      <td>494058</td>\n",
       "      <td>SAT</td>\n",
       "      <td>If you would like to elaborate on your responses above, or if you have any additional feedback r...</td>\n",
       "      <td>Verbatim</td>\n",
       "      <td>114.0</td>\n",
       "      <td>Comments re Work Environment at UCSD</td>\n",
       "      <td>Satisfaction with UC San Diego</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90920.0</td>\n",
       "      <td>3665</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>UC San Diego</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>VICE CHANCELLOR HEALTH SCIENCES</td>\n",
       "      <td>93000.0</td>\n",
       "      <td>SCHOOL OF MEDICINE</td>\n",
       "      <td>90900.0</td>\n",
       "      <td>NEUROSCIENCES</td>\n",
       "      <td>90920.0</td>\n",
       "      <td>ALZHEIMERS DISEASE RESEARCH CENTER (ADRC)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>Internal Processes Effective</td>\n",
       "      <td>InternalProcessesEffective</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Internal Processes Effective</td>\n",
       "      <td>The inefficiencies and chronic shortages of resources (IT/computer related/eqipment) is so frust...</td>\n",
       "      <td>1.820312</td>\n",
       "      <td>0.274224</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      id  question_ans_id                                                                                          answer_text  answer_text_non_english language  survey_id  survey_type_id benchmark_survey_type client_id  rsp_id question_category_abbr                                                                                        question_text     question_class  question_category_id                    question_report_abbr         question_category_label benchmark_level1 benchmark_level2 benchmark_level3  client_benchmark_level  group_code  group_id  group_level1_code                   group_level1_name  group_level2_code                group_level2_name  group_level3_code                            group_level3_name  group_level4_code  group_level4_name  group_level5_code                          group_level5_name  group_level6_code group_level6_name  group_level7_code group_level7_name  group_level8_code  group_level8_name  standard_theme_id  \\\n",
       "0      0  660454            93069  \"Academics at UC ANR value my contributions.\"\\r\\n\"Staff members at UC ANR value my contributions...                      NaN  English        396              47                   SAW     UCANR  480552                   None  Please provide any additional feedback regarding the work environment at UC ANR. Your comments w...  Verbatim-Comments                1141.0  Comments re Work Environment at UC ANR                        Comments             None             None             None                     3.0    250400.0      6984           999999.0  UC Agriculture & Natural Resources           200000.0     AVP Programs and Initiatives           250000.0  Strategic Institutes and Statewide Programs           250400.0      Statewide IPM                NaN                                        NaN                NaN               NaN                NaN               NaN                NaN                NaN                 10   \n",
       "1      1  589692             2576  *The MSO of this department consistently takes unfair advantage of power dynamics to intimidate ...                      NaN  English        212               9                   SAW      UCSD  447156                    C&B  If you would like to elaborate on any of your answers to the conduct and behavioral questions ab...           Verbatim                1240.0         Conduct & Behavioral - Comments            Conduct & Behavioral             None             None             None                     3.0     10104.0      3437           999999.0                        UC San Diego            10000.0                 ACADEMIC AFFAIRS            10002.0                            DIVISIONS/SCHOOLS            10003.0  ARTS & HUMANITIES            10104.0                                      MUSIC                NaN               NaN                NaN               NaN                NaN                NaN                 19   \n",
       "2      2  589041             1877  Hiring and staffing pools in the research administration series have been very weak and it is di...                      NaN  English        212               9                   SAW      UCSD  449832                    SAT  If you would like to elaborate on your responses above, or if you have any additional feedback r...           Verbatim                 114.0    Comments re Work Environment at UCSD  Satisfaction with UC San Diego             None             None             None                     1.0     99119.0      4731           999999.0                        UC San Diego            90000.0  VICE CHANCELLOR HEALTH SCIENCES            93000.0                           SCHOOL OF MEDICINE            96000.0      DEAN'S OFFICE            99119.0                   VC OPERATIONS LEADERSHIP                NaN               NaN                NaN               NaN                NaN                NaN                 45   \n",
       "3      3  683498             1877  My supervisor is great but her supervisors are not. Our department is not respectful of the need...                      NaN  English        401               9                   SAW      UCSD  495891                    SAT  If you would like to elaborate on your responses above, or if you have any additional feedback r...           Verbatim                 114.0    Comments re Work Environment at UCSD  Satisfaction with UC San Diego             None             None             None                     1.0    700001.0      3828           999999.0                        UC San Diego           700000.0                      ADVANCEMENT           700019.0                                       ALUMNI           700001.0   ALUMNI RELATIONS                NaN                                        NaN                NaN               NaN                NaN               NaN                NaN                NaN                 19   \n",
       "4      4  683028             1877  The inefficiencies and chronic shortages of resources (IT/computer related/eqipment) is so frust...                      NaN  English        401               9                   SAW      UCSD  494058                    SAT  If you would like to elaborate on your responses above, or if you have any additional feedback r...           Verbatim                 114.0    Comments re Work Environment at UCSD  Satisfaction with UC San Diego             None             None             None                     1.0     90920.0      3665           999999.0                        UC San Diego            90000.0  VICE CHANCELLOR HEALTH SCIENCES            93000.0                           SCHOOL OF MEDICINE            90900.0      NEUROSCIENCES            90920.0  ALZHEIMERS DISEASE RESEARCH CENTER (ADRC)                NaN               NaN                NaN               NaN                NaN                NaN                 45   \n",
       "\n",
       "                                                              theme                                       url_friendly_theme  theme_display_order  avg_sentiment  is_example  is_valid                                                           _theme_                                                                                               _text_  pred_sentiment  prob_is_example  targ_sentiment  targ_is_example      loss  \n",
       "0  Have Voice within my Institution/Valued Member of my Institution  HaveVoiceWithinMyInstitutionValuedMemberOfMyInstitution                    1            2.0           0     False  Have Voice within my Institution/Valued Member of my Institution  \"Academics at UC ANR value my contributions.\"\\r\\n\"Staff members at UC ANR value my contributions...        2.003906         0.272173             2.0              0.0  0.000837  \n",
       "1                    Supervisor Effectiveness/Resolves Staff Issues               SupervisorEffectivenessResolvesStaffIssues                    1            1.0           0     False                    Supervisor Effectiveness/Resolves Staff Issues  *The MSO of this department consistently takes unfair advantage of power dynamics to intimidate ...        1.587891         0.281652             1.0              0.0  0.348853  \n",
       "2                                      Internal Processes Effective                               InternalProcessesEffective                    1            2.0           0     False                                      Internal Processes Effective  Hiring and staffing pools in the research administration series have been very weak and it is di...        1.970703         0.270193             2.0              0.0  0.001177  \n",
       "3                    Supervisor Effectiveness/Resolves Staff Issues               SupervisorEffectivenessResolvesStaffIssues                    1            2.0           0     False                    Supervisor Effectiveness/Resolves Staff Issues  My supervisor is great but her supervisors are not. Our department is not respectful of the need...        2.398438         0.271427             2.0              0.0  0.159385  \n",
       "4                                      Internal Processes Effective                               InternalProcessesEffective                    1            2.0           0     False                                      Internal Processes Effective  The inefficiencies and chronic shortages of resources (IT/computer related/eqipment) is so frust...        1.820312         0.274224             2.0              0.0  0.033632  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "junior-aside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid_loss': 0.36898770928382874,\n",
       " 'sentiment_mse': 0.3652612566947937,\n",
       " 'is_example_acc': 0.9932935833930969,\n",
       " 'is_example_f05': {'threshold': 0.29000000000000004,\n",
       "  'score': 0.1724137931034483},\n",
       " 'is_example_f1': {'threshold': 0.28, 'score': 0.17142857142857146},\n",
       " 'is_example_f2': {'threshold': 0.28, 'score': 0.24193548387096775},\n",
       " 'sentiment': {'mae': 0.4495120048522949,\n",
       "  'mse': 0.3652612864971161,\n",
       "  'rmse': 0.6043685022377623}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load scores\n",
    "with open(f\"{train_config['learner_path']}/{train_yyyymmdd}_{full_model_name}_train_scores.json\") as f: \n",
    "    training_results = json.load(f)\n",
    "    \n",
    "training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "twelve-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(((1.6395916938781738,), '0'), [tensor([1.6396]), tensor(0)], [tensor([1.6396]), tensor([0.9834, 0.0166])])]\n",
      "[(((4.271517753601074,), '0'), [tensor([4.2715]), tensor(0)], [tensor([4.2715]), tensor([0.9952, 0.0048])])]\n"
     ]
    }
   ],
   "source": [
    "inf_learn = load_learner(f\"{train_config['learner_path']}/{train_yyyymmdd}_{full_model_name}_export.pkl\")\n",
    "\n",
    "print(inf_learn.blurr_predict('theme: Benefits comment: We are not paid enough and the benefits are horrible'))\n",
    "print(inf_learn.blurr_predict(\"theme: Benfits comment: The faculty really support us well!!! I feel valued\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-tribune",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-indication",
   "metadata": {},
   "source": [
    "### Inference (ad-hoc documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "supposed-panama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adequate_staffing', 'advancement_and_training_opportunities', 'appropriate_stress_work_assigned_equitably', 'benefits', 'better_ways_recognized_participate_in_decisions']\n"
     ]
    }
   ],
   "source": [
    "print(STANDARD_THEME_SAW_LABELS[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "liberal-amazon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(((1.9707868099212646,), '0'), [tensor([1.9708]), tensor(0)], [tensor([1.9708]), tensor([0.9966, 0.0034])])]\n",
      "[(((2.4395132064819336,), '0'), [tensor([2.4395]), tensor(0)], [tensor([2.4395]), tensor([0.9981, 0.0019])])]\n",
      "[(((3.7818827629089355,), '0'), [tensor([3.7819]), tensor(0)], [tensor([3.7819]), tensor([0.9962, 0.0038])])]\n",
      "[(((1.9311851263046265,), '0'), [tensor([1.9312]), tensor(0)], [tensor([1.9312]), tensor([0.9969, 0.0031])])]\n",
      "[(((1.893664836883545,), '0'), [tensor([1.8937]), tensor(0)], [tensor([1.8937]), tensor([0.9978, 0.0022])])]\n",
      "[(((4.187333106994629,), '0'), [tensor([4.1873]), tensor(0)], [tensor([4.1873]), tensor([0.9793, 0.0207])])]\n"
     ]
    }
   ],
   "source": [
    "test_comments = [\n",
    "    'theme: Benfits comment: Not paid enough.',\n",
    "    'theme: Benfits comment: I am satisfied with my benefits and we have enough people in my department. The faculty is mean to me.',\n",
    "    'theme: Benfits comment: I love cats',\n",
    "    \"theme: Benfits comment: I can never find a parking spot. The shuttles are not on time. Help\",\n",
    "    \"theme: Benfits comment: I was really uncomfortable to express my opinion!!!\",\n",
    "    \"theme: Benfits comment: Jeff Wadell is an exceptional leader.  He has gone above and beyond to create a positive working environment and provide growth opportunities.  His commitment to his team is unrivaled and commendable.\\\\r\\\\n\\\\r\\\\nNikki Panza is a model of authentic leadership for Building and Custodial Services.  She has earned the respect of those she leads through honest communication, empathy, and ethical consultation.\"\n",
    "]\n",
    "\n",
    "for c in test_comments: print(inf_learn.blurr_predict(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-thesis",
   "metadata": {},
   "source": [
    "### Inference (batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "chronic-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_preds(inf_df, yyyymmdd=None, learner_export_path=None, train_scores_path=None, \n",
    "              device=torch.device('cpu'), train_config_updates={}):\n",
    "    \n",
    "    config = {**train_config, **train_config_updates}    \n",
    "    m_pre, m_suf, base_model_name = config['m_pre'], config['m_suf'], config['base_model_name']\n",
    "    full_model_name = f'{m_pre}{base_model_name}{m_suf}'\n",
    "    \n",
    "    # 1. grab learner, procs, and data\n",
    "    cpu = device.type == 'cpu'\n",
    "    \n",
    "    if (yyyymmdd is None and learner_export_path is None): \n",
    "        export_dir = Path(config['learner_path'])\n",
    "        learner_export_path = sorted(export_dir.glob(f\"[0-9]*_{config['export_filename']}\"), reverse=True)[0]\n",
    "        train_scores_path = export_dir/f\"{learner_export_path.stem.split('_')[0]}_{full_model_name}_train_scores.json\"\n",
    "        \n",
    "    if (learner_export_path is None): \n",
    "        learner_export_path = f\"{config['learner_path']}/{yyyymmdd}_{config['export_filename']}\"\n",
    "        \n",
    "    if (train_scores_path is None): \n",
    "        train_scores_path = f\"{config['learner_path']}/{yyyymmdd}_{full_model_name}_train_scores.json\"\n",
    "        \n",
    "    with open(train_scores_path) as f: training_results = json.load(f)\n",
    "        \n",
    "    inf_learn = load_learner(fname=learner_export_path, cpu=cpu)\n",
    "    inf_learn.model = inf_learn.model.to(device)\n",
    "    inf_learn.model = inf_learn.model.eval()\n",
    "    \n",
    "    # 2. define a suitable dataloader\n",
    "    inf_df = inf_df.copy()\n",
    "    inf_df[config['txt_cols']] = inf_df[config['orig_txt_cols']]\n",
    "    inf_df.dropna(subset=config['txt_cols'], inplace=True)\n",
    "    inf_df.reset_index(drop=True, inplace=True)\n",
    "    inf_dl = inf_learn.dls.test_dl(inf_df, rm_type_tfms=None, bs=16)\n",
    "\n",
    "    # 3. get probs and document vectors\n",
    "    test_probs_sent, test_probs_is_example = [], []\n",
    "    with torch.no_grad():\n",
    "        for index, b in enumerate(inf_dl):\n",
    "            if index % 1000 == 0:  print(index)\n",
    "\n",
    "            # note: even though there is no targets, each batch is a tuple!\n",
    "            probs = inf_learn.model(b[0])[0]\n",
    "            \n",
    "            # why \"detach\"? the computation of gradients wrt the weights of netG can be fully \n",
    "            # avoided in the backward pass if the graph is detached where it is.\n",
    "            test_probs_sent.append(to_detach(probs[0]))\n",
    "            test_probs_is_example.append(to_detach(torch.softmax(probs[1], dim=-1)))\n",
    "\n",
    "    all_probs_sent = L(torch.cat(test_probs_sent))\n",
    "    all_probs_is_example = L(torch.cat(test_probs_is_example))\n",
    "\n",
    "    # 4. ensure results are returned in order\n",
    "    # test_dl.get_idxs() => unsorted/original order items\n",
    "    all_probs_sent = all_probs_sent[0][np.argsort(inf_dl.get_idxs())]\n",
    "    all_probs_is_example = all_probs_is_example[0][np.argsort(inf_dl.get_idxs())]\n",
    "        \n",
    "    # 5. return results with scores in a df, probs, and labels\n",
    "    combined_probs = np.concatenate((all_probs_sent.numpy(), all_probs_is_example.numpy()[:,1][:,None]), axis=1)\n",
    "    prob_labels = ['prob_' + lbl for lbl in STANDARD_THEME_META_LABELS]\n",
    "    probs_df = pd.DataFrame(combined_probs, columns=prob_labels)\n",
    "    \n",
    "    for lbl in STANDARD_THEME_META_LABELS[1:]:\n",
    "        probs_df[f'pred_{lbl}'] = (probs_df[f'prob_{lbl}'] > training_results['is_example_f05']['threshold']).astype(np.int64)\n",
    "        \n",
    "    final_df = pd.concat([inf_df, probs_df], axis=1)\n",
    "    final_df.drop(columns=config['txt_cols'], inplace=True)\n",
    "        \n",
    "#     final_df['valid_loss'] = training_results['valid_loss']\n",
    "#     final_df['sentiment_mse'] = training_results['sentiment']['mse']\n",
    "#     final_df['sentiment_mae'] = training_results['sentiment']['mae']\n",
    "#     final_df['sentiment_rmse'] = training_results['sentiment']['rmse']\n",
    "#     final_df['is_example_f05_threshold'] = training_results['is_example_f05']['threshold']\n",
    "#     final_df['is_example_f05_score'] = training_results['is_example_f05']['score']\n",
    "#     final_df['is_example_f1_threshold'] = training_results['is_example_f1']['threshold']\n",
    "#     final_df['is_example_f1_score'] = training_results['is_example_f1']['score']\n",
    "#     final_df['is_example_f2_threshold'] = training_results['is_example_f2']['threshold']\n",
    "#     final_df['is_example_f2_score'] = training_results['is_example_f2']['score']\n",
    "\n",
    "    # cleanup\n",
    "    try: del inf_learn; del inf_dl\n",
    "    except: pass\n",
    "    finally: gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "    return final_df, Path(learner_export_path).stem, training_results, STANDARD_THEME_META_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "motivated-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyyymmdd = train_yyyymmdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "atomic-strain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6094 6094\n"
     ]
    }
   ],
   "source": [
    "verbatims_df = pd.read_csv(STANDARD_THEME_SAW_PATH/'test_saw_themes_predictions_multilabel_hf.csv', parse_dates=[])\n",
    "\n",
    "inf_df = verbatims_df.copy() #verbatims_df[test_df.SurveyID == 130].copy()\n",
    "inf_df.reset_index(drop=True, inplace=True)\n",
    "print(len(verbatims_df), len(inf_df))\n",
    "\n",
    "orig_txt_cols = ['theme', 'AnswerText'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "hydraulic-image",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLVerbatimId</th>\n",
       "      <th>AnswerText</th>\n",
       "      <th>AnswerLang</th>\n",
       "      <th>_seq_id</th>\n",
       "      <th>_tl_item_id</th>\n",
       "      <th>_n_sentences</th>\n",
       "      <th>_text_</th>\n",
       "      <th>prob_adequate_staffing</th>\n",
       "      <th>prob_advancement_and_training_opportunities</th>\n",
       "      <th>prob_appropriate_stress_work_assigned_equitably</th>\n",
       "      <th>prob_benefits</th>\n",
       "      <th>prob_better_ways_recognized_participate_in_decisions</th>\n",
       "      <th>prob_career_advancement</th>\n",
       "      <th>prob_committed_to_diversity</th>\n",
       "      <th>prob_communicates_essential_information</th>\n",
       "      <th>prob_ethical_conduct_perform_responsibilities_spirit_of_cooperation</th>\n",
       "      <th>prob_evaluated_fairly</th>\n",
       "      <th>prob_experienced_discrimination</th>\n",
       "      <th>prob_facilities_workspace_safety</th>\n",
       "      <th>prob_faculty_value_contributions</th>\n",
       "      <th>prob_favoritism_cliques</th>\n",
       "      <th>prob_fear_of_retaliation_negative_consequences</th>\n",
       "      <th>prob_feel_valued_by_department</th>\n",
       "      <th>prob_flexibility_work_life_balance</th>\n",
       "      <th>prob_good_use_of_skills</th>\n",
       "      <th>prob_have_necessary_tools</th>\n",
       "      <th>prob_have_voice_within_my_institution_valued_member_of_my_institution</th>\n",
       "      <th>prob_internal_processes_effective</th>\n",
       "      <th>prob_parking_transportation</th>\n",
       "      <th>prob_salary_pay</th>\n",
       "      <th>prob_satisfied_with_diversity_progams</th>\n",
       "      <th>prob_supervisor_effectiveness_resolves_staff_issues</th>\n",
       "      <th>pred_adequate_staffing</th>\n",
       "      <th>pred_advancement_and_training_opportunities</th>\n",
       "      <th>pred_appropriate_stress_work_assigned_equitably</th>\n",
       "      <th>pred_benefits</th>\n",
       "      <th>pred_better_ways_recognized_participate_in_decisions</th>\n",
       "      <th>pred_career_advancement</th>\n",
       "      <th>pred_committed_to_diversity</th>\n",
       "      <th>pred_communicates_essential_information</th>\n",
       "      <th>pred_ethical_conduct_perform_responsibilities_spirit_of_cooperation</th>\n",
       "      <th>pred_evaluated_fairly</th>\n",
       "      <th>pred_experienced_discrimination</th>\n",
       "      <th>pred_facilities_workspace_safety</th>\n",
       "      <th>pred_faculty_value_contributions</th>\n",
       "      <th>pred_favoritism_cliques</th>\n",
       "      <th>pred_fear_of_retaliation_negative_consequences</th>\n",
       "      <th>pred_feel_valued_by_department</th>\n",
       "      <th>pred_flexibility_work_life_balance</th>\n",
       "      <th>pred_good_use_of_skills</th>\n",
       "      <th>pred_have_necessary_tools</th>\n",
       "      <th>pred_have_voice_within_my_institution_valued_member_of_my_institution</th>\n",
       "      <th>pred_internal_processes_effective</th>\n",
       "      <th>pred_parking_transportation</th>\n",
       "      <th>pred_salary_pay</th>\n",
       "      <th>pred_satisfied_with_diversity_progams</th>\n",
       "      <th>pred_supervisor_effectiveness_resolves_staff_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>612674</td>\n",
       "      <td>Accounts Payable department specifically Cyndi Williams &amp; Ann Avery.</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>30471</td>\n",
       "      <td>2</td>\n",
       "      <td>Accounts Payable department specifically Cyndi Williams &amp; Ann Avery.</td>\n",
       "      <td>0.03912</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.004735</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.060767</td>\n",
       "      <td>0.136294</td>\n",
       "      <td>0.02233</td>\n",
       "      <td>0.021006</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.022556</td>\n",
       "      <td>0.048244</td>\n",
       "      <td>0.114947</td>\n",
       "      <td>0.022684</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.005582</td>\n",
       "      <td>0.00314</td>\n",
       "      <td>0.06961</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.167219</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.138563</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MLVerbatimId                                                            AnswerText AnswerLang  _seq_id  _tl_item_id  _n_sentences                                                                _text_  prob_adequate_staffing  prob_advancement_and_training_opportunities  prob_appropriate_stress_work_assigned_equitably  prob_benefits  prob_better_ways_recognized_participate_in_decisions  prob_career_advancement  prob_committed_to_diversity  prob_communicates_essential_information  prob_ethical_conduct_perform_responsibilities_spirit_of_cooperation  prob_evaluated_fairly  prob_experienced_discrimination  prob_facilities_workspace_safety  prob_faculty_value_contributions  prob_favoritism_cliques  prob_fear_of_retaliation_negative_consequences  prob_feel_valued_by_department  prob_flexibility_work_life_balance  prob_good_use_of_skills  prob_have_necessary_tools  prob_have_voice_within_my_institution_valued_member_of_my_institution  prob_internal_processes_effective  \\\n",
       "0        612674  Accounts Payable department specifically Cyndi Williams & Ann Avery.    English        1        30471             2  Accounts Payable department specifically Cyndi Williams & Ann Avery.                 0.03912                                     0.002478                                         0.006757       0.008032                                              0.004365                 0.004735                     0.007501                                 0.060767                                                             0.136294                0.02233                         0.021006                          0.002022                          0.022556                 0.048244                                        0.114947                        0.022684                            0.002981                 0.001792                   0.005582                                                                0.00314                            0.06961   \n",
       "\n",
       "   prob_parking_transportation  prob_salary_pay  prob_satisfied_with_diversity_progams  prob_supervisor_effectiveness_resolves_staff_issues  pred_adequate_staffing  pred_advancement_and_training_opportunities  pred_appropriate_stress_work_assigned_equitably  pred_benefits  pred_better_ways_recognized_participate_in_decisions  pred_career_advancement  pred_committed_to_diversity  pred_communicates_essential_information  pred_ethical_conduct_perform_responsibilities_spirit_of_cooperation  pred_evaluated_fairly  pred_experienced_discrimination  pred_facilities_workspace_safety  pred_faculty_value_contributions  pred_favoritism_cliques  pred_fear_of_retaliation_negative_consequences  pred_feel_valued_by_department  pred_flexibility_work_life_balance  pred_good_use_of_skills  pred_have_necessary_tools  pred_have_voice_within_my_institution_valued_member_of_my_institution  pred_internal_processes_effective  pred_parking_transportation  pred_salary_pay  \\\n",
       "0                     0.005311         0.167219                               0.005125                                             0.138563                       0                                            0                                                0              0                                                     0                        0                            0                                        0                                                                    0                      0                                0                                 0                                 0                        0                                               0                               0                                   0                        0                          0                                                                      0                                  0                            0                0   \n",
       "\n",
       "   pred_satisfied_with_diversity_progams  pred_supervisor_effectiveness_resolves_staff_issues  \n",
       "0                                      0                                                    0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "apart-retailer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLVerbatimId',\n",
       " 'AnswerText',\n",
       " 'AnswerLang',\n",
       " '_seq_id',\n",
       " '_tl_item_id',\n",
       " '_n_sentences',\n",
       " '_text_']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_ml_cols = [ col for col in inf_df.columns if not col.startswith('pred_') and not col.startswith('prob_') ]\n",
    "non_ml_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "english-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_theme_cols = filter_col = [col for col in inf_df if col.startswith('prob_')]\n",
    "# prob_theme_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "healthy-diabetes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152350"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_df = inf_df.melt(id_vars=non_ml_cols, \n",
    "                     value_vars=pred_theme_cols, \n",
    "                     var_name='theme', \n",
    "                     value_name='theme_prob')\n",
    "len(inf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "latter-thing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1566"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_df = inf_df.loc[inf_df.theme_prob >= 0.42000000000000004]\n",
    "len(inf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "spectacular-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df['url_friendly_theme'] = inf_df.theme.apply(\n",
    "    lambda s: re.sub(\"(.*?)_([a-zA-Z])\",\"\\g<1> \\g<2>\",s).replace('prob', '').strip().title().replace(' ',''))\n",
    "\n",
    "inf_df['theme'] = inf_df.url_friendly_theme.apply(lambda s: re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",s))\n",
    "# inf_df['answer_text'] = inf_df['AnswerText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "published-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "miniature-flight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>MLVerbatimId</th>\n",
       "      <th>AnswerText</th>\n",
       "      <th>AnswerLang</th>\n",
       "      <th>_seq_id</th>\n",
       "      <th>_tl_item_id</th>\n",
       "      <th>_n_sentences</th>\n",
       "      <th>_text_</th>\n",
       "      <th>theme</th>\n",
       "      <th>theme_prob</th>\n",
       "      <th>url_friendly_theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191</td>\n",
       "      <td>612608</td>\n",
       "      <td>Get that department more resources PLEASE!</td>\n",
       "      <td>English</td>\n",
       "      <td>4</td>\n",
       "      <td>30655</td>\n",
       "      <td>4</td>\n",
       "      <td>Get that department more resources PLEASE!</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.424475</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>798</td>\n",
       "      <td>613014</td>\n",
       "      <td>The team had to start an involutary enrollment migration from one campus to another, while prepa...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>31247</td>\n",
       "      <td>2</td>\n",
       "      <td>The team had to start an involutary enrollment migration from one campus to another, while prepa...</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.565492</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800</td>\n",
       "      <td>613019</td>\n",
       "      <td>Human resources/personnel management is critical and often difficult for divisions/departments t...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>31249</td>\n",
       "      <td>2</td>\n",
       "      <td>Human resources/personnel management is critical and often difficult for divisions/departments t...</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.450304</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>880</td>\n",
       "      <td>612993</td>\n",
       "      <td>The CO team has been a valuable resource and support to our campus which has/is experiencing hig...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>31332</td>\n",
       "      <td>1</td>\n",
       "      <td>The CO team has been a valuable resource and support to our campus which has/is experiencing hig...</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.459479</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>912</td>\n",
       "      <td>612952</td>\n",
       "      <td>Most of CSULB Procurement and Contractual staff go above and beyond the call of duty, daily.</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>31363</td>\n",
       "      <td>4</td>\n",
       "      <td>Most of CSULB Procurement and Contractual staff go above and beyond the call of duty, daily.</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.499364</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  MLVerbatimId                                                                                           AnswerText AnswerLang  _seq_id  _tl_item_id  _n_sentences                                                                                               _text_              theme  theme_prob url_friendly_theme\n",
       "0    191        612608                                                           Get that department more resources PLEASE!    English        4        30655             4                                                           Get that department more resources PLEASE!  Adequate Staffing    0.424475   AdequateStaffing\n",
       "1    798        613014  The team had to start an involutary enrollment migration from one campus to another, while prepa...    English        1        31247             2  The team had to start an involutary enrollment migration from one campus to another, while prepa...  Adequate Staffing    0.565492   AdequateStaffing\n",
       "2    800        613019  Human resources/personnel management is critical and often difficult for divisions/departments t...    English        1        31249             2  Human resources/personnel management is critical and often difficult for divisions/departments t...  Adequate Staffing    0.450304   AdequateStaffing\n",
       "3    880        612993  The CO team has been a valuable resource and support to our campus which has/is experiencing hig...    English        1        31332             1  The CO team has been a valuable resource and support to our campus which has/is experiencing hig...  Adequate Staffing    0.459479   AdequateStaffing\n",
       "4    912        612952         Most of CSULB Procurement and Contractual staff go above and beyond the call of duty, daily.    English        1        31363             4         Most of CSULB Procurement and Contractual staff go above and beyond the call of duty, daily.  Adequate Staffing    0.499364   AdequateStaffing"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "original-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def build_meta_inf_df(themes_df, theme_prob_threshold= 0.5, fixed_cols=None):\n",
    "    inf_df = themes_df.copy()\n",
    "    \n",
    "    if (fixed_cols is None):\n",
    "        fixed_cols = [ col for col in inf_df.columns if not col.startswith('pred_') and not col.startswith('prob_') ]\n",
    "    \n",
    "    prob_theme_cols = filter_col = [col for col in inf_df if col.startswith('prob_')]\n",
    "    \n",
    "    inf_df = inf_df.melt(id_vars=fixed_cols, value_vars=prob_theme_cols, var_name='theme', value_name='theme_prob')\n",
    "    inf_df = inf_df.loc[inf_df.theme_prob >= theme_prob_threshold]\n",
    "    \n",
    "    inf_df['url_friendly_theme'] = inf_df.theme.apply(\n",
    "        lambda s: re.sub(\"(.*?)_([a-zA-Z])\",\"\\g<1> \\g<2>\",s).replace('prob', '').strip().title().replace(' ',''))\n",
    "    \n",
    "    inf_df['theme'] = inf_df.url_friendly_theme.apply(lambda s: re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",s))\n",
    "\n",
    "    inf_df.reset_index(inplace=True)\n",
    "    return inf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-religious",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "compliant-alignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "0\n",
      "(1566, 15) 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "preds_df, model_name, train_res, inf_labels = get_preds(inf_df, device=device, yyyymmdd=train_yyyymmdd, \n",
    "                                                        train_config_updates={'orig_txt_cols': ['theme', '_text_']})\n",
    "print(preds_df.shape, len(inf_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "secret-metabolism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>MLVerbatimId</th>\n",
       "      <th>AnswerText</th>\n",
       "      <th>AnswerLang</th>\n",
       "      <th>_seq_id</th>\n",
       "      <th>_tl_item_id</th>\n",
       "      <th>_n_sentences</th>\n",
       "      <th>_text_</th>\n",
       "      <th>theme</th>\n",
       "      <th>theme_prob</th>\n",
       "      <th>url_friendly_theme</th>\n",
       "      <th>_theme_</th>\n",
       "      <th>prob_avg_sentiment</th>\n",
       "      <th>prob_is_example</th>\n",
       "      <th>pred_is_example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191</td>\n",
       "      <td>612608</td>\n",
       "      <td>Get that department more resources PLEASE!</td>\n",
       "      <td>English</td>\n",
       "      <td>4</td>\n",
       "      <td>30655</td>\n",
       "      <td>4</td>\n",
       "      <td>Get that department more resources PLEASE!</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.424475</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>2.156372</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>798</td>\n",
       "      <td>613014</td>\n",
       "      <td>The team had to start an involutary enrollment migration from one campus to another, while prepa...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>31247</td>\n",
       "      <td>2</td>\n",
       "      <td>The team had to start an involutary enrollment migration from one campus to another, while prepa...</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.565492</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>2.386427</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800</td>\n",
       "      <td>613019</td>\n",
       "      <td>Human resources/personnel management is critical and often difficult for divisions/departments t...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>31249</td>\n",
       "      <td>2</td>\n",
       "      <td>Human resources/personnel management is critical and often difficult for divisions/departments t...</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.450304</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>2.012432</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>880</td>\n",
       "      <td>612993</td>\n",
       "      <td>The CO team has been a valuable resource and support to our campus which has/is experiencing hig...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>31332</td>\n",
       "      <td>1</td>\n",
       "      <td>The CO team has been a valuable resource and support to our campus which has/is experiencing hig...</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.459479</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>4.221957</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>912</td>\n",
       "      <td>612952</td>\n",
       "      <td>Most of CSULB Procurement and Contractual staff go above and beyond the call of duty, daily.</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>31363</td>\n",
       "      <td>4</td>\n",
       "      <td>Most of CSULB Procurement and Contractual staff go above and beyond the call of duty, daily.</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>0.499364</td>\n",
       "      <td>AdequateStaffing</td>\n",
       "      <td>Adequate Staffing</td>\n",
       "      <td>3.993353</td>\n",
       "      <td>0.015695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  MLVerbatimId                                                                                           AnswerText AnswerLang  _seq_id  _tl_item_id  _n_sentences                                                                                               _text_              theme  theme_prob url_friendly_theme            _theme_  prob_avg_sentiment  prob_is_example  pred_is_example\n",
       "0    191        612608                                                           Get that department more resources PLEASE!    English        4        30655             4                                                           Get that department more resources PLEASE!  Adequate Staffing    0.424475   AdequateStaffing  Adequate Staffing            2.156372         0.001103                0\n",
       "1    798        613014  The team had to start an involutary enrollment migration from one campus to another, while prepa...    English        1        31247             2  The team had to start an involutary enrollment migration from one campus to another, while prepa...  Adequate Staffing    0.565492   AdequateStaffing  Adequate Staffing            2.386427         0.002057                0\n",
       "2    800        613019  Human resources/personnel management is critical and often difficult for divisions/departments t...    English        1        31249             2  Human resources/personnel management is critical and often difficult for divisions/departments t...  Adequate Staffing    0.450304   AdequateStaffing  Adequate Staffing            2.012432         0.001548                0\n",
       "3    880        612993  The CO team has been a valuable resource and support to our campus which has/is experiencing hig...    English        1        31332             1  The CO team has been a valuable resource and support to our campus which has/is experiencing hig...  Adequate Staffing    0.459479   AdequateStaffing  Adequate Staffing            4.221957         0.002297                0\n",
       "4    912        612952         Most of CSULB Procurement and Contractual staff go above and beyond the call of duty, daily.    English        1        31363             4         Most of CSULB Procurement and Contractual staff go above and beyond the call of duty, daily.  Adequate Staffing    0.499364   AdequateStaffing  Adequate Staffing            3.993353         0.015695                0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dangerous-genius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid_loss': 0.36898770928382874,\n",
       " 'sentiment_mse': 0.3652612566947937,\n",
       " 'is_example_acc': 0.9932935833930969,\n",
       " 'is_example_f05': {'threshold': 0.29000000000000004,\n",
       "  'score': 0.1724137931034483},\n",
       " 'is_example_f1': {'threshold': 0.28, 'score': 0.17142857142857146},\n",
       " 'is_example_f2': {'threshold': 0.28, 'score': 0.24193548387096775},\n",
       " 'sentiment': {'mae': 0.4495120048522949,\n",
       "  'mse': 0.3652612864971161,\n",
       "  'rmse': 0.6043685022377623}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name, train_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-stocks",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "varied-trick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_verbatims-sentiment.ipynb.\n",
      "Converted 02a_verbatims-standard-themes-saw.ipynb.\n",
      "Converted 02b_verbatims-standard-themes-css.ipynb.\n",
      "Converted 02c_verbatims-standard-themes-meta.ipynb.\n",
      "Converted 99_inference.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-discrimination",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "molecular-congo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== text ===\n",
      "Human resources/personnel management is critical and often difficult for divisions/departments to handle when problems arise.\n",
      "\n",
      "=== preds ===\n",
      "[2.0124316e+00 1.5482951e-03]\n"
     ]
    }
   ],
   "source": [
    "verbatim_id = 613019\n",
    "\n",
    "pred_lbls = [ f'prob_{lbl}' for lbl in STANDARD_THEME_META_LABELS ]\n",
    "prob_lbls = [ f'prob_{lbl}' for lbl in STANDARD_THEME_META_LABELS ]\n",
    "\n",
    "print(\"=== text ===\")\n",
    "print(preds_df.AnswerText[preds_df.MLVerbatimId == verbatim_id].values[0])\n",
    "print('\\n=== preds ===')\n",
    "preds = preds_df[pred_lbls][preds_df.MLVerbatimId == verbatim_id].values[0]\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "responsible-investigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>MLVerbatimId</th>\n",
       "      <th>AnswerText</th>\n",
       "      <th>AnswerLang</th>\n",
       "      <th>_seq_id</th>\n",
       "      <th>_tl_item_id</th>\n",
       "      <th>_n_sentences</th>\n",
       "      <th>_text_</th>\n",
       "      <th>theme</th>\n",
       "      <th>theme_prob</th>\n",
       "      <th>url_friendly_theme</th>\n",
       "      <th>_theme_</th>\n",
       "      <th>prob_avg_sentiment</th>\n",
       "      <th>prob_is_example</th>\n",
       "      <th>pred_is_example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, MLVerbatimId, AnswerText, AnswerLang, _seq_id, _tl_item_id, _n_sentences, _text_, theme, theme_prob, url_friendly_theme, _theme_, prob_avg_sentiment, prob_is_example, pred_is_example]\n",
       "Index: []"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df[preds_df.pred_is_example == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "intellectual-productivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041846033"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.prob_is_example.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "annual-appendix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29000000000000004, 0.28, 0.28)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['is_example_f05']['threshold'], scores['is_example_f1']['threshold'], scores['is_example_f2']['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-child",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
